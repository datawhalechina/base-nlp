# ç¬¬äºŒèŠ‚ NER æµç¨‹åŒ–ä»£ç å®è·µ

åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç®€å•ä»‹ç»äº†å‘½åå®ä½“è¯†åˆ«çš„ä»»åŠ¡å®šä¹‰ã€åº”ç”¨åœºæ™¯åŠä¸»æµå®ç°æ–¹æ³•ã€‚æœ¬èŠ‚å°†æ­£å¼è¿›å…¥ç¼–ç é˜¶æ®µï¼Œä»æ•°æ®å¤„ç†å¼€å§‹ï¼Œé€æ­¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„ NER é¡¹ç›®ã€‚ä¸ºäº†æ¸…æ™°åœ°æ„å»º NER çš„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬é‡‡ç”¨æµç¨‹åŒ–çš„ä»£ç ç»„ç»‡æ€è·¯ï¼Œå°†æ•´ä¸ªæµç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„è„šæœ¬ã€‚

## ä¸€ã€æ•°æ®å¤„ç†æµç¨‹æ€»è§ˆ

åœ¨ NLP ä¸­ï¼ŒåŸå§‹çš„æ–‡æœ¬å’Œæ ‡æ³¨æ•°æ®æ˜¯æ— æ³•ç›´æ¥è¢«ç¥ç»ç½‘ç»œæ¨¡å‹åˆ©ç”¨çš„ã€‚éœ€è¦å°†è¿™äº›åŸå§‹æ•°æ®è½¬æ¢æˆæ¨¡å‹èƒ½å¤Ÿç†è§£çš„ã€æ ‡å‡†åŒ–çš„æ•°å­—å¼ é‡ã€‚

### 1.1 æ˜ç¡®æ•°æ®å¤„ç†çš„ç›®æ ‡

åœ¨è®¾è®¡ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ˜ç¡®æœ€ç»ˆçš„ç›®æ ‡ã€‚å¯¹äºä¸€ä¸ªå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ•°æ®å¤„ç†éœ€è¦äº§å‡ºä»€ä¹ˆï¼Ÿ

1.  **æ¨¡å‹çš„è¾“å…¥ (X) æ˜¯ä»€ä¹ˆï¼Ÿ**
    -   å®ƒåº”è¯¥æ˜¯ä¸€ä¸ªæ•´æ•°å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[batch_size, seq_len]`ã€‚
    -   å…¶ä¸­ `batch_size` æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œ`seq_len` æ˜¯åºåˆ—é•¿åº¦ï¼ˆé€šå¸¸æ˜¯æ‰¹æ¬¡å†…æœ€é•¿å¥å­çš„é•¿åº¦ï¼‰ã€‚
    -   å¼ é‡ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œéƒ½ä»£è¡¨åŸå§‹å¥å­ä¸­ä¸€ä¸ªå­—ç¬¦ï¼ˆTokenï¼‰åœ¨è¯æ±‡è¡¨é‡Œå¯¹åº”çš„å”¯ä¸€ IDã€‚

2.  **æ¨¡å‹çš„æ ‡ç­¾ (Y) æ˜¯ä»€ä¹ˆï¼Ÿ**
    -   å®ƒä¹Ÿåº”è¯¥æ˜¯ä¸€ä¸ªæ•´æ•°å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ X å®Œå…¨ç›¸åŒï¼Œå³ `[batch_size, seq_len]`ã€‚
    -   å…¶ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œä»£è¡¨ç€å¯¹åº”ä½ç½®å­—ç¬¦çš„å®ä½“æ ‡ç­¾ IDï¼ˆä¾‹å¦‚ï¼Œ`B-bod` å¯¹åº”çš„ IDï¼‰ã€‚

3.  **å¦‚ä½•å®ç°ä»â€œæ–‡æœ¬â€åˆ°â€œIDâ€çš„è½¬æ¢ï¼Ÿ**
    -   **æ–‡æœ¬ -> Token ID**ï¼šéœ€è¦æ„å»ºä¸€ä¸ª â€œå­—ç¬¦-IDâ€ çš„æ˜ å°„è¡¨ï¼Œä¹Ÿå°±æ˜¯**è¯æ±‡è¡¨ (Vocabulary)**ã€‚
    -   **å®ä½“ -> æ ‡ç­¾ ID**ï¼šéœ€è¦æ„å»ºä¸€ä¸ª â€œæ ‡ç­¾-IDâ€ çš„æ˜ å°„è¡¨ã€‚

### 1.2 æ•°æ®æ ¼å¼è§£æ

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `CMeEE-V2`ï¼ˆä¸­æ–‡åŒ»å­¦å®ä½“æŠ½å–ï¼‰æ•°æ®é›†ã€‚ç»è¿‡åˆ†æï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨çš„æ˜¯æ ‡å‡†çš„ **JSON æ•°ç»„**æ ¼å¼ã€‚

#### 1.2.1 åŸå§‹æ•°æ®ç¤ºä¾‹

æ‰“å¼€ `CMeEE-V2_train.json`ï¼Œå¯ä»¥çœ‹åˆ°æ–‡ä»¶å†…å®¹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼š

```json
[
  ...,
  {
    "text": "ï¼ˆ2ï¼‰å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿå¯ç”¨å¸¸è§„æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©æ§åˆ¶ï¼Œå¹´é¾„å°äº5å²ã€‚",
    "entities": [
      {
        "start_idx": 3,
        "end_idx": 9,
        "type": "dis",
        "entity": "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"
      },
      {
        "start_idx": 14,
        "end_idx": 20,
        "type": "dru",
        "entity": "æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©"
      }
    ]
  },
  ...
]
```

#### 1.2.2 å­—æ®µè¯´æ˜

-   **`text`**ï¼šåŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
-   **`entities`**ï¼šå®ä½“æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«ï¼š
    -   `start_idx`ï¼šå®ä½“èµ·å§‹ä½ç½®ï¼ˆ**åŒ…å«**ï¼‰
    -   `end_idx`ï¼šå®ä½“ç»“æŸä½ç½®ï¼ˆ**ä¸åŒ…å«**ï¼‰
    -   `type`ï¼šå®ä½“ç±»å‹ï¼ˆå¦‚ `dis` ç–¾ç—…ã€`dru` è¯ç‰©ï¼‰
    -   `entity`ï¼šå®ä½“æ–‡æœ¬ï¼ˆç”¨äºéªŒè¯ï¼‰

> **ç´¢å¼•çš„åŒ…å«æ€§**
>
> é€šè¿‡å®é™…æµ‹è¯•å¯ä»¥éªŒè¯ï¼š`start_idx` **åŒ…å«**åœ¨å®ä½“èŒƒå›´å†…ï¼Œ`end_idx` **ä¸åŒ…å«**ã€‚è¿™ä¸ Python çš„åˆ‡ç‰‡æ“ä½œ `text[start:end]` è¡Œä¸ºä¸€è‡´ã€‚ä¾‹å¦‚ï¼š
> - æ–‡æœ¬ï¼š"ï¼ˆ2ï¼‰å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿå¯ç”¨å¸¸è§„æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©æ§åˆ¶ï¼Œå¹´é¾„å°äº5å²ã€‚"
> - å®ä½“ "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"ï¼š`start_idx=3, end_idx=9`
> - å®é™…å­—ç¬¦ï¼š`text[3:9]` = "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"ï¼ˆç´¢å¼•3åˆ°8ï¼‰
>
> æ‰€ä»¥ï¼Œå®ä½“é•¿åº¦ = `end_idx - start_idx`

## äºŒã€æ„å»ºæ ‡ç­¾æ˜ å°„

> **ç›®æ ‡**ï¼šä»åŸå§‹æ•°æ®ä¸­æå–æ‰€æœ‰å®ä½“ç±»å‹ï¼Œç„¶ååŸºäº `BMES` æ ‡æ³¨æ–¹æ¡ˆæ„å»ºä¸€ä¸ªå…¨å±€ç»Ÿä¸€çš„â€œæ ‡ç­¾-IDâ€æ˜ å°„è¡¨ã€‚

### 2.1 åŠ è½½æ•°æ®

åœ¨å¤„ç†ä»»ä½•æ•°æ®ä¹‹å‰ï¼Œé¦–è¦éœ€è¦æŠŠå®ƒåŠ è½½åˆ°å†…å­˜é‡Œã€‚

#### 2.1.1 è°ƒè¯•è§‚å¯Ÿæ•°æ®ç»“æ„

å¼€å§‹çš„ä»£ç å¾ˆç®€å•ï¼Œåªæœ‰ä¸€ä¸ªç›®çš„ï¼šè¯»å–æ–‡ä»¶å¹¶åŠ è½½å…¶å†…å®¹ã€‚

```python
import json

def collect_entity_types_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f) # ä¸‹æ–­ç‚¹

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    collect_entity_types_from_file(train_file)
```

**æ“ä½œæŒ‡å¼•**ï¼š

å¦‚ **å›¾ 2.1** æ‰€ç¤ºï¼Œè°ƒè¯•è¿‡ç¨‹åˆ†ä¸ºä¸‰æ­¥ï¼š
1.  **è®¾ç½®æ–­ç‚¹**ï¼šåœ¨ä»£ç è¡Œ `all_data = json.load(f)` å·¦ä¾§çš„è¡Œå·æ—è¾¹å•å‡»ï¼Œè®¾ç½®ä¸€ä¸ªæ–­ç‚¹ã€‚
2.  **å¯åŠ¨è°ƒè¯•**ï¼šç‚¹å‡» PyCharm å³ä¸Šè§’çš„â€œDebugâ€æŒ‰é’®ï¼ˆç»¿è‰²ç”²è™«å›¾æ ‡ï¼‰ï¼Œä»¥è°ƒè¯•æ¨¡å¼è¿è¡Œå½“å‰è„šæœ¬ã€‚ç¨‹åºä¼šè‡ªåŠ¨æ‰§è¡Œåˆ°æ–­ç‚¹æ‰€åœ¨è¡Œå¹¶**æš‚åœ**ï¼Œæ­¤æ—¶ `all_data` å˜é‡è¿˜æœªè¢«èµ‹å€¼ã€‚
3.  **å•æ­¥æ‰§è¡Œ (Step Over)**ï¼šç‚¹å‡»è°ƒè¯•æ§åˆ¶å°ä¸­çš„â€œStep Overâ€æŒ‰é’®ã€‚æ­¤æ“ä½œä¼šæ‰§è¡Œå½“å‰è¡Œä»£ç ã€‚æ‰§è¡Œåï¼Œ`all_data` å˜é‡æ‰ä¼šè¢«æˆåŠŸèµ‹å€¼ã€‚

<div align="center">
  <img src="./images/8_2_1.png" alt="PyCharm è°ƒè¯•å™¨è§‚å¯Ÿæ•°æ®ç»“æ„" />
  <p>å›¾ 2.1: PyCharm è°ƒè¯•å™¨è§‚å¯Ÿæ•°æ®ç»“æ„</p>
</div>

å®Œæˆä»¥ä¸Šæ­¥éª¤åï¼Œå¯ä»¥åœ¨ä¸‹æ–¹çš„â€œDebugâ€å·¥å…·çª—å£ä¸­å±•å¼€ `all_data` å˜é‡ï¼Œä»è€Œå®¡æŸ¥å…¶å†…éƒ¨ç»“æ„ã€‚é€šè¿‡è§‚å¯Ÿ **å›¾ 2.1**ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼š
-   `all_data` æ˜¯ä¸€ä¸ª `list`ï¼ˆåˆ—è¡¨ï¼‰ã€‚
-   åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª `dict`ï¼ˆå­—å…¸ï¼‰ï¼Œä»£è¡¨ä¸€æ¡æ ‡æ³¨æ•°æ®ã€‚
-   æ¯ä¸ªå­—å…¸éƒ½åŒ…å« `text` å’Œ `entities` ä¸¤ä¸ªé”®ã€‚

> ä»¥ä¸Šæ­¥éª¤ä»¥ PyCharm ä¸ºä¾‹ï¼Œä½†å…¶è°ƒè¯•é€»è¾‘ï¼ˆè®¾ç½®æ–­ç‚¹ã€å¯åŠ¨è°ƒè¯•ã€å•æ­¥æ‰§è¡Œï¼‰åœ¨ VS Code ç­‰å…¶ä»–ä¸»æµ IDE ä¸­æ˜¯å®Œå…¨é€šç”¨çš„ã€‚
>
> åˆšåˆšæˆ‘ä»¬é€šè¿‡æ–­ç‚¹è°ƒè¯•ï¼Œæ¸…æ¥šåœ°çœ‹åˆ°äº† `all_data` çš„å†…éƒ¨ç»“æ„ï¼Œè¿™ä¸ºç¼–å†™åç»­çš„éå†ä»£ç æä¾›äº†ä¾æ®ã€‚è¯·è®°ä½è¿™ç§æ–¹æ³•ï¼Œåç»­å­¦ä¹ ä¸­å¦‚æœé‡åˆ°ä»»ä½•ä¸ç†è§£çš„ä»£ç æˆ–ä¸æ¸…æ¥šçš„å˜é‡ï¼Œéƒ½å¯ä»¥ä½¿ç”¨åŒæ ·çš„æ–¹å¼ï¼šâ€œ**å“ªé‡Œä¸ä¼š D å“ªé‡Œ**ğŸ˜‰â€ã€‚

#### 2.1.2 æå–å®ä½“ç±»å‹

æ—¢ç„¶å·²ç»æ¸…æ¥šäº†æ•°æ®ç»“æ„ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯éå†è¿™ä¸ªåˆ—è¡¨ï¼Œä»æ¯ä¸ªå­—å…¸ä¸­æå–å‡ºæˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„ä¿¡æ¯â€”â€”å®ä½“ç±»å‹ã€‚

```python
import json

def collect_entity_types_from_file(file_path):
    types = set()
    with open(file_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
        for data in all_data:
            # éå†å®ä½“åˆ—è¡¨ï¼Œæå– 'type' å­—æ®µ
            for entity in data['entities']:
                types.add(entity['type'])
    return types

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    entity_types = collect_entity_types_from_file(train_file)
    print(f"ä» {train_file} ä¸­æå–çš„å®ä½“ç±»å‹: {entity_types}")
```

è¿è¡Œç»“æœï¼š
```
ä» ./data/CMeEE-V2_train.json ä¸­æå–çš„å®ä½“ç±»å‹: {'dru', 'dep', 'dis', 'bod', 'mic', 'equ', 'sym', 'pro', 'ite'}
```

### 2.2 å¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶ä¿è¯é¡ºåº

ä¸‹ä¸€æ­¥éœ€è¦å®Œæˆä¸¤ä»¶äº‹ï¼š
1.  å¤„ç†æ‰€æœ‰çš„æ•°æ®æ–‡ä»¶ï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†ï¼‰ï¼Œä»¥ç¡®ä¿åŒ…å«äº†å…¨éƒ¨çš„å®ä½“ç±»å‹ã€‚
2.  å¯¹æå–å‡ºçš„å®ä½“ç±»å‹è¿›è¡Œ**æ’åº**ï¼Œä»¥ä¿è¯æ¯æ¬¡ç”Ÿæˆçš„æ ‡ç­¾ ID æ˜ å°„éƒ½æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚

åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯¹ä»£ç è¿›è¡Œæ‰©å±•ï¼š

```python
# (collect_entity_types_from_file å‡½æ•°ä¿æŒä¸å˜ï¼Œæ­¤å¤„çœç•¥)
# ...

def generate_tag_map(data_files):
    all_entity_types = set()
    for file_path in data_files:
        types_in_file = collect_entity_types_from_file(file_path)
        all_entity_types.update(types_in_file)
    
    # æ’åºï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
    sorted_types = sorted(list(all_entity_types))

    # åç»­å°†åœ¨è¿™é‡Œæ„å»º BMES æ˜ å°„
    # ...

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    
    generate_tag_map(data_files=[train_file, dev_file])
```

### 2.3 æ„å»º BMES æ ‡ç­¾æ˜ å°„

æœ‰äº†æ’åºåçš„å®ä½“ç±»å‹åˆ—è¡¨ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ„å»ºæœ€ç»ˆçš„ `tag_to_id` æ˜ å°„å­—å…¸äº†ã€‚è§„åˆ™å¦‚ä¸‹ï¼š
- éå®ä½“æ ‡ç­¾ `'O'` çš„ ID ä¸º `0`ã€‚
- å¯¹äºæ¯ä¸€ç§å®ä½“ç±»å‹ï¼ˆå¦‚ `dis`ï¼‰ï¼Œæˆ‘ä»¬éƒ½ç”Ÿæˆ `B-dis`, `M-dis`, `E-dis`, `S-dis` å››ç§æ ‡ç­¾ï¼Œå¹¶æŒ‰é¡ºåºèµ‹äºˆé€’å¢çš„ IDã€‚

```python
# ... (åœ¨ generate_tag_map å‡½æ•°å†…éƒ¨) ...

# ... (æ±‡æ€»å’Œæ’åºé€»è¾‘) ...
sorted_types = sorted(list(all_entity_types))

# æ„å»º BMES æ ‡ç­¾æ˜ å°„
tag_to_id = {'O': 0}  # 'O' ä»£è¡¨éå®ä½“
for entity_type in sorted_types:
    for prefix in ['B', 'M', 'E', 'S']:
        tag_name = f"{prefix}-{entity_type}"
        tag_to_id[tag_name] = len(tag_to_id)

print(f"\nå·²ç”Ÿæˆ {len(tag_to_id)} ä¸ªæ ‡ç­¾æ˜ å°„ã€‚")
```

### 2.4 å°è£…ä¸ä¿å­˜

æœ€åï¼Œä¸ºäº†è®©è¿™ä¸ªæ˜ å°„è¡¨èƒ½å¤Ÿè¢«å…¶ä»–è„šæœ¬æ–¹ä¾¿åœ°ä½¿ç”¨ï¼Œéœ€è¦å°†å®ƒä¿å­˜æˆä¸€ä¸ª JSON æ–‡ä»¶ã€‚

```python
def save_json(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def collect_entity_types_from_file(file_path):
    # ... (å‡½æ•°å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„çœç•¥)

def generate_tag_map(data_files, output_file): # æ·»åŠ  output_file å‚æ•°
    # 1. æ±‡æ€»æ‰€æœ‰å®ä½“ç±»å‹ ...

    # 2. æ’åºä»¥ä¿è¯æ˜ å°„ä¸€è‡´æ€§ ...

    # 3. æ„å»º BMES æ ‡ç­¾æ˜ å°„ ...

    # 4. ä¿å­˜æ˜ å°„æ–‡ä»¶
    save_json(tag_to_id, output_file)
    print(f"æ ‡ç­¾æ˜ å°„å·²ä¿å­˜è‡³: {output_file}")

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    output_path = './data/categories.json'
    generate_tag_map(data_files=[train_file, dev_file], output_file=output_path)
```

é€šè¿‡è¿™æ ·ä¸€æ­¥æ­¥çš„è¿­ä»£å’Œå®Œå–„ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªåŸºç¡€çš„æ€è·¯ï¼Œæœ€ç»ˆæ„å»ºå‡ºäº†ä¸€ä¸ªå¯å¤ç”¨çš„é¢„å¤„ç†è„šæœ¬ã€‚

### 2.5 è¿è¡Œç»“æœ

æ‰§è¡Œæœ€ç»ˆçš„ `01_build_category.py` è„šæœ¬ï¼Œä¼šç”Ÿæˆ `data/categories.json` æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼ˆéƒ¨åˆ†å±•ç¤ºï¼‰ï¼š

```json
{
    "O": 0,
    "B-bod": 1,
    "M-bod": 2,
    "E-bod": 3,
    "S-bod": 4,
    "B-dep": 5,
    "M-dep": 6,
    "E-dep": 7,
    "S-dep": 8,
    "B-dis": 9,
    "M-dis": 10,
    "E-dis": 11,
    "S-dis": 12,
    ...
}
```

## ä¸‰ã€æ„å»ºè¯æ±‡è¡¨

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªâ€œå­—ç¬¦-IDâ€çš„æ˜ å°„è¡¨ï¼ˆå³è¯æ±‡è¡¨ï¼‰ï¼Œä¸ºåç»­å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—åšå‡†å¤‡ã€‚

### 3.1 ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦

æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡æ˜¯è·å–æ•°æ®ä¸­å‡ºç°çš„æ‰€æœ‰å­—ç¬¦ã€‚`collections.Counter` æ˜¯å®Œæˆè¿™é¡¹ä»»åŠ¡çš„ç»ä½³å·¥å…·ã€‚

```python
from collections import Counter
import json

def create_char_vocab(data_files):
    char_counts = Counter()
    for file_path in data_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
            for data in all_data:
                char_counts.update(list(data['text']))
    
    print(f"åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦ç§ç±»æ•°: {len(char_counts)}")
    print("é¢‘ç‡æœ€é«˜çš„5ä¸ªå­—ç¬¦:", char_counts.most_common(5))
```

### 3.2 æ–‡æœ¬è§„èŒƒåŒ–

åœ¨æ£€æŸ¥åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦æ—¶ï¼Œä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼šæ•°æ®ä¸­å¯èƒ½åŒæ—¶åŒ…å«**å…¨è§’å­—ç¬¦**ï¼ˆå¦‚ `ï¼Œ`ï¼Œ`ï¼ˆ`ï¼‰å’Œ**åŠè§’å­—ç¬¦**ï¼ˆå¦‚ `,`ï¼Œ`(`ï¼‰ã€‚å®ƒä»¬åœ¨è¯­ä¹‰ä¸Šç›¸åŒï¼Œä½†ä¼šè¢«è§†ä¸ºä¸¤ä¸ªä¸åŒçš„ tokenã€‚

ä¸ºäº†å‡å°è¯æ±‡è¡¨è§„æ¨¡å¹¶æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å°†å®ƒä»¬ç»Ÿä¸€ã€‚ä¸€ä¸ªé€šç”¨çš„ç­–ç•¥æ˜¯**å°†æ‰€æœ‰å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦**ã€‚

```python
def normalize_text(text):
    """
    è§„èŒƒåŒ–æ–‡æœ¬ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    """
    full_width = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï¼‚îˆ¶"
    half_width = r"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!#$%&'" + r'()*+,-./:;<=>?@[\]^_`{|}~".'
    mapping = str.maketrans(full_width, half_width)
    return text.translate(mapping)

def create_char_vocab(data_files):
    char_counts = Counter()
    for file_path in data_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
            for data in all_data:
                # åœ¨ç»Ÿè®¡å‰å…ˆè¿›è¡Œè§„èŒƒåŒ–
                normalized_text = normalize_text(data['text'])
                char_counts.update(list(normalized_text))
    
    print(f"åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦ç§ç±»æ•°: {len(char_counts)}")
    print("é¢‘ç‡æœ€é«˜çš„5ä¸ªå­—ç¬¦:", char_counts.most_common(5))
```

### 3.3 è¿‡æ»¤ã€æ’åºä¸æ·»åŠ ç‰¹æ®Šç¬¦

æ¥ä¸‹æ¥ï¼Œè¿›è¡Œæ”¶å°¾å·¥ä½œï¼š
1.  **è¿‡æ»¤ä½é¢‘è¯**ï¼šå¯ä»¥è®¾å®šä¸€ä¸ªé˜ˆå€¼ `min_freq`ï¼Œç§»é™¤å‡ºç°æ¬¡æ•°è¿‡å°‘çš„ç½•è§å­—ï¼Œä»¥è¿›ä¸€æ­¥ç²¾ç®€è¯æ±‡è¡¨ã€‚
2.  **æ’åº**ï¼šä¸æ ‡ç­¾æ˜ å°„ä¸€æ ·ï¼Œå¯¹æœ€ç»ˆçš„å­—ç¬¦åˆ—è¡¨è¿›è¡Œæ’åºï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆçš„è¯æ±‡è¡¨æ–‡ä»¶å†…å®¹å®Œå…¨ä¸€è‡´ã€‚
3.  **æ·»åŠ ç‰¹æ®Š Token**ï¼šåœ¨åˆ—è¡¨çš„æœ€å‰é¢ï¼ŒåŠ å…¥ä¸¤ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼š`<PAD>`ï¼ˆç”¨äºåç»­å¯¹é½åºåˆ—ï¼‰å’Œ `<UNK>`ï¼ˆç”¨äºè¡¨ç¤ºè¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æœªçŸ¥å­—ç¬¦ï¼‰ã€‚

### 3.4 å°è£…ä¸ä¿å­˜

æˆ‘ä»¬å°†ä»¥ä¸Šæ‰€æœ‰é€»è¾‘æ•´åˆï¼Œå¹¶åŠ å…¥ä¿å­˜æ–‡ä»¶çš„åŠŸèƒ½ï¼Œä¾¿å¾—åˆ°äº†æœ€ç»ˆçš„ `02_build_vocabulary.py` è„šæœ¬ã€‚

```python
import json
import os
from collections import Counter


def save_json(data, file_path):
    """
    å°†æ•°æ®ä»¥æ˜“äºé˜…è¯»çš„æ ¼å¼ä¿å­˜ä¸º JSON æ–‡ä»¶ã€‚
    """
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def normalize_text(text):
    """
    è§„èŒƒåŒ–æ–‡æœ¬ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    """
    full_width = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï¼‚îˆ¶"
    half_width = r"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!#$%&'" + r'()*+,-./:;<=>?@[\]^_`{|}~".'
    mapping = str.maketrans(full_width, half_width)
    return text.translate(mapping)

def create_char_vocab(data_files, output_file, min_freq=1):
    # 1. ç»Ÿè®¡è§„èŒƒåŒ–åçš„å­—ç¬¦é¢‘ç‡
    char_counts = Counter()
    for file_path in data_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
            for data in all_data:
                text = normalize_text(data['text'])
                char_counts.update(list(text))

    # 2. è¿‡æ»¤ä½é¢‘è¯
    frequent_chars = [char for char, count in char_counts.items() if count >= min_freq]
    
    # 3. æ’åº
    frequent_chars.sort()

    # 4. æ·»åŠ ç‰¹æ®Šæ ‡è®°
    special_tokens = ["<PAD>", "<UNK>"]
    final_vocab_list = special_tokens + frequent_chars
    
    print(f"è¯æ±‡è¡¨å¤§å° (min_freq={min_freq}): {len(final_vocab_list)}")

    # 5. ä¿å­˜è¯æ±‡è¡¨
    save_json(final_vocab_list, output_file)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    output_path = './data/vocabulary.json'
    create_char_vocab(data_files=[train_file, dev_file], output_file=output_path, min_freq=1)
```

## å››ã€æ­¥éª¤ä¸‰ï¼šå°è£…æ•°æ®åŠ è½½å™¨

> **ç›®æ ‡**ï¼šåˆ©ç”¨å‰ä¸¤æ­¥ç”Ÿæˆçš„æ˜ å°„æ–‡ä»¶ï¼Œå°†åŸå§‹æ•°æ®å½»åº•è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„ã€æ‰¹æ¬¡åŒ–çš„ PyTorch Tensorã€‚
>
> **å¯¹åº”è„šæœ¬**ï¼š`code/C8/03_data_loader.py`

### 4.1 è®¾è®¡æ€è·¯

1.  **é—®é¢˜åˆ†æ**ï¼š
    *   æˆ‘ä»¬ç°åœ¨æœ‰äº†åŸå§‹æ•°æ®ã€è¯æ±‡è¡¨å’Œæ ‡ç­¾æ˜ å°„ï¼Œå¦‚ä½•å°†å®ƒä»¬é«˜æ•ˆåœ°æ•´åˆèµ·æ¥ï¼Ÿ
    *   æ¨¡å‹è®­ç»ƒæ—¶éœ€è¦ä»¥â€œæ‰¹æ¬¡â€ï¼ˆbatchï¼‰ä¸ºå•ä½è¾“å…¥æ•°æ®ï¼Œè€Œä¸æ˜¯å•æ¡æ•°æ®ã€‚
    *   åŒä¸€æ‰¹æ¬¡å†…çš„æ–‡æœ¬é•¿åº¦å¾€å¾€ä¸åŒï¼Œä½†è¾“å…¥æ¨¡å‹çš„ Tensor å¿…é¡»æ˜¯è§„æ•´çš„çŸ©å½¢ï¼Œå¦‚ä½•å¤„ç†ä¸ç­‰é•¿åºåˆ—ï¼Ÿ
2.  **æ ¸å¿ƒé€»è¾‘ï¼ˆé‡‡ç”¨ PyTorch æ ‡å‡†å®è·µï¼‰**ï¼š
    *   **`Vocabulary` ç±»**ï¼šåˆ›å»ºä¸€ä¸ªç±»æ¥å°è£…è¯æ±‡è¡¨åŠ è½½å’Œâ€œtoken-IDâ€è½¬æ¢çš„é€»è¾‘ï¼Œä½¿å…¶æ¸…æ™°ã€å¯å¤ç”¨ã€‚
    *   **`NerDataProcessor` (Dataset) ç±»**ï¼šè¿™æ˜¯æ•°æ®å¤„ç†çš„æ ¸å¿ƒã€‚å®ƒç»§æ‰¿ PyTorch çš„ `Dataset`ï¼Œè´Ÿè´£ï¼š
        *   åœ¨ `__init__` ä¸­åŠ è½½æ‰€æœ‰åŸå§‹æ•°æ®è®°å½•ã€‚
        *   åœ¨ `__getitem__` ä¸­å¤„ç†**å•æ¡**æ•°æ®ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸º `token_ids`ï¼Œå¹¶æ ¹æ®å®ä½“æ ‡æ³¨ç”Ÿæˆ `tag_ids`ã€‚
    *   **`create_ner_dataloader` å·¥å‚å‡½æ•°**ï¼šè¿™ä¸ªå‡½æ•°å°è£…äº†åˆ›å»º `DataLoader` çš„å…¨éƒ¨é€»è¾‘ï¼ŒåŒ…æ‹¬ä¸€ä¸ªéå¸¸å…³é”®çš„å†…éƒ¨å‡½æ•° `collate_batch`ã€‚
    *   **`collate_batch` å‡½æ•°**ï¼šå®ƒè´Ÿè´£è§£å†³ä¸ç­‰é•¿åºåˆ—çš„é—®é¢˜ã€‚å…¶å·¥ä½œåŸç†æ˜¯ï¼š
        *   æ¥æ”¶ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼ˆä¸€ä¸ªç”± `__getitem__` è¿”å›çš„å­—å…¸ç»„æˆçš„åˆ—è¡¨ï¼‰ã€‚
        *   æ‰¾åˆ°å½“å‰æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦ã€‚
        *   ä½¿ç”¨ `pad_sequence` å‡½æ•°ï¼Œå°†æ‰€æœ‰åºåˆ—éƒ½å¡«å……ï¼ˆpadï¼‰åˆ°è¿™ä¸ªæœ€å¤§é•¿åº¦ã€‚å¯¹äº `token_ids` ä½¿ç”¨ `pad_id` (é€šå¸¸æ˜¯0)ï¼Œå¯¹äº `tag_ids` ä½¿ç”¨ `-100`ï¼ˆPyTorch æŸå¤±å‡½æ•°ä¼šå¿½ç•¥è¿™ä¸ªå€¼ï¼‰ã€‚
        *   ç”Ÿæˆä¸€ä¸ª `attention_mask`ï¼Œæ ‡è®°å‡ºå“ªäº›æ˜¯çœŸå® tokenï¼ˆå€¼ä¸º1ï¼‰ï¼Œå“ªäº›æ˜¯å¡«å…… tokenï¼ˆå€¼ä¸º0ï¼‰ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨è®¡ç®—æ—¶å¿½ç•¥å¡«å……éƒ¨åˆ†ã€‚

### 4.2 æ ¸å¿ƒä»£ç å®ç°

```python:code/C8/03_data_loader.py
import json
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence


def normalize_text(text):
    """
    è§„èŒƒåŒ–æ–‡æœ¬ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    """
    full_width = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï¼‚îˆ¶"
    half_width = r"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!#$%&'" + r'()*+,-./:;<=>?@[\]^_`{|}~".'
    mapping = str.maketrans(full_width, half_width)
    return text.translate(mapping)


class Vocabulary:
    """
    è´Ÿè´£ç®¡ç†è¯æ±‡è¡¨å’Œ token åˆ° id çš„æ˜ å°„ã€‚
    """
    def __init__(self, vocab_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.tokens = json.load(f)
        self.token_to_id = {token: i for i, token in enumerate(self.tokens)}
        self.pad_id = self.token_to_id['<PAD>']
        self.unk_id = self.token_to_id['<UNK>']

    def __len__(self):
        return len(self.tokens)

    def convert_tokens_to_ids(self, tokens):
        return [self.token_to_id.get(token, self.unk_id) for token in tokens]


class NerDataProcessor(Dataset):
    """
    å¤„ç† NER æ•°æ®ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé€‚ç”¨äº PyTorch æ¨¡å‹çš„æ ¼å¼ã€‚
    """
    def __init__(self, data_path, vocab: Vocabulary, tag_map: dict):
        self.vocab = vocab
        self.tag_to_id = tag_map
        self.records = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    self.records.append(json.loads(line))
                except json.JSONDecodeError:
                    print(f"è­¦å‘Š: æ— æ³•è§£æè¡Œ: {line}")

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        record = self.records[idx]
        text = normalize_text(record['text'])
        tokens = list(text)
        
        # å°†æ–‡æœ¬ tokens è½¬æ¢ä¸º ids
        token_ids = self.vocab.convert_tokens_to_ids(tokens)

        # åˆå§‹åŒ–æ ‡ç­¾åºåˆ—ä¸º 'O'
        tags = ['O'] * len(tokens)
        for entity in record.get('entities', []):
            entity_type = entity['type']
            start = entity['start_idx']
            end = entity['end_idx'] - 1  # è½¬æ¢ä¸ºåŒ…å«æ¨¡å¼

            if end >= len(tokens): continue

            if start == end:
                tags[start] = f'S-{entity_type}'
            else:
                tags[start] = f'B-{entity_type}'
                tags[end] = f'E-{entity_type}'
                for i in range(start + 1, end):
                    tags[i] = f'M-{entity_type}'
        
        # å°†æ ‡ç­¾è½¬æ¢ä¸º ids
        tag_ids = [self.tag_to_id.get(tag, self.tag_to_id['O']) for tag in tags]

        return {
            "token_ids": torch.tensor(token_ids, dtype=torch.long),
            "tag_ids": torch.tensor(tag_ids, dtype=torch.long)
        }


def create_ner_dataloader(data_path, vocab, tag_map, batch_size, shuffle=False):
    """
    åˆ›å»º NER ä»»åŠ¡çš„ DataLoaderã€‚
    """
    dataset = NerDataProcessor(data_path, vocab, tag_map)
    
    def collate_batch(batch):
        token_ids_list = [item['token_ids'] for item in batch]
        tag_ids_list = [item['tag_ids'] for item in batch]

        padded_token_ids = pad_sequence(token_ids_list, batch_first=True, padding_value=vocab.pad_id)
        padded_tag_ids = pad_sequence(tag_ids_list, batch_first=True, padding_value=-100) # -100 ç”¨äºåœ¨è®¡ç®—æŸå¤±æ—¶å¿½ç•¥å¡«å……éƒ¨åˆ†

        attention_mask = (padded_token_ids != vocab.pad_id).long()

        return padded_token_ids, padded_tag_ids, attention_mask

    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_batch)


if __name__ == '__main__':
    # æ–‡ä»¶è·¯å¾„
    train_file = './data/CMeEE-V2_train.json'
    vocab_file = './data/vocabulary.json'
    categories_file = './data/categories.json'

    # 1. åŠ è½½è¯æ±‡è¡¨å’Œæ ‡ç­¾æ˜ å°„
    vocabulary = Vocabulary(vocab_path=vocab_file)
    with open(categories_file, 'r', encoding='utf-8') as f:
        tag_map = json.load(f)
    print("è¯æ±‡è¡¨å’Œæ ‡ç­¾æ˜ å°„åŠ è½½å®Œæˆã€‚")

    # 2. åˆ›å»º DataLoader
    train_loader = create_ner_dataloader(
        data_path=train_file,
        vocab=vocabulary,
        tag_map=tag_map,
        batch_size=4,
        shuffle=True
    )
    print("DataLoader åˆ›å»ºå®Œæˆã€‚")

    # 3. éªŒè¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    print("\n--- éªŒè¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ® ---")
    tokens, labels, mask = next(iter(train_loader))
    
    print(f"  Token IDs (shape): {tokens.shape}")
    print(f"  Label IDs (shape): {labels.shape}")
    print(f"  Attention Mask (shape): {mask.shape}")
    print(f"  Token IDs (sample): {tokens[0][:20]}...")
    print(f"  Label IDs (sample): {labels[0][:20]}...")
    print(f"  Attention Mask (sample): {mask[0][:20]}...")
```

### 4.3 è¿è¡ŒéªŒè¯

æ‰§è¡Œ `03_data_loader.py` è„šæœ¬ä¼šå®Œæ•´åœ°åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ‰¹æ¬¡æ•°æ®çš„å½¢çŠ¶å’Œç¤ºä¾‹ï¼ŒéªŒè¯æ•´ä¸ªæ•°æ®åŠ è½½æµç¨‹çš„æ­£ç¡®æ€§ã€‚

```
è¯æ±‡è¡¨å’Œæ ‡ç­¾æ˜ å°„åŠ è½½å®Œæˆã€‚
DataLoader åˆ›å»ºå®Œæˆã€‚

--- éªŒè¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ® ---
  Token IDs (shape): torch.Size([4, 152])
  Label IDs (shape): torch.Size([4, 152])
  Attention Mask (shape): torch.Size([4, 152])
  ...
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡ä¸‰ä¸ªç‹¬ç«‹çš„ã€æµç¨‹åŒ–çš„è„šæœ¬ï¼Œå®Œæˆäº†ä»åŸå§‹ JSON æ•°æ®åˆ°æ¨¡å‹å¯ç”¨çš„ã€æ‰¹æ¬¡åŒ–çš„ PyTorch Tensor çš„å…¨éƒ¨è½¬æ¢å·¥ä½œã€‚

---

> å®Œæ•´ä»£ç è¯·å‚è€ƒï¼š[GitHub ä»“åº“](https://github.com/FutureUnreal/base-nlp/tree/main/code/C8)