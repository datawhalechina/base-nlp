# ç¬¬äºŒèŠ‚ NER é¡¹ç›®çš„æ•°æ®å¤„ç†

åœ¨ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬ç®€å•äº†è§£äº†å‘½åå®ä½“è¯†åˆ«çš„ä»»åŠ¡å®šä¹‰ã€åº”ç”¨åœºæ™¯åŠä¸»æµå®ç°æ–¹æ³•ã€‚æœ¬èŠ‚å°†æ­£å¼è¿›å…¥ç¼–ç é˜¶æ®µï¼Œä»æ•°æ®å¤„ç†å¼€å§‹ï¼Œé€æ­¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„ NER é¡¹ç›®ã€‚ä¸ºäº†æ¸…æ™°åœ°æ„å»º NER çš„å¤„ç†æµç¨‹ï¼Œæˆ‘ä»¬é‡‡ç”¨æµç¨‹åŒ–çš„ä»£ç ç»„ç»‡æ€è·¯ï¼Œå°†æ•´ä¸ªæµç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„è„šæœ¬ã€‚

> [æœ¬èŠ‚å®Œæ•´ä»£ç ](https://github.com/datawhalechina/base-nlp/tree/main/code/C8)

## ä¸€ã€æ•°æ®å¤„ç†æµç¨‹æ€»è§ˆ

åœ¨ NLP ä¸­ï¼ŒåŸå§‹çš„æ–‡æœ¬å’Œæ ‡æ³¨æ•°æ®æ˜¯æ— æ³•ç›´æ¥è¢«ç¥ç»ç½‘ç»œæ¨¡å‹åˆ©ç”¨çš„ã€‚éœ€è¦å°†è¿™äº›åŸå§‹æ•°æ®è½¬æ¢æˆæ¨¡å‹èƒ½å¤Ÿç†è§£çš„ã€æ ‡å‡†åŒ–çš„æ•°å­—å¼ é‡ã€‚é‚£ä¹ˆï¼Œå…·ä½“è¦è½¬æ¢æˆä»€ä¹ˆæ ·ï¼Ÿåˆè¯¥å¦‚ä½•è½¬æ¢ï¼Ÿè¿™å°±æ˜¯æœ¬èŠ‚æ•°æ®å¤„ç†æµç¨‹è¦è§£å†³çš„é—®é¢˜ã€‚

### 1.1 æ˜ç¡®æ•°æ®å¤„ç†çš„ç›®æ ‡

åœ¨è®¾è®¡ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ˜ç¡®æœ€ç»ˆçš„ç›®æ ‡ã€‚å¯¹äºä¸€ä¸ªå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ•°æ®å¤„ç†éœ€è¦äº§å‡ºä»€ä¹ˆï¼Ÿ

1.  **æ¨¡å‹çš„è¾“å…¥ (X) æ˜¯ä»€ä¹ˆï¼Ÿ**
    -   å®ƒåº”è¯¥æ˜¯ä¸€ä¸ªæ•´æ•°å¼ é‡ï¼Œå½¢çŠ¶ä¸º `[batch_size, seq_len]`ã€‚
    -   å…¶ä¸­ `batch_size` æ˜¯æ‰¹æ¬¡å¤§å°ï¼Œ`seq_len` æ˜¯åºåˆ—é•¿åº¦ï¼ˆé€šå¸¸æ˜¯æ‰¹æ¬¡å†…æœ€é•¿å¥å­çš„é•¿åº¦ï¼‰ã€‚
    -   å¼ é‡ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œéƒ½ä»£è¡¨åŸå§‹å¥å­ä¸­ä¸€ä¸ªå­—ç¬¦ï¼ˆTokenï¼‰åœ¨è¯æ±‡è¡¨é‡Œå¯¹åº”çš„å”¯ä¸€ IDã€‚

2.  **æ¨¡å‹çš„æ ‡ç­¾ (Y) æ˜¯ä»€ä¹ˆï¼Ÿ**
    -   å®ƒä¹Ÿåº”è¯¥æ˜¯ä¸€ä¸ªæ•´æ•°å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ X å®Œå…¨ç›¸åŒï¼Œå³ `[batch_size, seq_len]`ã€‚
    -   å…¶ä¸­çš„æ¯ä¸€ä¸ªæ•°å­—ï¼Œä»£è¡¨ç€å¯¹åº”ä½ç½®å­—ç¬¦çš„å®ä½“æ ‡ç­¾ IDï¼ˆä¾‹å¦‚ï¼Œ`B-bod` å¯¹åº”çš„ IDï¼‰ã€‚

3.  **å¦‚ä½•å®ç°ä»â€œæ–‡æœ¬â€åˆ°â€œIDâ€çš„è½¬æ¢ï¼Ÿ**
    -   **æ–‡æœ¬ -> Token ID**ï¼šéœ€è¦æ„å»ºä¸€ä¸ª â€œå­—ç¬¦-IDâ€ çš„æ˜ å°„è¡¨ï¼Œä¹Ÿå°±æ˜¯**è¯æ±‡è¡¨ (Vocabulary)**ã€‚
    -   **å®ä½“ -> æ ‡ç­¾ ID**ï¼šéœ€è¦æ„å»ºä¸€ä¸ª â€œæ ‡ç­¾-IDâ€ çš„æ˜ å°„è¡¨ã€‚

### 1.2 æ•°æ®æ ¼å¼è§£æ

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `CMeEE-V2`ï¼ˆä¸­æ–‡åŒ»å­¦å®ä½“æŠ½å–ï¼‰æ•°æ®é›†ã€‚ç»è¿‡åˆ†æï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨çš„æ˜¯æ ‡å‡†çš„ **JSON æ•°ç»„** æ ¼å¼ã€‚

#### 1.2.1 åŸå§‹æ•°æ®ç¤ºä¾‹

æ‰“å¼€ `CMeEE-V2_train.json`ï¼Œå¯ä»¥çœ‹åˆ°æ–‡ä»¶å†…å®¹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼š

```json
[
  ...,
  {
    "text": "ï¼ˆ2ï¼‰å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿå¯ç”¨å¸¸è§„æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©æ§åˆ¶ï¼Œå¹´é¾„å°äº5å²ã€‚",
    "entities": [
      {
        "start_idx": 3,
        "end_idx": 9,
        "type": "dis",
        "entity": "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"
      },
      {
        "start_idx": 14,
        "end_idx": 20,
        "type": "dru",
        "entity": "æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©"
      }
    ]
  },
  ...
]
```

#### 1.2.2 å­—æ®µè¯´æ˜

-   **`text`**ï¼šåŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
-   **`entities`**ï¼šå®ä½“æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«ï¼š
    -   `start_idx`ï¼šå®ä½“èµ·å§‹ä½ç½®ï¼ˆ**åŒ…å«**ï¼‰
    -   `end_idx`ï¼šå®ä½“ç»“æŸä½ç½®ï¼ˆ**åŒ…å«**ï¼‰
    -   `type`ï¼šå®ä½“ç±»å‹ï¼ˆå¦‚ `dis` ç–¾ç—…ã€`dru` è¯ç‰©ï¼‰
    -   `entity`ï¼šå®ä½“æ–‡æœ¬ï¼ˆç”¨äºéªŒè¯ï¼‰

> **ç´¢å¼•çš„åŒ…å«æ€§**
>
> å¯¹äºå½“å‰ `data/` ç›®å½•ä¸‹çš„æ•°æ®ï¼Œç»å®æµ‹ï¼š`start_idx` ä¸ `end_idx` å‡ä¸º**åŒ…å«**ï¼ˆé—­åŒºé—´ï¼‰ã€‚å®ä½“åº”ç”± `text[start_idx : end_idx + 1]` å–å¾—ã€‚ä¾‹å¦‚ï¼š
> - æ–‡æœ¬ï¼š"ï¼ˆ2ï¼‰å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿå¯ç”¨å¸¸è§„æŠ—å¿ƒå¾‹å¤±å¸¸è¯ç‰©æ§åˆ¶ï¼Œå¹´é¾„å°äº5å²ã€‚"
> - å®ä½“ "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"ï¼š`start_idx=3, end_idx=9`
> - å®é™…å­—ç¬¦ï¼š`text[3:10]` = "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ"
>
> æ‰€ä»¥ï¼Œå®ä½“é•¿åº¦ = `end_idx - start_idx + 1`ã€‚

## äºŒã€æ„å»ºæ ‡ç­¾æ˜ å°„

> **ç›®æ ‡**ï¼šä»åŸå§‹æ•°æ®ä¸­æå–æ‰€æœ‰å®ä½“ç±»å‹ï¼Œç„¶ååŸºäº `BMES` æ ‡æ³¨æ–¹æ¡ˆæ„å»ºä¸€ä¸ªå…¨å±€ç»Ÿä¸€çš„â€œæ ‡ç­¾-IDâ€æ˜ å°„è¡¨ã€‚

### 2.1 åŠ è½½æ•°æ®

åœ¨å¤„ç†ä»»ä½•æ•°æ®ä¹‹å‰ï¼Œé¦–è¦éœ€è¦æŠŠå®ƒåŠ è½½åˆ°å†…å­˜é‡Œã€‚

#### 2.1.1 è°ƒè¯•è§‚å¯Ÿæ•°æ®ç»“æ„

å¼€å§‹çš„ä»£ç å¾ˆç®€å•ï¼Œåªæœ‰ä¸€ä¸ªç›®çš„ï¼šè¯»å–æ–‡ä»¶å¹¶åŠ è½½å…¶å†…å®¹ã€‚

```python
import json

def collect_entity_types_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f) # ä¸‹æ–­ç‚¹

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    collect_entity_types_from_file(train_file)
```

**æ“ä½œæŒ‡å¼•**ï¼š

å¦‚ **å›¾ 2.1** æ‰€ç¤ºï¼Œè°ƒè¯•è¿‡ç¨‹åˆ†ä¸ºä¸‰æ­¥ï¼š
1.  **è®¾ç½®æ–­ç‚¹**ï¼šåœ¨ä»£ç è¡Œ `all_data = json.load(f)` å·¦ä¾§çš„è¡Œå·æ—è¾¹å•å‡»ï¼Œè®¾ç½®ä¸€ä¸ªæ–­ç‚¹ã€‚
2.  **å¯åŠ¨è°ƒè¯•**ï¼šç‚¹å‡» PyCharm å³ä¸Šè§’çš„â€œDebugâ€æŒ‰é’®ï¼ˆç»¿è‰²ç”²è™«å›¾æ ‡ï¼‰ï¼Œä»¥è°ƒè¯•æ¨¡å¼è¿è¡Œå½“å‰è„šæœ¬ã€‚ç¨‹åºä¼šè‡ªåŠ¨æ‰§è¡Œåˆ°æ–­ç‚¹æ‰€åœ¨è¡Œå¹¶ **æš‚åœ**ï¼Œæ­¤æ—¶ `all_data` å˜é‡è¿˜æœªè¢«èµ‹å€¼ã€‚
3.  **å•æ­¥æ‰§è¡Œ (Step Over)**ï¼šç‚¹å‡»è°ƒè¯•æ§åˆ¶å°ä¸­çš„â€œStep Overâ€æŒ‰é’®ã€‚æ­¤æ“ä½œä¼šæ‰§è¡Œå½“å‰è¡Œä»£ç ã€‚æ‰§è¡Œåï¼Œ`all_data` å˜é‡æ‰ä¼šè¢«æˆåŠŸèµ‹å€¼ã€‚

<div align="center">
  <img src="./images/8_2_1.png" alt="PyCharm è°ƒè¯•å™¨è§‚å¯Ÿæ•°æ®ç»“æ„" />
  <p>å›¾ 2.1: PyCharm è°ƒè¯•å™¨è§‚å¯Ÿæ•°æ®ç»“æ„</p>
</div>

å®Œæˆä»¥ä¸Šæ­¥éª¤åï¼Œå¯ä»¥åœ¨ä¸‹æ–¹çš„â€œDebugâ€å·¥å…·çª—å£ä¸­å±•å¼€ `all_data` å˜é‡ï¼Œä»è€Œå®¡æŸ¥å…¶å†…éƒ¨ç»“æ„ã€‚é€šè¿‡è§‚å¯Ÿ **å›¾ 2.1**ï¼Œå¯ä»¥å¾—å‡ºç»“è®ºï¼š
-   `all_data` æ˜¯ä¸€ä¸ª `list`ï¼ˆåˆ—è¡¨ï¼‰ã€‚
-   åˆ—è¡¨ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª `dict`ï¼ˆå­—å…¸ï¼‰ï¼Œä»£è¡¨ä¸€æ¡æ ‡æ³¨æ•°æ®ã€‚
-   æ¯ä¸ªå­—å…¸éƒ½åŒ…å« `text` å’Œ `entities` ä¸¤ä¸ªé”®ã€‚

> ä»¥ä¸Šæ­¥éª¤ä»¥ PyCharm ä¸ºä¾‹ï¼Œä½†å…¶è°ƒè¯•é€»è¾‘ï¼ˆè®¾ç½®æ–­ç‚¹ã€å¯åŠ¨è°ƒè¯•ã€å•æ­¥æ‰§è¡Œï¼‰åœ¨ VS Code ç­‰å…¶ä»–ä¸»æµ IDE ä¸­æ˜¯å®Œå…¨é€šç”¨çš„ã€‚
>
> åˆšåˆšæˆ‘ä»¬é€šè¿‡æ–­ç‚¹è°ƒè¯•ï¼Œæ¸…æ¥šåœ°çœ‹åˆ°äº† `all_data` çš„å†…éƒ¨ç»“æ„ï¼Œè¿™ä¸ºç¼–å†™åç»­çš„éå†ä»£ç æä¾›äº†ä¾æ®ã€‚è¯·è®°ä½è¿™ç§æ–¹æ³•ï¼Œåç»­å­¦ä¹ ä¸­å¦‚æœé‡åˆ°ä»»ä½•ä¸ç†è§£çš„ä»£ç æˆ–ä¸æ¸…æ¥šçš„å˜é‡ï¼Œéƒ½å¯ä»¥ä½¿ç”¨åŒæ ·çš„æ–¹å¼ï¼šâ€œ**å“ªé‡Œä¸ä¼š D å“ªé‡Œ**ğŸ˜‰â€ã€‚

#### 2.1.2 æå–å®ä½“ç±»å‹

æ—¢ç„¶å·²ç»æ¸…æ¥šäº†æ•°æ®ç»“æ„ï¼Œç°åœ¨è¦åšçš„å°±æ˜¯éå†è¿™ä¸ªåˆ—è¡¨ï¼Œä»æ¯ä¸ªå­—å…¸ä¸­æå–å‡ºæˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„ä¿¡æ¯â€”â€”å®ä½“ç±»å‹ã€‚

```python
import json

def collect_entity_types_from_file(file_path):
    types = set()
    with open(file_path, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
        for data in all_data:
            # éå†å®ä½“åˆ—è¡¨ï¼Œæå– 'type' å­—æ®µ
            for entity in data['entities']:
                types.add(entity['type'])
    return types

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    entity_types = collect_entity_types_from_file(train_file)
    print(f"ä» {train_file} ä¸­æå–çš„å®ä½“ç±»å‹: {entity_types}")
```

è¿è¡Œç»“æœï¼š
```
ä» ./data/CMeEE-V2_train.json ä¸­æå–çš„å®ä½“ç±»å‹: {'dru', 'dep', 'dis', 'bod', 'mic', 'equ', 'sym', 'pro', 'ite'}
```

### 2.2 å¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶ä¿è¯é¡ºåº

ä¸‹ä¸€æ­¥éœ€è¦å®Œæˆä¸¤ä»¶äº‹ï¼š
1.  å¤„ç†æ‰€æœ‰çš„æ•°æ®æ–‡ä»¶ï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†ï¼‰ï¼Œä»¥ç¡®ä¿åŒ…å«äº†å…¨éƒ¨çš„å®ä½“ç±»å‹ã€‚
2.  å¯¹æå–å‡ºçš„å®ä½“ç±»å‹è¿›è¡Œ**æ’åº**ï¼Œä»¥ä¿è¯æ¯æ¬¡ç”Ÿæˆçš„æ ‡ç­¾ ID æ˜ å°„éƒ½æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚

åŸºäºæ­¤ï¼Œå¯¹ä»£ç è¿›è¡Œæ‰©å±•ï¼š

```python
# (collect_entity_types_from_file å‡½æ•°ä¿æŒä¸å˜ï¼Œæ­¤å¤„çœç•¥)
# ...

def generate_tag_map(data_files):
    all_entity_types = set()
    for file_path in data_files:
        types_in_file = collect_entity_types_from_file(file_path)
        all_entity_types.update(types_in_file)
    
    # æ’åºï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
    sorted_types = sorted(list(all_entity_types))

    # åç»­å°†åœ¨è¿™é‡Œæ„å»º BMES æ˜ å°„
    # ...

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    
    generate_tag_map(data_files=[train_file, dev_file])
```

### 2.3 æ„å»º BMES æ ‡ç­¾æ˜ å°„

æœ‰äº†æ’åºåçš„å®ä½“ç±»å‹åˆ—è¡¨ï¼Œå°±å¯ä»¥æ„å»ºæœ€ç»ˆçš„ `tag_to_id` æ˜ å°„å­—å…¸äº†ã€‚è§„åˆ™å¦‚ä¸‹ï¼š
- éå®ä½“æ ‡ç­¾ `'O'` çš„ ID ä¸º `0`ã€‚
- å¯¹äºæ¯ä¸€ç§å®ä½“ç±»å‹ï¼ˆå¦‚ `dis`ï¼‰ï¼Œéƒ½ç”Ÿæˆ `B-dis`, `M-dis`, `E-dis`, `S-dis` å››ç§æ ‡ç­¾ï¼Œå¹¶æŒ‰é¡ºåºèµ‹äºˆé€’å¢çš„ IDã€‚

```python
# ... (åœ¨ generate_tag_map å‡½æ•°å†…éƒ¨) ...

# ... (æ±‡æ€»å’Œæ’åºé€»è¾‘) ...
sorted_types = sorted(list(all_entity_types))

# æ„å»º BMES æ ‡ç­¾æ˜ å°„
tag_to_id = {'O': 0}  # 'O' ä»£è¡¨éå®ä½“
for entity_type in sorted_types:
    for prefix in ['B', 'M', 'E', 'S']:
        tag_name = f"{prefix}-{entity_type}"
        tag_to_id[tag_name] = len(tag_to_id)

print(f"\nå·²ç”Ÿæˆ {len(tag_to_id)} ä¸ªæ ‡ç­¾æ˜ å°„ã€‚")
```

### 2.4 å°è£…ä¸ä¿å­˜

ä¸ºäº†è®©è¿™ä¸ªæ˜ å°„è¡¨èƒ½å¤Ÿè¢«å…¶ä»–è„šæœ¬æ–¹ä¾¿åœ°ä½¿ç”¨ï¼Œéœ€è¦å°†å®ƒä¿å­˜æˆä¸€ä¸ª JSON æ–‡ä»¶ã€‚

```python
def save_json(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def collect_entity_types_from_file(file_path):
    # ... (å‡½æ•°å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„çœç•¥)

def generate_tag_map(data_files, output_file): # æ·»åŠ  output_file å‚æ•°
    # 1. æ±‡æ€»æ‰€æœ‰å®ä½“ç±»å‹ ...

    # 2. æ’åºä»¥ä¿è¯æ˜ å°„ä¸€è‡´æ€§ ...

    # 3. æ„å»º BMES æ ‡ç­¾æ˜ å°„ ...

    # 4. ä¿å­˜æ˜ å°„æ–‡ä»¶
    save_json(tag_to_id, output_file)
    print(f"æ ‡ç­¾æ˜ å°„å·²ä¿å­˜è‡³: {output_file}")

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    output_path = './data/categories.json'
    generate_tag_map(data_files=[train_file, dev_file], output_file=output_path)
```

é€šè¿‡è¿™æ ·ä¸€æ­¥æ­¥çš„è¿­ä»£å’Œå®Œå–„ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªåŸºç¡€çš„æ€è·¯ï¼Œæœ€ç»ˆæ„å»ºå‡ºäº†ä¸€ä¸ªå¯å¤ç”¨çš„é¢„å¤„ç†è„šæœ¬ã€‚

### 2.5 è¿è¡Œç»“æœ

æ‰§è¡Œæœ€ç»ˆçš„ `01_build_category.py` è„šæœ¬ï¼Œä¼šç”Ÿæˆ `categories.json` æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼ˆéƒ¨åˆ†å±•ç¤ºï¼‰ï¼š

```json
{
    "O": 0,
    "B-bod": 1,
    "M-bod": 2,
    "E-bod": 3,
    "S-bod": 4,
    "B-dep": 5,
    "M-dep": 6,
    "E-dep": 7,
    "S-dep": 8,
    "B-dis": 9,
    "M-dis": 10,
    "E-dis": 11,
    "S-dis": 12,
    ...
}
```

## ä¸‰ã€æ„å»ºè¯æ±‡è¡¨

æœ‰äº†æ ‡ç­¾æ˜ å°„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªâ€œå­—ç¬¦-IDâ€çš„æ˜ å°„è¡¨ï¼ˆå³è¯æ±‡è¡¨ï¼‰ï¼Œä¸ºåç»­å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—åšå‡†å¤‡ã€‚

### 3.1 ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦

ç›®å‰çš„é¦–è¦ä»»åŠ¡æ˜¯è·å–æ•°æ®ä¸­å‡ºç°çš„æ‰€æœ‰å­—ç¬¦ã€‚

```python
from collections import Counter
import json

def create_char_vocab(data_files):
    char_counts = Counter()
    with open(data_files, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
        for data in all_data:
            char_counts.update(list(data['text']))
    
    print(f"åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦ç§ç±»æ•°: {len(char_counts)}")

if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    create_char_vocab(train_file)
```

### 3.2 æ–‡æœ¬è§„èŒƒåŒ–

åœ¨æ£€æŸ¥åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦æ—¶ï¼Œä¼šå‘ç°ä¸€ä¸ªé—®é¢˜ï¼šæ•°æ®ä¸­å¯èƒ½åŒæ—¶åŒ…å« **å…¨è§’å­—ç¬¦**ï¼ˆå¦‚ `ï¼Œ`ï¼Œ`ï¼ˆ`ï¼‰å’Œ **åŠè§’å­—ç¬¦**ï¼ˆå¦‚ `,`ï¼Œ`(`ï¼‰ã€‚å®ƒä»¬åœ¨è¯­ä¹‰ä¸Šç›¸åŒï¼Œä½†ä¼šè¢«è§†ä¸ºä¸¤ä¸ªä¸åŒçš„ tokenï¼ˆå¦‚å›¾ 2.2 æ‰€ç¤ºï¼‰ã€‚

<div align="center">
  <img src="./images/8_2_2.png" alt="è°ƒè¯•å™¨ä¸­æ˜¾ç¤ºçš„å­—ç¬¦é¢‘ç‡"/>
  <p>å›¾ 2.2: å…¨è§’/åŠè§’å­—ç¬¦æ··ç”¨</p>
</div>

ä¸ºäº†å‡å°è¯æ±‡è¡¨è§„æ¨¡å¹¶æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥å°†å®ƒä»¬ç»Ÿä¸€ã€‚ä¸€ä¸ªé€šç”¨çš„ç­–ç•¥æ˜¯ **å°†æ‰€æœ‰å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦**ã€‚

```python
def normalize_text(text):
    """
    è§„èŒƒåŒ–æ–‡æœ¬ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    """
    full_width = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï¼‚îˆ¶"
    half_width = r"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!#$%&'" + r'()*+,-./:;<=>?@[\]^_`{|}~".'
    mapping = str.maketrans(full_width, half_width)
    return text.translate(mapping)

def create_char_vocab(data_files):
    char_counts = Counter()
    with open(data_files, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
        for data in all_data:
            # åœ¨ç»Ÿè®¡å‰å…ˆè¿›è¡Œè§„èŒƒåŒ–
            normalized_text = normalize_text(data['text'])
            char_counts.update(list(normalized_text))
    
    print(f"åˆæ­¥ç»Ÿè®¡çš„å­—ç¬¦ç§ç±»æ•°: {len(char_counts)}")
```

### 3.3 è¿‡æ»¤ã€æ’åºä¸æ·»åŠ ç‰¹æ®Šç¬¦

æ¥ä¸‹æ¥ï¼Œè¿›è¡Œæ”¶å°¾å·¥ä½œï¼š
1.  **è¿‡æ»¤ä½é¢‘è¯**ï¼šå¯ä»¥è®¾å®šä¸€ä¸ªé˜ˆå€¼ `min_freq`ï¼Œç§»é™¤å‡ºç°æ¬¡æ•°è¿‡å°‘çš„ç½•è§å­—ï¼Œä»¥è¿›ä¸€æ­¥ç²¾ç®€è¯æ±‡è¡¨ã€‚
2.  **æ’åº**ï¼šä¸æ ‡ç­¾æ˜ å°„ä¸€æ ·ï¼Œå¯¹æœ€ç»ˆçš„å­—ç¬¦åˆ—è¡¨è¿›è¡Œæ’åºï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆçš„è¯æ±‡è¡¨æ–‡ä»¶å†…å®¹å®Œå…¨ä¸€è‡´ã€‚
3.  **æ·»åŠ ç‰¹æ®Š Token**ï¼šåœ¨åˆ—è¡¨çš„æœ€å‰é¢ï¼ŒåŠ å…¥ä¸¤ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼š`<PAD>`ï¼ˆç”¨äºåç»­å¯¹é½åºåˆ—ï¼‰å’Œ `<UNK>`ï¼ˆç”¨äºè¡¨ç¤ºè¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æœªçŸ¥å­—ç¬¦ï¼‰ã€‚

### 3.4 å°è£…ä¸ä¿å­˜

å°†ä»¥ä¸Šæ‰€æœ‰é€»è¾‘æ•´åˆï¼Œå¹¶åŠ å…¥ä¿å­˜æ–‡ä»¶çš„åŠŸèƒ½ï¼Œä¾¿å¾—åˆ°äº†æœ€ç»ˆçš„è„šæœ¬ã€‚

```python
 # ... 

def save_json(data, file_path):
    # ... (å‡½æ•°ä¸ä¸Šä¸ªè„šæœ¬ä¸­ç›¸åŒï¼Œæ­¤å¤„çœç•¥)

def normalize_text(text):
    # ... (å‡½æ•°å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„çœç•¥)

def create_char_vocab(data_files, output_file, min_freq=1):
    # 1. ç»Ÿè®¡è§„èŒƒåŒ–åçš„å­—ç¬¦é¢‘ç‡
    char_counts = Counter()
    for file_path in data_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
            for data in all_data:
                text = normalize_text(data['text'])
                char_counts.update(list(text))

    # 2. è¿‡æ»¤ä½é¢‘è¯
    frequent_chars = [char for char, count in char_counts.items() if count >= min_freq]
    
    # 3. æ’åº
    frequent_chars.sort()

    # 4. æ·»åŠ ç‰¹æ®Šæ ‡è®°
    special_tokens = ["<PAD>", "<UNK>"]
    final_vocab_list = special_tokens + frequent_chars
    
    print(f"è¯æ±‡è¡¨å¤§å° (min_freq={min_freq}): {len(final_vocab_list)}")

    # 5. ä¿å­˜è¯æ±‡è¡¨
    save_json(final_vocab_list, output_file)
    print(f"è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    train_file = './data/CMeEE-V2_train.json'
    dev_file = './data/CMeEE-V2_dev.json'
    output_path = './data/vocabulary.json'
    create_char_vocab(data_files=[train_file, dev_file], output_file=output_path, min_freq=1)
```

## å››ã€å°è£…æ•°æ®åŠ è½½å™¨

ç°åœ¨æœ‰äº†æ ‡ç­¾æ˜ å°„å’Œè¯æ±‡è¡¨ï¼Œæœ€åä¸€æ­¥å°±æ˜¯æ„å»ºä¸€ä¸ªå¯å¤ç”¨çš„ `DataLoader`ï¼Œå°†æ–‡æœ¬æ•°æ®é«˜æ•ˆåœ°è½¬æ¢æˆ PyTorch æ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ ¼å¼ã€‚ç›´æ¥ç”¨å¾ªç¯è¯»å–æ•°æ®å¹¶æ‰‹åŠ¨è½¬æ¢æ˜¯ä½æ•ˆä¸”ä¸çµæ´»çš„ã€‚ä¸€ä¸ªåˆæ ¼çš„æ•°æ®åŠ è½½å™¨éœ€è¦è§£å†³**è‡ªåŠ¨æ‰¹é‡åŒ–**ã€**åºåˆ—å¡«å……**ã€**æ•°æ®è½¬æ¢**å’Œ**éšæœºåŒ–**è¿™å‡ ä¸ªé—®é¢˜ã€‚

æ‰€ä»¥æˆ‘ä»¬å°†æ•´ä¸ªæµç¨‹æ‹†åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ­¥éª¤æ¥é€æ­¥å®ç°ï¼š
-   **æ­¥éª¤ä¸€ï¼šå°è£… `Vocabulary` ç±»**ï¼Œä¸“é—¨è´Ÿè´£ Token å’Œ ID ä¹‹é—´çš„è½¬æ¢ã€‚
-   **æ­¥éª¤äºŒï¼šåˆ›å»º `NerDataset`**ï¼Œç»§æ‰¿è‡ª PyTorch çš„ `Dataset`ï¼Œè´Ÿè´£å¤„ç†å•ä¸ªæ•°æ®æ ·æœ¬çš„è½¬æ¢ã€‚
-   **æ­¥éª¤ä¸‰ï¼šå®šä¹‰ `collate_fn` å‡½æ•°**ï¼Œè´Ÿè´£å°†å¤šä¸ªæ ·æœ¬æ‰“åŒ…ã€å¡«å……æˆä¸€ä¸ª batchã€‚
-   **æ­¥éª¤å››ï¼šæ•´åˆæ‰€æœ‰ç»„ä»¶**ï¼Œåˆ›å»ºä¸€ä¸ª `DataLoader` å®ä¾‹å¹¶è¿›è¡Œæµ‹è¯•ã€‚

### 4.1 å°è£… Vocabulary ç±»

ç¬¬ä¸€æ­¥ï¼Œåˆ›å»ºä¸€ä¸ª `Vocabulary` ç±»æ¥åŠ è½½ä¹‹å‰ç”Ÿæˆçš„ `vocabulary.json`ï¼Œå¹¶æä¾›æ–¹ä¾¿çš„æŸ¥è¯¢æ¥å£ã€‚è¿™ä¸ªç±»ä¸»è¦è´Ÿè´£ Token å’Œ ID ä¹‹é—´çš„è½¬æ¢ã€‚

```python
import json

class Vocabulary:
    """
    è´Ÿè´£ç®¡ç†è¯æ±‡è¡¨å’Œ token åˆ° id çš„æ˜ å°„ã€‚
    """
    def __init__(self, vocab_path):
        with open(vocab_path, 'r', encoding='utf-8') as f:
            self.tokens = json.load(f)
        self.token_to_id = {token: i for i, token in enumerate(self.tokens)}
        self.pad_id = self.token_to_id['<PAD>']
        self.unk_id = self.token_to_id['<UNK>']

    def __len__(self):
        return len(self.tokens)

    def convert_tokens_to_ids(self, tokens):
        return [self.token_to_id.get(token, self.unk_id) for token in tokens]

if __name__ == '__main__':
    vocab_file = './data/vocabulary.json'
    vocabulary = Vocabulary(vocab_path=vocab_file)
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vocabulary)}")
```

### 4.2 åˆ›å»º NerDataset

ç°åœ¨è¦åˆ›å»ºçš„æ˜¯æ ¸å¿ƒçš„æ•°æ®é›†ç±»ï¼Œå®ƒç»§æ‰¿äº† `torch.utils.data.Dataset`ã€‚è´Ÿè´£å°†å•æ¡åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„ `token_ids` å’Œ `label_ids`ã€‚å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªæ•°æ®å¤„ç†çš„â€œå•ä»¶å·¥å‚â€ï¼Œ`DataLoader` æ¯æ¬¡éœ€è¦æ•°æ®æ—¶ï¼Œéƒ½ä¼šå‘è¿™ä¸ªå·¥å‚ç´¢è¦ä¸€ä»¶ï¼ˆ`__getitem__`ï¼‰åŠ å·¥å¥½çš„äº§å“ã€‚

```python
# ... 
from torch.utils.data import Dataset
# ... (éœ€è¦ normalize_text å‡½æ•°) ...

class Vocabulary:
    # ... (ç±»å·²åœ¨å‰é¢å®šä¹‰ï¼Œæ­¤å¤„çœç•¥)

class NerDataset(Dataset):
    def __init__(self, data_path, vocab: Vocabulary, tag_map: dict):
        # ä¸€æ¬¡æ€§å°†æ•´ä¸ª JSON æ–‡ä»¶ï¼ˆä¸€ä¸ªå¤§åˆ—è¡¨ï¼‰è¯»å…¥å†…å­˜
        self.vocab = vocab
        self.tag_to_id = tag_map
        with open(data_path, 'r', encoding='utf-8') as f:
            self.records = json.load(f)

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        # 1. æ ¹æ®ç´¢å¼•è·å–åŸå§‹è®°å½•
        record = self.records[idx]
        text = normalize_text(record['text'])
        tokens = list(text)
        
        # 2. å°†æ–‡æœ¬å­—ç¬¦è½¬æ¢ä¸º token_ids
        token_ids = self.vocab.convert_tokens_to_ids(tokens)

        # 3. ç”Ÿæˆä¸æ–‡æœ¬ç­‰é•¿çš„ tag åºåˆ—ï¼Œé»˜è®¤ä¸º 'O'
        tags = ['O'] * len(tokens)
        
        # 4. éå†å®ä½“åˆ—è¡¨ï¼Œç”¨ BMES æ ‡ç­¾è¦†ç›–é»˜è®¤çš„ 'O'
        for entity in record.get('entities', []):
            entity_type = entity['type']
            start = entity['start_idx']
            end = entity['end_idx']  # é—­åŒºé—´ç»“æŸç´¢å¼•

            if end >= len(tokens): continue

            if start == end:
                tags[start] = f'S-{entity_type}' # å•å­—å®ä½“
            else:
                tags[start] = f'B-{entity_type}' # å®ä½“å¼€å§‹
                tags[end] = f'E-{entity_type}'   # å®ä½“ç»“æŸ
                for i in range(start + 1, end):
                    tags[i] = f'M-{entity_type}' # å®ä½“ä¸­é—´

        # 5. å°† BMES æ ‡ç­¾å­—ç¬¦ä¸²åºåˆ—è½¬æ¢ä¸º label_ids
        label_ids = [self.tag_to_id[tag] for tag in tags]

        # 6. è¿”å›åŒ…å«ä¸¤ä¸ª Tensor çš„å­—å…¸
        return {
            "token_ids": torch.tensor(token_ids, dtype=torch.long),
            "label_ids": torch.tensor(label_ids, dtype=torch.long)
        }

if __name__ == '__main__':
    # ä¸ºæµ‹è¯• NerDataProcessor å‡†å¤‡æ‰€éœ€çš„ vocab å’Œ tag_map
    vocab_file = './data/vocabulary.json'
    categories_file = './data/categories.json'
    train_file = './data/CMeEE-V2_train.json'
    
    vocabulary = Vocabulary(vocab_path=vocab_file)
    with open(categories_file, 'r', encoding='utf-8') as f:
        tag_map = json.load(f)
        
    # åˆ›å»ºæ•°æ®é›†å®ä¾‹
    train_dataset = NerDataset(train_file, vocabulary, tag_map)
    print(f"æ•°æ®é›†å¤§å°: {len(train_dataset)}")
```

### 4.3 æ•´åˆä¸º DataLoader

æœ€åï¼Œå®šä¹‰ `create_ner_dataloader` å‡½æ•°ã€‚å®ƒæ¥æ”¶ `Dataset` å®ä¾‹ï¼Œå¹¶å°†å…¶å°è£…æˆä¸€ä¸ª `DataLoader`ã€‚åœ¨ NLP ä»»åŠ¡ä¸­ï¼Œç”±äºæ¯ä¸ªæ ·æœ¬ï¼ˆå¥å­ï¼‰çš„é•¿åº¦éƒ½ä¸åŒï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥è®© `DataLoader` ä½¿ç”¨é»˜è®¤çš„æ–¹å¼æ‰“åŒ…æ•°æ®ï¼Œå¦åˆ™ä¼šå› åºåˆ—é•¿åº¦ä¸ä¸€è€ŒæŠ¥é”™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æä¾›ä¸€ä¸ªè‡ªå®šä¹‰çš„ `collate_fn` (æ ¡å¯¹å‡½æ•°) æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

`collate_fn` çš„ä¸»è¦ä»»åŠ¡ï¼Œå°±æ˜¯å°†ä» `Dataset` ä¸­å–å‡ºçš„ã€ç”±å¤šæ¡æ•°æ®ç»„æˆçš„åˆ—è¡¨ï¼ˆ`batch`ï¼‰ï¼Œâ€œèšåˆâ€æˆä¸€ä¸ªç»Ÿä¸€çš„ã€è§„æ•´çš„æ‰¹æ¬¡ã€‚åœ¨å½“å‰ä»»åŠ¡ä¸­ï¼Œå®ƒä¸»è¦è´Ÿè´£ä¸¤ä»¶äº‹ï¼š

1.  **åŠ¨æ€å¡«å…… (Padding)**ï¼šæ‰¾åˆ°å½“å‰æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼Œå¹¶å°†è¿™ä¸ªæ‰¹æ¬¡å†…çš„æ‰€æœ‰æ ·æœ¬éƒ½å¡«å……åˆ°è¿™ä¸ªæœ€å¤§é•¿åº¦ã€‚
2.  **ç”Ÿæˆ Attention Mask**ï¼šåˆ›å»ºä¸€ä¸ª `mask` çŸ©é˜µï¼Œç”¨æ¥æ ‡è®°å“ªäº›æ˜¯çœŸå®çš„ Token (å€¼ä¸º `1`)ï¼Œå“ªäº›æ˜¯å¡«å……çš„ Token (å€¼ä¸º `0`)ã€‚

```python
# ... 
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
# ... (çœç•¥å‰é¢æ‰€æœ‰çš„ç±»å’Œå‡½æ•°å®šä¹‰) ...

def create_ner_dataloader(data_path, vocab, tag_map, batch_size, shuffle=False):
    dataset = NerDataset(data_path, vocab, tag_map)
    
    def collate_batch(batch):
        token_ids_list = [item['token_ids'] for item in batch]
        label_ids_list = [item['label_ids'] for item in batch]

        padded_token_ids = pad_sequence(token_ids_list, batch_first=True, padding_value=vocab.pad_id)
        padded_label_ids = pad_sequence(label_ids_list, batch_first=True, padding_value=-100)
        attention_mask = (padded_token_ids != vocab.pad_id).long()

        return {
            "token_ids": padded_token_ids,
            "label_ids": padded_label_ids,
            "attention_mask": attention_mask
        }

    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_batch
    )

if __name__ == '__main__':
    # æ–‡ä»¶è·¯å¾„
    train_file = './data/CMeEE-V2_train.json'
    vocab_file = './data/vocabulary.json'
    categories_file = './data/categories.json'

    # 1. åŠ è½½èµ„æº
    vocabulary = Vocabulary(vocab_path=vocab_file)
    with open(categories_file, 'r', encoding='utf-8') as f:
        tag_map = json.load(f)

    # 2. åˆ›å»º DataLoader
    train_loader = create_ner_dataloader(
        data_path=train_file,
        vocab=vocabulary,
        tag_map=tag_map,
        batch_size=4,
        shuffle=True
    )

    # 3. éªŒè¯ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    batch = next(iter(train_loader))
    
    print("\n--- DataLoader è¾“å‡ºéªŒè¯ ---")
    print(f"  Token IDs shape: {batch['token_ids'].shape}")
    print(f"  Label IDs shape: {batch['label_ids'].shape}")
    print(f"  Attention Mask shape: {batch['attention_mask'].shape}")
```

`torch.utils.data.DataLoader` æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®åŠ è½½å·¥å…·ï¼Œå®ƒåƒä¸€ä¸ªé«˜åº¦è‡ªåŠ¨åŒ–çš„â€œæ•°æ®ä¾›åº”ç®¡é“â€ã€‚å°† `NerDataProcessor` å®ä¾‹ï¼ˆ`dataset`ï¼‰ä½œä¸ºæ•°æ®æºä¼ å…¥ï¼Œå¹¶é…ç½®äº†å‡ ä¸ªå…³é”®å‚æ•°ï¼š
-   **`batch_size`**ï¼šå®šä¹‰äº†æ¯ä¸ªæ‰¹æ¬¡åŒ…å«å¤šå°‘æ ·æœ¬ã€‚
-   **`shuffle=True`**ï¼šä½¿å¾—åŠ è½½å™¨åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶éƒ½éšæœºæ‰“ä¹±æ•°æ®é¡ºåºï¼Œèƒ½æœ‰æ•ˆæå‡æ³›åŒ–èƒ½åŠ›ã€‚
-   **`collate_fn`**ï¼šè¿™æ˜¯æœ€å…³é”®çš„å‚æ•°ï¼Œå®ƒæŒ‡å®šäº†å¦‚ä½•å°† `batch_size` ä¸ªå•ç‹¬çš„æ ·æœ¬â€œæ ¡å¯¹â€å’Œâ€œæ‰“åŒ…â€æˆä¸€ä¸ªè§„æ•´çš„æ‰¹æ¬¡ã€‚ä¼ å…¥çš„ `collate_batch` å‡½æ•°åœ¨è¿™é‡Œå®Œæˆäº†åŠ¨æ€å¡«å……å’Œ `attention_mask` çš„åˆ›å»ºå·¥ä½œã€‚


> **ä¸ºä»€ä¹ˆ `tag_ids` çš„å¡«å……å€¼æ˜¯ `-100`ï¼Ÿ**
>
> è¿™æ˜¯ä¸€ä¸ª PyTorch ä¸­çš„æƒ¯ä¾‹ã€‚åœ¨è®¡ç®—æŸå¤±æ—¶ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›å¡«å……ä½ç½®çš„æ ‡ç­¾å¯¹æœ€ç»ˆçš„æŸå¤±å€¼å’Œæ¢¯åº¦äº§ç”Ÿå½±å“ã€‚PyTorch çš„äº¤å‰ç†µæŸå¤±å‡½æ•° `torch.nn.CrossEntropyLoss` ä¸­æœ‰ä¸€ä¸ªå‚æ•° `ignore_index`ï¼Œå®ƒçš„é»˜è®¤å€¼æ°å¥½æ˜¯ `-100`ã€‚
>
> å½“æŸå¤±å‡½æ•°çœ‹åˆ°æ ‡ç­¾å€¼ä¸º `-100` æ—¶ï¼Œä¼šè‡ªåŠ¨â€œå¿½ç•¥â€è¿™ä¸ªä½ç½®ï¼Œä¸è®¡ç®—å®ƒçš„æŸå¤±ã€‚
