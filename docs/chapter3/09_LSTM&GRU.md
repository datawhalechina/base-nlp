# 第二节 LSTM 与 GRU

在了解了 RNN 的基本架构及其固有的缺陷后，本节将探讨两种经典的 RNN 改进方案——长短期记忆网络 (LSTM) 与门控循环单元 (GRU)，并剖析它们是如何通过精巧的结构设计来克服长距离依赖这一核心挑战的。

## 一、LSTM 与门控机制

简单 RNN 的问题在于，其内部状态的更新方式是“粗暴”的：每一步的新信息都会与旧信息（隐藏状态）无差别地混合，并通过权重矩阵 $W$ 进行变换。这种强制性的矩阵乘法，无论信息重要与否，都会在反向传播中形成梯度累乘，导致梯度信号的衰减或爆炸。

LSTM 的设计哲学是：**赋予网络自行决定信息取舍的能力**。它不再强制性地混合所有信息，而是引入了一个精巧的 **“门控机制” (Gating Mechanism)**，让模型在训练过程中学会 **有选择地** 让信息通过、遗忘旧信息或输出信息。

### 1.1 从单一状态到双轨并行

与简单 RNN 只有一个隐藏状态 $h_t$ 在时间步之间传递不同，LSTM 引入了两个独立的状态向量在时间轴上并行传递：

1.  **细胞状态 (Cell State, $c_t$)**: 这是 LSTM 的核心。可以把它想象成一条“信息高速公路”或“传送带”，负责在整个序列中传递 **长期记忆**。信息可以直接在这条传送带上流动，只受到轻微的线性变换，从而极大地缓解了梯度消失问题。
2.  **隐藏状态 (Hidden State, $h_t$)**: 这与 RNN 中的隐藏状态类似，代表了当前时间步的 **短期记忆** 和 **最终输出**。 $h_t$ 的计算依赖于当前的细胞状态 $c_t$。

LSTM 通过精密的门控单元，来控制细胞状态 $c_t$ 这条“高速公路”在每个时间点应该 **遗忘** 什么旧内容，以及应该 **写入** 什么新内容。

### 1.2 门 (Gate) 的结构

LSTM 中的“门”是一种让信息选择性通过的结构。它的实现非常简单：一个全连接层，其激活函数为 **Sigmoid**。

-   **输入**：通常是当前时间步的输入 $x_t$ 和上一个时间步的隐藏状态 $h_{t-1}$ 的拼接。
-   **计算**：对输入进行线性变换，然后通过 Sigmoid 函数。
-   **输出**：一个元素值在 **(0, 1)** 区间内的向量。这个向量将与另一个向量进行 **按元素乘法 (element-wise product)**。
    -   当门输出向量的某个元素接近 **1** 时，意味着“允许”对应维度的信息完全通过。
    -   当门输出向量的某个元素接近 **0** 时，意味着“阻止”对应维度的信息通过，即“遗忘”或“忽略”它。

LSTM 内部署了三个这样的门，来精确控制信息的流动。

## 二、LSTM 单元结构

一个 LSTM 单元在 $t$ 时刻接收三个输入：当前输入 $x_t$、前一时刻的隐藏状态 $h_{t-1}$ 和前一时刻的细胞状态 $c_{t-1}$。然后，它通过内部的三个门和一个tanh层，计算出新的细胞状态 $c_t$ 和隐藏状态 $h_t$。

![LSTM 单元结构](./images/9_2.svg)

为了方便后续公式的表述，我们首先将当前输入 $x_t$ 和前一时刻的隐藏状态 $h_{t-1}$ 拼接起来，记为 $[h_{t-1}, x_t]$。LSTM 内部的每一次线性变换，都是作用在这个拼接后的向量上，只是各自使用不同的权重矩阵。

### 2.1 第一步：遗忘门 (Forget Gate)

**作用**：决定应该从上一个细胞状态 $c_{t-1}$ 中 **丢弃** 多少信息。

这个门会审视 $h_{t-1}$ 和 $x_t$，然后为 $c_{t-1}$ 中的每个数值输出一个介于 0 和 1 之间的数字。1 代表“完全保留”，0 代表“完全丢弃”。

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

其中，$f_t$ 是遗忘门的输出向量。

### 2.2 第二步：输入门 (Input Gate)

**作用**：决定要将哪些 **新信息** 存储到细胞状态 $c_t$ 中。

这一步分为两个部分：
1.  **筛选信息**：首先，一个称为“输入门”的 Sigmoid 层决定了我们将更新哪些值。

    $$
    i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
    $$

2.  **准备新内容**：然后，一个 `tanh` 层创建一个新的候选值向量 $\tilde{c}_t$，这些是有可能被加入到细胞状态中的新内容。这部分的计算与简单 RNN 的核心计算非常相似

    $$
    \tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
    $$

$i_t$ 负责决定 $\tilde{c}_t$ 中哪些部分是重要的、需要被加入的。

### 2.3 第三步：更新细胞状态

**作用**：组合旧记忆和新信息，生成新的细胞状态 $c_t$。

现在，我们可以更新细胞状态了。
-   首先，我们将旧状态 $c_{t-1}$ 与遗忘门 $f_t$ 相乘，丢弃掉我们决定要忘记的部分。
-   然后，我们将输入门 $i_t$ 与候选值 $\tilde{c}_t$ 相乘，得到需要加入的新信息。
-   最后，将这两部分相加，得到新的细胞状态 $c_t$。

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

(注：$\odot$ 表示按元素乘法)

### 2.4 第四步：输出门 (Output Gate)

**作用**：决定要从细胞状态中 **输出** 什么信息，作为当前时间步的隐藏状态 $h_t$。

输出同样基于我们的细胞状态，但会经过一个“过滤”处理。
1.  **决定输出部分**：首先，我们运行一个 Sigmoid 层（输出门），它决定了我们要输出细胞状态的哪些部分。

    $$
    o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
    $$

2.  **过滤并输出**：然后，我们将细胞状态 $c_t$ 通过 `tanh` 函数（将其值规范化到 -1 和 1 之间），并将其与输出门 $o_t$ 的输出相乘，最终只输出我们决定的那部分信息。

    $$
    h_t = o_t \odot \tanh(c_t)
    $$

这个 $h_t$ 将作为当前时间步的最终输出，并传递给下一个时间步。

## 三、LSTM 如何缓解长距离依赖

现在，回到最初的问题：LSTM 是如何通过这套复杂的门控机制来缓解梯度消失问题的？

关键在于 **细胞状态 $c_t$ 的更新法则** 和 **遗忘门 $f_t$**。

回顾反向传播过程（BPTT），在简单 RNN 中，梯度在时间步之间传递时，每一步都必须乘以权重矩阵 $W$。而 LSTM 的情况有所不同，我们来考察一下损失 $L$ 对细胞状态 $c_{t-1}$ 的梯度：

$$
\frac{\partial L}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \frac{\partial c_t}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \odot f_t
$$

这个关系非常关键。从 $t$ 时刻的细胞状态反向传播到 $t-1$ 时刻，梯度仅仅是按元素乘以了遗忘门 $f_t$ 的值，而**没有乘以一个共享的权重矩阵**。

> 可以将细胞状态的更新 $c_t = f_t \odot c_{t-1} + \dots$ 视作一个带有动态权重的 **自循环 (self-loop)**。这个循环的权重就是遗忘门 $f_t$。正是这个结构上的改变，打破了简单 RNN 中梯度的累乘效应。

这意味着：
-   **梯度的“高速公路”**：如果模型在训练中发现某个早期信息非常重要，它可以通过学习将中间所有时间步的遗忘门 $f_t$ 的值都设置为接近 1。在这种情况下，梯度就可以几乎无衰减地从序列末端传播到序列开端。LSTM 为梯度的传递创造了一条可控的“高速公路”。
-   **可学习的依赖**：与简单 RNN 的结构性问题不同，LSTM 的长距离依赖问题变成了一个 **可学习** 的问题。模型可以通过优化损失函数，自行调整门控单元的参数，来决定哪些信息需要长期记忆（保持 $f_t \approx 1$），哪些信息可以被遗忘（让 $f_t \approx 0$）。

因此，我们不说 LSTM 完全 *解决* 了梯度消失问题，而是极大地 *缓解* 了它。它依然可能在处理极长（如上千个时间步）的序列时遇到困难，但相比简单 RNN 几十步就会出现问题的窘境，LSTM 已经实现了巨大的飞跃。
