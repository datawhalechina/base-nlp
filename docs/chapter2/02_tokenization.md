# 二、初级分词技术

要让计算机开始理解人类语言，首先要做的就是把连续不断的文本“敲碎”，变成一块块有意义的“积木”——这个过程就是**分词 (Tokenization)**。如果说文本是一座宏伟的建筑，那么“词”就是构成这座建筑的砖瓦。分词任务的质量，直接决定了上层建筑（如信息检索、机器翻译、情感分析等）的稳固程度。

## 1. 分词的定义与重要性

**分词 (Tokenization / Word Segmentation)** 的任务，是把连续的文本序列切分成具有独立语义的基本单元（即“词”或“词元”）。

-   对于英文等天然有空格作为分隔符的语言，分词相对简单。
-   但对于中文、日文、泰文等语言，文本是连续的字符流，词之间没有明确的边界。例如，`"给阿姨倒一杯卡布奇诺"`，计算机需要依据算法将其正确地切分为 `["给", "阿姨", "倒", "一杯", "卡布奇诺"]`。

> 在传统的NLP处理流程中，分词是后续所有任务的基础。其处理范式通常是将分词作为一个独立且“硬性”的预处理步骤，这意味着一个微小的分词错误就可能导致语义信息的丢失。这种错误会在后续的处理链条中被不断放大，产生“差之毫厘，谬以千里”的级联效应 (Cascading Effect)。例如，在传统的搜索引擎里，一旦“南京市长江大桥”被错分为 ["南京", "市长", "江大桥"]，系统就很难再从这三个错误的词块中还原出原始的、正确的地理位置含义，从而导致搜索结果完全跑偏。现代的NLP方法则通过更灵活的切分策略，在很大程度上缓解了这个问题。

## 2. 通过jieba认识分词[^2]

> [本节完整代码](https://github.com/FutureUnreal/base-nlp/blob/main/code/C2/01_jieba.py)

诚然，在以Transformer为基础的大语言模型（LLM）兴起后，传统意义上“语言学分词”的应用场景在逐步减少，现代大语言模型更多采用子词（Subword）等切分策略。但`jieba`作为一个经典、简单、代码开源的中文分词库，至今仍是**理解传统分词思想、学习NLP基础工程实践**的重要工具。

### 2.1 jieba安装

`jieba` 是目前流行的 Python 中文分词库之一，对初学者较为友好。可以使用如下 `pip` 命令安装。

```bash
pip install jieba
```

安装完成后，输入以下命令查看 `jieba` 的版本等信息，以确认是否安装成功。

```bash
pip show jieba
```

### 2.2 基于规则与词典

这是最早期也是最符合直觉的分词方法，其核心是**一部大型词典**和**一套匹配规则**。`jieba`的默认模式正是这种思想的体现。它首先基于一个**前缀词典（Trie树）**，高效地构建出一个包含句子中所有可能词语组合的**有向无环图（DAG）**。接着，通过动态规划算法寻找一条概率最大的路径，作为最终分词结果。

这个过程可以被量化为一个概率计算问题。假设一个分词路径由一个词语序列组成，将其表示为 $W_1, W_2, ..., W_n$，其中 $W_i$ 代表序列中的第 i 个词。那么这条路径的概率可以近似为：

$$ P(W_1, W_2, ..., W_n) \approx P(W_1) \times P(W_2) \times ... \times P(W_n) $$

其中，每个词 $W_i$ 的概率 $P(W_i)$ 可以通过其在词典（语料库）中的频率来估算：

$$ P(W_i) = \frac{\text{词 } W_i \text{ 的词频}}{\text{词典中所有词的总词频}} $$

`jieba`的目标就是找到一条路径，使得这个累乘的概率值最大。

### 2.2.1 实现原理：Log概率与动态规划

在实际工程中，将大量小于1的概率值直接相乘，很容易导致结果趋近于0，造成**浮点数下溢**，从而无法比较路径优劣。

`jieba` 采用了两种关键技术来解决这个问题：

1.  **对数概率**：利用对数函数 `log` 的性质，将概率的**累乘**转换为 `log` 概率的**累加**。寻找概率最大值就等价于寻找 `log` 概率之和的最大值，这有效地避免了下溢问题。

$$ \underset{W}{\arg\max} \sum_{i=1}^{n} \log P(W_i) $$

2.  **动态规划**：暴力计算所有可能路径的概率和是计算量很大的。`jieba` 使用动态规划的思想，从句子的**末尾**开始，**从后向前**递推计算到每个位置的最优切分路径及其 `log` 概率之和，并记录下来。最终，从句子开头出发，根据记录好的最优路径信息，就能反推出整个句子的最优分词结果。

例如，对于句子“给阿姨倒一杯卡布奇诺”，`jieba`可能会找到两条切分路径：
-   **路径 A**: `给 阿姨 倒 一杯 卡布奇诺`
-   **路径 B**: `给 阿姨 倒 一 杯 卡布奇诺`

模型会分别计算两条路径的概率：
-   $P(A) \approx P(\text{给}) \times P(\text{阿姨}) \times P(\text{倒}) \times P(\text{一杯}) \times P(\text{卡布奇诺})$
-   $P(B) \approx P(\text{给}) \times P(\text{阿姨}) \times P(\text{倒}) \times P(\text{一}) \times P(\text{杯}) \times P(\text{卡布奇诺})$

假设“一杯”是一个高频词，其单独出现的概率 $P(\text{一杯})$ 会远大于两个单字“一”和“杯”的概率之积 $P(\text{一}) \times P(\text{杯})$。因此，路径A的最终累乘概率会更高，模型会选择它作为最佳分词结果。

#### 2.2.2 Jieba 实践

`jieba`的**精确模式**正是前文所述 **“基于词典和动态规划寻找最大概率路径”** 这一方法的典型应用，它会力图将句子尽可能精确地切开。

```python
import jieba

text = "我在梦里收到清华大学录取通知书"
seg_list = jieba.lcut(text, cut_all=False) # cut_all=False 表示精确模式
print(seg_list)
```

```bash
['我', '在', '梦里', '收到', '清华大学', '录取', '通知书']
```

当遇到词典不认识的词时，比如“奔波儿灞”，精确模式就可能出错。这时，**基于词典的分词方法**其主要优势——**人工干预**——就体现出来了。可以通过**自定义词典**来“教会”`jieba`认识新词。

创建一个自定义字典`user_dict.txt`：
```text
九头虫
奔波儿灞
```

`jieba`自定义词典的格式为：`词语 [词频] [词性]`。词频和词性是可选的，用空格隔开。这里先不添加这两个选项。

```python
# 未加载词典前的错误分词
text = "九头虫让奔波儿灞把唐僧师徒除掉"
print(f"精准模式: {jieba.lcut(text, cut_all=False)}")

# 加载自定义词典
jieba.load_userdict("./user_dict.txt") 
print(f"加载词典后: {jieba.lcut(text, cut_all=False)}")
```

```bash
精准模式: ['九头', '虫', '让', '奔波', '儿', '灞', '把', '唐僧', '师徒', '除掉']
加载词典后: ['九头虫', '让', '奔波儿灞', '把', '唐僧', '师徒', '除掉']
```

通过自定义词典，能很方便地解决 **未登录词（OOV）**[^1] 的问题，让分词结果符合预期，这在处理特定业务领域文本时尤其有用。

当向`jieba`中添加自定义词（如“奔波儿灞”）但**不指定词频**时，`jieba`会采取一种自动的方式来估算其词频。它会首先尝试对这个新词进行分词（例如，默认会切成 `["奔", "波", "儿", "灞"]`），然后基于这些组成部分的基础词频计算出一个概率。最后，它会给“奔波儿灞”这个整体赋予一个**略高于**其组成部分概率之积的词频。这样，在后续的分词计算中，模型就会更倾向于将“奔波儿灞”视为一个完整的词，从而实现了“强制”分词的效果。

#### 2.2.3 精确模式工作流程

![精确模式工作流程](./images/2_2_2_2.svg)

分析底层源码可以发现，`jieba` 精确模式的分词过程主要是下面四步：

1.  **文本预处理与分块 (`cut` 方法)**

    `cut` 函数是总调度。它首先通过正则表达式 `re_han_default` 将整个句子切分成连续的汉字区块和非汉字部分（如英文、数字、标点）。非汉字部分被直接输出，而每个汉字区块则被送入核心分词流程。

2.  **构建有向无环图 (DAG) (`get_DAG` 方法)**

    这一步为每个汉字区块生成一个记录所有可能分词路径的图。`get_DAG` 的逻辑是：
    -   从句子的第 `k` 个字开始，向后扫描，形成词语 `frag` (fragment)。
    -   只要 `frag` 存在于 `self.FREQ` 这个前缀词典中，就继续向后扫描。
    -   在扫描过程中，如果 `self.FREQ[frag]` 的值**不为0**，说明 `frag` 是一个能独立成词的词语，就将它的结束位置 `i` 记录下来。词频为 `0` 的词条仅被当作前缀，不会被记录为成词路径。
    -   最终，`get_DAG` 返回一个字典 `DAG`，`key` 是每个字的起始位置，`value` 是所有可能的、能成词的结束位置列表。

3.  **计算最优路径 (`calc` 方法)**

    这是动态规划算法的核心实现，用于从DAG中寻找最优路径。`calc` 的计算方向很重要：它是**从句子的末尾向前反向计算的** (`for idx in xrange(N - 1, -1, -1)`)。
    -   对于句中的每个位置 `idx`，它会考察所有从 `idx` 出发的可能词语（由 `DAG[idx]` 提供）。
    -   对于每个可能的词 `sentence[idx:x + 1]`，它会计算一个路径“分数”，这个分数由两部分相加而成：
        1.  **当前词的log概率**：源码中为 `log(self.FREQ.get(...) or 1) - logtotal`。
        2.  **该词之后剩余句子的最优log概率**：这部分已经在之前的迭代中计算好并储存在 `route[x + 1][0]` 中。
    -   `calc` 会选择使这个总分数最高的那个词语作为从 `idx` 出发的“最佳下一步”，并将这个最高分和“下一步”的起始位置 `x` 记录在 `route[idx]` 中。

4.  **从路由表中重建结果 (`__cut_DAG_NO_HMM` 方法)**

    当 `calc` 计算完成后，`route` 字典中已经储存了从每个位置出发的最优选择。`__cut_DAG_NO_HMM` 的工作就相对简单：
    -   它从句子的第 `0` 位开始 (`x=0`)。
    -   通过 `route[x][1]` 查找下一步应该跳到哪里，从而得到第一个最优的词。
    -   然后将 `x` 更新为这个词的结束位置，继续循环查找，直到句子末尾。
    -   最终，通过 `yield` 逐个返回最优路径上的所有词语。

### 2.3 统计学习时代的方法

为了解决对人工词典的过度依赖，研究者们转向了统计学习。其核心思想是把分词看作一个**序列标注**问题。即为每个字标注其在词中的位置（`B-Begin`, `M-Middle`, `E-End`, `S-Single`），然后利用**隐马尔可夫模型（HMM）**等模型来预测每个字最可能的位置标签序列。

在统计方法时代，分词被巧妙地转化成了一个**序列标注（Sequence Labeling）**问题。即为句子中的每个字打上一个位置标签，例如：
-   `B` (Begin)：词的开始
-   `M` (Middle)：词的中间
-   `E` (End)：词的结束
-   `S` (Single)：单字成词

这样，“我爱北京”就会被标注为 `S S B E`。分词任务就变成了为字序列寻找最合理的标签序列的问题。

隐马尔可夫模型（HMM）是解决这类问题的经典生成式模型。它能学习到字与标签之间的对应关系（发射概率）以及标签与标签之间的转移关系（转移概率）。`jieba` 就利用了HMM来识别词典中不存在的**OOV**[^3]。当基于词典的图算法在句子中遇到一个无法切分的、连续的未登录字串时，就会调用HMM模块对这个局部子句进行分词。

-   **优点**：能够发现词典外的新词，一定程度上解决了OOV问题。
-   **缺点**：HMM的两个核心假设（观测独立和齐次马尔可夫）过于严格，限制了其利用更丰富的上下文特征的能力，因此在处理复杂的歧义场景时效果不如后续的CRF等模型。

#### 2.3.1 Jieba 实践：HMM对未登录词的识别

前面分析了 `__cut_DAG_NO_HMM` 的源码，它直接使用动态规划（`calc` 方法）计算出的最优路径。如果最优路径将“直聘”切分为两个单字，它就会直接输出两个单字。

而当 `HMM=True`（默认情况）时，`jieba` 调用的是 `__cut_DAG` 方法，其源码展现了混合策略：

1.  **设立缓冲区**：函数内有一个缓冲区 `buf`。
2.  **收集单字**：当 `calc` 计算出的最优路径是一个**单字**时，该函数并**不立即输出**，而是将这个单字存入 `buf`。
3.  **HMM介入**：当遇到一个**多字词**，或者整个句子遍历结束时，函数会检查 `buf`。如果 `buf` 中积累了连续的单字（如 `"直聘"`），它并不会直接输出这些单字，而是调用 `finalseg.cut(buf)` 来处理。`finalseg` 正是 `jieba` 内置的**HMM模型**。
4.  **二次分词**：HMM模型会对 `buf` 中的字符串进行一次基于统计模型的“二次分词”，尝试从中识别出完整的未登录词。

下面的例子直观地展示了 `__cut_DAG` 中HMM模块的作用：

```python
text = "我在Boss直聘找工作"

# 开启HMM（默认）
seg_list_hmm = jieba.lcut(text, HMM=True)
print(f"HMM开启: {seg_list_hmm}")
# 可能的输出: HMM开启: 我/ 爱/ 蓝翔技校

# 关闭HMM
seg_list_no_hmm = jieba.lcut(text, HMM=False)
print(f"HMM关闭: {seg_list_no_hmm}")
```

```bash
HMM开启: ['我', '在', 'Boss', '直聘', '找', '工作']
HMM关闭: ['我', '在', 'Boss', '直', '聘', '找', '工作']
```

在这个例子中，当HMM开启时，`__cut_DAG` 方法将动态规划产生的 `['直', '聘']` 序列在缓冲区内拼接成 `"直聘"`，然后交由HMM模型（`finalseg.cut`）处理，最终HMM通过其学到的统计规律，“猜”出这两个字是一个完整的词语，从而弥补了词典法的不足。

#### 2.3.2 词性标注

除了分词，`jieba` 还提供了词性标注功能。它采用了一种**词典查询与隐马尔可夫模型（HMM）相结合**的混合策略，来识别出每个词语的语法属性（名词、动词、形容词等）。这需要使用`jieba.posseg`模块。

由于在前面已经通过 `jieba.load_userdict()` 加载了包含“奔波儿灞”的词典，因此 `jieba` 已经能够正确地将其切分出来。但是，因为初始词典未提供词性，`jieba` 会给它一个默认的、不一定准确的词性（如下面的`x`）。

```python
import jieba.posseg as pseg

text = "九头虫让奔波儿灞把唐僧师徒除掉"

# HMM=False 强制只使用词典和动态规划
words = pseg.lcut(text, HMM=False)
print(f"默认词性输出: {words}")
```

此时的输出，分词是正确的，但“九头虫”和“奔波儿灞”的词性是`x`（非语素字）：
```bash
默认词性输出: [pair('九头虫', 'x'), pair('让', 'v'), pair('奔波儿灞', 'x'), pair('把', 'p'), pair('唐僧', 'nr'), pair('师徒', 'n'), pair('除掉', 'v')]
```

接下来，尝试通过调整词频来“干预”分词路径。如果希望将‘九头虫’完全切分为‘九’、‘头’、‘虫’三个单字，可以修改词典如下。为单个字“九”和“头”赋予了很高的词频，这会使`jieba`在进行动态规划计算时，认为 `九`+`头` 这条路径的概率远大于 `九头` 这条路径，从而优先选择前者。

将`user_pos_dict.txt`修改为：
```text
九 10000000 n
头 1000000 n
奔波儿灞 nr
```

为了让修改生效，需要重新加载词典：
```python
# 重新加载修改后的词典
jieba.load_userdict("./user_pos_dict.txt")

dic_words = pseg.lcut(text2, HMM=False)
print(f"加载词性词典后: {dic_words}")
```

可以看到，`jieba`根据词典的“指示”，成功地将“九头”拆分：
```bash
加载词性词典后: [pair('九', 'n'), pair('头', 'n'), pair('虫', 'n'), pair('让', 'v'), pair('奔波儿灞', 'nr'), pair('把', 'p'), pair('唐僧', 'nr'), pair('师徒', 'n'), pair('除掉', 'v')]
```

`jieba` 使用的是兼容ICTCLAS的词性标记集，常见的标签如下：

| 标签 | 含义 | 标签 | 含义 |
| :--- | :--- | :--- | :--- |
| n | 名词 | nr | 人名 |
| ns | 地名 | nt | 机构团体 |
| nz | 其他专名 | v | 动词 |
| a | 形容词 | d | 副词 |
| m | 数词 | q | 量词 |
| r | 代词 | p | 介词 |
| c | 连词 | u | 助词 |
| t | 时间词 | x | 非语素字 |
| w | 标点符号 | un | 未知词 |

### 2.4 从“分词”到“分块”

随着深度学习，特别是`BERT`和`GPT`等大规模预训练模型的兴起，传统意
义上“将句子切分成标准词语”的分词范式有了重大改变。现代NLP模
型更倾向于采用**“无分词”或“弱分词”**的策略，将文本处理成更基础的、
数据驱动的单元，主要分为以下两种流派：

#### 2.4.1 字粒度分词 (Character-level)

以 `BERT` 模型为代表，在处理中文时，其最基础的分词策略就是**字粒度
**，即直接将每个汉字视为一个独立的Token。

- **优点**：
    - **有效解决了OOV问题**：常用汉字的数量是有限
    的（几千个），模型可以构建一个全覆盖的“字表”。任何由标准汉
    字组成的词语都不会“未登录”，因为构成它的每个字都在字表里。
    - **无需维护庞大词典**：摆脱了对词典的依赖。
- **缺点**：
    - **丢失了词汇语义**：像“博物馆”这样的词，其整体语义在输入层
    面被人为地拆散为三个独立的字，模型需要消耗更多的计算资源在内部
    重新学习这些字的组合关系。
    - **输入序列更长**：相较于词，字的序列长度会显著增加，加大了模
    型的处理负担。

#### 2.4.2 子词分词 (Subword)

以 `GPT` 系列为代表的大语言模型，则采用了更灵活的**子词
（Subword）**切分方案，其中最主流的算法是 **BPE (Byte Pair 
Encoding)**[^4]。

BPE的核心思想是：在原始的字符语料库上，迭代地将高频的相邻字节对
（或字符对）合并成一个新的、更大的单元，并将其加入词表。

例如，对于语料中频繁出现的 "deeper"，BPE可能会先学习到 "er"，然后
是 "deep"，最终可能将 "deeper" 作为一个整体或 "deep"+"er" 的组合
加入词表。

- **优点**：
    - **有效平衡**：它在“词”和“字”之间取得了较好的平衡。高频词
    （如“机器学习”）可以被完整保留，低频词（如“擘画”）可以被拆分为
    更小的有意义的子词或单字（`擘`+`画`），而OOV（如一个新造
    的网络词）则可以被拆解成字符或字节组合，从而**在保持信息完整性的
    前提下有效解决了OOV问题**。
    - **词表大小可控**：可以通过控制合并次数，将词表大小有效控制
    在预设范围内（如5万或10万）。

子词分词是当前大语言模型处理文本的标准方案，既保留了词的语义信息，又具备了字的灵活性。

---

## 参考文献

[^1]: Sun, F. (2012). *jieba Chinese segmentation*. GitHub. [https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba)

[^2]: **未登录词 (Out-of-Vocabulary, OOV)**: 指在模型的词典（Vocabulary）中没有收录的词。当模型在处理文本时遇到一个它在训练阶段从未见过的词时，这个词就是一个未登录词。这是传统基于词典的NLP方法面临的一大挑战，因为模型不知道如何处理这些词，常常导致错误的切分。

[^3]: Rabiner, L. R. (1989). *A tutorial on hidden Markov models and selected applications in speech recognition*. Proceedings of the IEEE, 77(2), 257-286. [PDF](https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf)

[^4]: Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*. arXiv:1508.07909. [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)