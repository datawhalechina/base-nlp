# 第二节 词向量表示

在上一章中，简单学习了如何将连续的文本切分成一个个独立的词元（Token）。然而，无论是`jieba`分出的词语，还是BERT模型中的单个汉字，它们本质上仍然是计算机无法直接理解的"字符串"。

机器学习和深度学习模型，无论结构多么复杂，其处理的输入都必须是 **数值形式** ——具体来说，是由数字组成的 **特征向量或矩阵** 。因此，在分词之后，必须将这些词元转换为模型可以"消化"的数字形式。这个过程称为 **词向量表示 (Word Representation)** 。 **词嵌入 (Word Embedding)** 通常特指通过神经网络学习得到的稠密向量表示，是词向量表示的一个重要子集。

## 一、为什么需要词向量？

模型无法直接处理文本，它需要数字化的特征。以最基础的NLP任务——**文本分类**为例，需要判断一段文本“国足爱吃海参”属于“负面”类别。模型无法直接理解这串汉字，它只能处理数字。

词向量的核心任务就是 **弥合自然语言（符号世界）与数学模型（向量空间）之间的鸿沟** 。需要一种系统性的方法，将分词后得到的`["国足", "爱", "吃", "海参"]`这个词元序列，整体转换成一个或一组有意义的数字（即向量），然后才能将其输入给分类模型进行训练和预测。一个好的词向量表示，不仅要能唯一地标识一个词，更理想的情况是，向量本身能够 **蕴含词语的语义信息** 。例如，我们希望“国王”和“女王”的向量在空间中的距离，会比“国王”和“香蕉”的向量距离更近。实现这一目标，也是NLP领域的重要发展方向之一。

## 二、离散表示

在深度学习普及之前，研究者们提出了多种将词语表示为固定向量的方法。这些方法通常将每个词视为一个独立的、不可再分的单元，生成的向量因此也被称为**离散向量（Discrete Vector）**。其特点是维度高且稀疏，在机器学习领域应用广泛。

### 2.1 独热编码 (One-Hot Encoding)

这是最直观、最基础的**词元级别**表示方法。它将每个词元都看作一个独立的类别，其思想与机器学习中的类别特征处理一致。

#### 2.1.1 编码原理

**独热编码**，也称"哑编码"，其核心步骤如下：

1.  **构建词典**：首先，从整个语料库中收集所有出现过的**唯一**词语，构成一个词典。
2.  **分配索引**：为词典中的每个词语分配一个从0开始的唯一整数索引。
3.  **创建向量**：用一个长度等于词典大小的向量来表示每个词。向量中，该词对应索引的位置为`1`，其余所有位置均为`0`。

#### 2.1.2 编码示例

假设词典是从句子"我先挣它一个亿"构建的，分词后为`["我", "先", "挣", "它", "一个", "亿"]`。

-   `我`    -> `[1, 0, 0, 0, 0, 0]`
-   `先`    -> `[0, 1, 0, 0, 0, 0]`
-   `挣`    -> `[0, 0, 1, 0, 0, 0]`
-   `它`    -> `[0, 0, 0, 1, 0, 0]`
-   `一个`  -> `[0, 0, 0, 0, 1, 0]`
-   `亿`    -> `[0, 0, 0, 0, 0, 1]`

#### 2.1.3 优点与缺陷

-   **优点**：实现简单，能够清晰地将词语区分开。
-   **缺点**：
    1.  **维度灾难**：如果词典中有数万个词，那么每个词的向量维度就高达数万，造成数据极其稀疏，浪费计算和存储资源。
    2.  **语义鸿沟**：任意两个不同词的独热向量都是**正交**的（它们的点积为0）。
        
        > **为什么点积为0？**
        > 
        > **点积**是通过将两个向量的对应元素相乘再求和来计算的。在独热编码中，每个向量只有一个位置是`1`，其余都是`0`。对于任意两个**不同**的词，它们值为`1`的位置必然是错开的。
        > 
        > 因此，在计算点积时，`1`总是与`0`相乘，导致所有乘积项都为0，最终的和（点积）也为0。在几何上，点积为0意味着向量**正交**（互相垂直），这在语义上表示所有词都被视为同等的不相似。
        
        意味着模型无法从向量层面得知词与词之间的任何相似关系。在模型看来，"国王"与"女王"的距离，和"国王"与"香蕉"的距离是完全一样的，这严重丢失了语义信息。

### 2.2 词袋模型 (Bag-of-Words, BoW)

哑编码表示的是单个词，但实际通常需要表示整个句子或文档。**词袋模型**正是为此而生，它是表示**文档级别**特征最常用的方法之一。这种方法的理论基础可以追溯到向量空间模型[^1]的提出。

#### 2.2.1 基本思想

**词袋模型**的基本思想是：**忽略文本中的词序和语法，将其仅仅视作一个装满词的"袋子"**，用袋子中每个词出现的**统计量**来表示整个文档。

它的实现过程可以理解为：将文档中所有词的**独热向量相加**，得到一个最终的向量。这个向量的维度等于词典大小，每一维的值代表了对应词语在文档中的出现频次。实际实现时，通常直接统计每个词的出现次数，而不需要真正构造和相加 One-Hot 向量。

#### 2.2.2 实现示例

假设词典与上文相同（基于"我先挣它一个亿"），有两个文档：
-   文档1: `我 先 挣 一个 亿`
-   文档2: `我 挣 它 一个 亿`

它们的词袋表示为：
-   `vec(文档1)` = `vec(我)` + `vec(先)` + `vec(挣)` + `vec(一个)` + `vec(亿)` = `[1, 1, 1, 0, 1, 1]`
-   `vec(文档2)` = `vec(我)` + `vec(挣)` + `vec(它)` + `vec(一个)` + `vec(亿)` = `[1, 0, 1, 1, 1, 1]`

这个结果向量的每一维，代表了对应词典中的词在该文档中出现的次数。通过计算两个向量的距离（如余弦相似度），可以发现这两个文档是比较相似的，因为它们共享了"我"、"挣"、"一个"、"亿"这几个词。

#### 2.2.3 余弦相似度计算

**余弦相似度 (Cosine Similarity)** 通过计算两个向量夹角的余弦值来衡量它们的相似性。对于非负向量（如词袋模型产生的向量），其值范围在 `[0, 1]` 之间，值越接近`1`，表示两个向量越相似。

**公式:**

$$
\text{similarity} = \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

令 **A** = `vec(文档1)` = `[1, 1, 1, 0, 1, 1]`，**B** = `vec(文档2)` = `[1, 0, 1, 1, 1, 1]`

1.  **计算点积 A · B**: $(1 \times 1) + (1 \times 0) + (1 \times 1) + (0 \times 1) + (1 \times 1) + (1 \times 1) = 1 + 0 + 1 + 0 + 1 + 1 = 4$

2.  **计算模长 ||A|| 和 ||B||**:

    `||A||` = $\sqrt{1^2+1^2+1^2+0^2+1^2+1^2} = \sqrt{5}$

    `||B||` = $\sqrt{1^2+0^2+1^2+1^2+1^2+1^2} = \sqrt{5}$

3.  **计算相似度**:

    $$
    \cos(\theta) = \frac{4}{\sqrt{5} \cdot \sqrt{5}} = \frac{4}{5} = 0.8
    $$

结果为`0.8`，这是一个非常接近`1`的值，这从数学上证明了这两个文档是高度相似的。

#### 2.2.4 不同统计方式

词袋模型向量中每一维的值，可以根据不同策略来确定：

- **频数** : 直接使用单词在文档中出现的次数。这是最简单直接的方式，但会受到文章长度影响，长文章的计数值会普遍偏高。
- **频率** : 使用单词在文档中出现的次数除以文档的总词数，即词频（TF）。这在一定程度上缓解了文档长度不同带来的问题。
- **二进制** : 只关心单词是否出现，出现即为`1`，不出现为`0`，不关心出现的次数。

#### 2.2.5 优点与局限

-   **优点**：实现简单，并且在**文本分类**等任务上，因为这类任务的核心在于判断"文档里有什么词"，而非"词与词之间如何关联"，所以即使丢失了词序，也常常能取得不错的效果。
-   **缺点**：
    1.  **丢失词序**：`"我 爱 你"` 和 `"你 爱 我"` 的词袋表示完全相同，无法区分语义差异。
    2.  **未考虑词的重要性**：像"的"、"是"这类在所有文档中都频繁出现的 **停用词** ，会获得很高的频次，但它们对区分文档主题几乎没有贡献，反而会形成干扰。

### 2.3 TF-IDF

为了解决词袋模型中"常见词权重过高"的问题，**TF-IDF (Term Frequency-Inverse Document Frequency)** 被提了出来。它是一种经典的加权技术，用于评估一个词对于一个文件集或一个语料库中的其中一份文件的重要程度。

> TF-IDF 指出：一个词的重要性，与其在**当前文档中出现的次数**成正比，与其在**整个语料库中出现的频率**成反比。一个词在当前文档里越常见，但在其他文档里越罕见，其权重就越高。

#### 2.3.1 计算公式

它由两部分组成：

1.  **词频 (Term Frequency, TF)**：衡量一个词在当前文档中出现的频繁程度。常见的计算方式有：

    - 原始频数： $TF(t, d) = f_{t,d}$
    - 归一化频率： $TF(t, d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}$
    
    其中 $f_{t,d}$ 表示词 $t$ 在文档 $d$ 中出现的次数。

2.  **逆文档频率 (Inverse Document Frequency, IDF)**：衡量一个词的"稀有"程度或"信息量"。这一概念由Karen Sparck Jones在1972年提出[^2]。

    $$ IDF(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} $$
    
    其中 $|D|$ 是语料库中的总文档数， $|\{d \in D : t \in d\}|$ 是包含词 $t$ 的文档数。
    
    为避免除零错误，实际应用中常使用平滑版本：

    $$ IDF(t, D) = \log \frac{|D|}{1 + |\{d \in D : t \in d\}|} $$

最终, 一个词的TF-IDF权重就是这两者的乘积：

$$ TF-IDF(t, d, D) = TF(t, d) \times IDF(t, D) $$

一个文档的TF-IDF向量, 就是由该文档中每个词的TF-IDF值构成的向量。

#### 2.3.2 实际应用

-   **关键词提取**：计算一篇文章中每个词的TF-IDF值，并按降序排列，排在最前面的通常就是这篇文章的关键词。`jieba`的关键词提取也内置了这种算法。
-   **文本相似度计算**：计算两篇文档的TF-IDF向量，再通过余弦相似度等方法判断它们的相似性。
-   **传统搜索引擎**：在早期的搜索引擎中，TF-IDF是衡量查询词与网页相关性的核心指标之一。

## 三、序号化表示

虽然上述方法在传统机器学习时代扮演了重要角色，但在深度学习时代，它们已不再是主流。现代深度学习模型，尤其是大语言模型，采用的是一种更简洁、更灵活的输入方式—— **序号化 (Sequentialization)** 。

### 3.1 核心思想转变

在深度学习中，通常**只进行最少的预处理**。不再像传统方法那样，费尽心思地设计复杂的特征工程（如计算TF-IDF）来告诉模型哪些词重要。相反，只把文本转换成最基础的 **整数ID序列** ，然后把"学习词语的含义和重要性"这个更复杂的任务， **交给模型自己去完成** 。

### 3.2 序号化过程

**序号化** ，也称"整数编码"，是将 **分词后的词元序列** 转换为深度学习模型能够处理的 **整数序列** 的 **核心步骤** 。其过程如下：

1.  **构建词典** ：与One-Hot类似，首先从训练语料中构建一个词典。但在深度学习中，这个词典通常是 **字级别** 的（如BERT），或是 **子词级别** 的（如GPT），而不是词级别的。
2.  **增加特殊词元** ：在词典中加入一些有特殊功能的Token，至少包括：
    -   `[PAD]` (Padding)：用于 **填充** 。因为模型通常需要批处理（Batch Processing），一个批次内的所有句子必须长度相同。短句子会用`[PAD]`填充到与最长句子一致的长度。其对应的ID通常是`0`。
    -   `[UNK]` (Unknown)：用于表示所有在词典中 **未出现过** 的词。其对应的ID通常是`1`。
    -   此外，还可能有 `[CLS]` (Classification), `[SEP]` (Separator) 等用于特定任务的特殊词元。
3.  **ID映射** ：将文本序列中的每个词元（字/子词）直接映射为其在词典中的 **整数ID** 。

**预训练模型的词典：**

在实践中，很少从零开始为自己的小数据集构建词典。更常见的做法是，直接使用像 `BERT`、`GPT` 这类预训练模型官方提供的 **词典文件 (vocab.txt)** 。这些词典通常包含了数万个字、子词、符号等，是在海量通用语料上构建的，覆盖面非常广。

例如，Google的中文BERT模型词典 `vocab.txt` 中就包含了约21128个词元，其中不仅有常用汉字，还包括了英文字母、数字、标点及 `[PAD]`, `[UNK]` 等特殊符号。

### 3.3 序号化实例

假设有一个精简词典：

`{'[PAD]': 0, '[UNK]': 1, '比': 2, '方': 3, '说': 4, '我': 5, '先': 6, '挣': 7, '它': 8, '一': 9, '个': 10, '亿': 11}`

现在有三个句子需要处理：
1.  `我挣一个亿`
2.  `比方说我`
3.  `我先挣钱`

**第一步：分词 & 查找ID**
-   句子1 (`我挣一个亿`): `我` (5), `挣` (7), `一` (9), `个` (10), `亿` (11) -> `[5, 7, 9, 10, 11]`
-   句子2 (`比方说我`): `比` (2), `方` (3), `说` (4), `我` (5) -> `[2, 3, 4, 5]`
-   句子3 (`我先挣钱`): `我` (5), `先` (6), `挣` (7), `钱` (不在词典中) -> `[5, 6, 7, 1]`

**第二步：填充 (Padding)**
为了将这三个长短不一的序列组成一个矩阵，需要以最长的序列（句子1，长度为5）为基准，对其他短序列用`[PAD]`的ID `0`进行填充。

-   序列1 (长度5): `[5, 7, 9, 10, 11]`
-   序列2 (长度4→5): `[2, 3, 4, 5, 0]`
-   序列3 (长度4→5): `[5, 6, 7, 1, 0]`

最终，我们得到一个 `3x5` 的 **整数矩阵** 。这个矩阵，就是喂给深度学习模型的最终输入。

```bash
# 最终输入模型的张量 (Tensor)
[[5, 7, 9, 10, 11],
 [2, 3, 4, 5,  0],
 [5, 6, 7, 1,  0]]
```

> 序号化本身并未解决语义鸿沟，其整数ID（如 `2` 和 `3`）不具备数学意义。它的真正价值是作为后续 **嵌入层** 的输入。嵌入层会将这些ID查询并映射为低维、稠密的浮点数向量（即 **词向量** ），而这个映射关系本身是在模型训练中 **学习** 出来的[^3]。

### 3.4 Embedding（嵌入）

**嵌入层**是深度学习模型中处理离散序号化输入的关键组件。它本质上是一个可学习的“查找表”或参数矩阵（形状通常为 `(|V|, D)`），其作用是将输入的每个整数ID映射到一个低维、稠密的浮点数向量。这个过程可以看作是一次高效的查表操作（`embedding_matrix[id]`），将原本不带任何语义信息的整数ID转换成了包含丰富语义的词向量。这个嵌入矩阵的参数通常会与下游任务（如文本分类、机器翻译等）一起进行端到端训练，使得词向量能够学习到对特定任务最有用的语义表示。当然，也可以使用在大规模无标签语料上预训练好的词向量（如Word2Vec或GloVe）来初始化这个矩阵，然后根据任务需求选择在训练中微调或保持冻结。最终，由嵌入层输出的这一系列词向量将作为后续如RNN、LSTM或Transformer等序列模型的实际输入特征。

---

## 参考文献

[^1]: [Salton, G., Wong, A., & Yang, C. S. (1975). *A vector space model for automatic indexing*. Communications of the ACM, 18(11), 613-620](https://doi.org/10.1145/361219.361220)

[^2]: [Jones, K. S. (1972). *A statistical interpretation of term specificity and its application in retrieval*. Journal of Documentation, 28(1), 11-21](https://doi.org/10.1108/eb026526)

[^3]: [Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). *A neural probabilistic language model*. Journal of Machine Learning Research, 3(Feb), 1137-1155](https://www.jmlr.org/papers/v3/bengio03a.html)

