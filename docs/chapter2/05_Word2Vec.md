# 第三节 从主题模型到Word2Vec

无论是哑编码还是序号化，它们本身都存在一个根本性的缺陷：无法表达词与词之间的**语义关系**。在这些表示方法中，不同词的向量通常是正交的（如One-Hot编码中点积为0），或者其ID大小关系是随机的，这导致模型无法从输入层面理解“国王”与“女王”的语义比“国王”与“苹果”更近。

为了解决这个问题，研究者们提出了 **分布式表示（Distributed Representation）** 的思想，旨在将词语映射到一个**低维、稠密、且蕴含丰富语义信息**的连续向量空间中。本节将首先介绍一种基于传统机器学习思想的分布式表示方法——**主题模型**，并在此基础上，引出真正开启了NLP新时代的经典算法——**Word2Vec**。

## 一、寻找理想的词向量

理想中的词向量需要同时满足两个主要目标：

1.  **语义蕴含**：向量之间的距离（如余弦相似度或欧氏距离）能够度量词语之间的语义相似度。这背后的核心思想是**分布式假设**的朴素应用：**如果两个词经常在相似的上下文中共同出现（Co-occurrence），那么它们的向量在空间上应该是彼此靠近的**。例如，"被子"和"床铺"经常一起出现，它们的向量就应该接近；而"椰子"和"企鹅"则应该相互远离。语义的相似性，正是这种"上下文共现"关系在向量空间中的自然体现。
2.  **低维稠密**：摆脱“维度灾难”。词向量的维度应该是一个较小的、可控的超参数，而不是动辄数万的词典大小。并且，向量中的每一维都应是有意义的浮点数，而非绝大部分为0的稀疏表示。

为了实现这一目标，研究者们从两个不同的视角出发，发展出了两大主流技术路线：基于**全局文档统计**的**主题模型**，和基于**局部上下文预测**的**神经网络模型**。

## 二、主题模型

主题模型（Topic Model）是基于机器学习和传统数学思想的经典方法。它尝试从宏观的视角，通过分析大量文档的词语共现统计，来发现词语间的潜在语义关联。其中，最核心的技术是**矩阵分解（Matrix Factorization）**。

### 2.1 设计原理

**注意：** 主题模型的关键假设是：**一篇文档由多个"主题"按一定比例混合而成，而一个主题又由多个"词语"按一定概率组成**。词语之所以会一同出现在某篇文档中，是因为它们都在共同描述这篇文章所包含的某个或某些潜在主题。

例如，一篇关于"人工智能"的文档，会高频出现"深度学习"、"Transformer"、"注意力机制"等词。正是因为这些词都强关联于"AI技术"这个主题，它们才频繁地共现在一起。

因此，**一个词的向量，就可以用它与各个主题的关联强度来表示**。

### 2.2 SVD矩阵分解

该方法将获取词向量的过程，巧妙地转化成了一个矩阵分解问题。

![矩阵分解图示](./images/3_2_2.gif)

1.  **构建“词-文档”矩阵**
    首先，以整个语料库为基础，构建一个巨大的**词-文档矩阵 `V`**。这个矩阵的每一行代表一个词，每一列代表一篇文档，矩阵中 `V(i, j)` 的值是词 `i` 在文档 `j` 中的重要性权重，通常使用 **TF-IDF** 值来填充。这个矩阵通常是**非常巨大且高度稀疏**的。

2.  **矩阵分解**
    从线性代数的角度看，这个巨大的稀疏矩阵 `V` 可以被近似分解为两个更小的、更稠密的矩阵的乘积。最常用的分解技术之一是**奇异值分解（SVD）**。

    $$
    V_{m \times n} \approx W_{m \times k} \times H_{k \times n}
    $$

    -   $V_{m \times n}$ 是原始的词-文档矩阵，`m` 是词典大小，`n` 是文档数量。
    -   $k$ 是一个远小于 `m` 和 `n` 的超参数，代表期望发现的**潜在主题数量**。
    -   $W_{m \times k}$ 被称为 **“词-主题矩阵”** 。它的每一行，都是一个 `k` 维的稠密向量，表示一个词与 `k` 个主题的关联度。
    -   $H_{k \times n}$ 被称为 **“文档-主题矩阵”** 。它的每一列，都是一个 `k` 维的稠密向量，表示一篇文档在 `k` 个主题上的分布。

3.  **获取词向量**
    分解完成后，我们真正关心的是 **“词-主题”矩阵 `W`** 。这个矩阵的**每一行**，正是我们需要的**词向量**。
    -   它将原来 `m` 维的One-Hot编码，降维到了 `k` 维。
    -   它是一个稠密向量，每个维度都代表了与某个主题的关联强度。
    -   它蕴含了语义信息。如果两个词（如"CPU"和"GPU"）经常在描述"硬件"这个主题的文档中共同出现，那么SVD分解的结果会使它们在对应"硬件"主题的那个维度上都有很高的值，从而使它们的最终词向量在空间上非常接近。

**聚类视角理解主题模型：**

从机器学习的角度看，主题模型本质上是一个**聚类算法**：

-   **文档聚类**：文档主题矩阵 `H` 将 `n` 篇文档聚成 `k` 个主题类别。每篇文档都有一个 `k` 维向量，表示其属于各个主题的**置信度**或**软分配**。例如，一篇文档可能70%属于"AI技术"主题，30%属于"数学理论"主题。

-   **词语倾向性**：单词主题矩阵 `W` 揭示了词语的主题倾向。有些词语（如"深度学习"、"Transformer"、"注意力机制"）更倾向于描述"AI技术"主题，而另一些词语（如"坦克"、"导弹"、"战争"）则更倾向于描述"军事"主题。

-   **语义相关性的来源**：正因为描述同一主题的词语会在相同的主题维度上有较高的权重，它们的词向量才会在空间中彼此靠近，从而实现了语义信息的捕捉。

**总结与局限：**

主题模型（如其更广为人知的名字**LSA, Latent Semantic Analysis**[^3]）通过对全局的"词-文档"共现矩阵进行分解，成功地将词语映射到了一个低维的"主题空间"，从而得到了能够表达语义的稠密词向量。相关的技术还包括 **PCA** 和 **NMF（非负矩阵分解）**。

然而，这种方法也存在明显的局限性：
1.  **计算代价高昂**：对一个大型语料库进行SVD分解，计算量和内存开销都极大。
2.  **依赖全局统计**：它依赖的是全局的、粗粒度的文档级别共现信息，忽略了词语在句子中的局部上下文和词序信息，这使得它难以捕捉更精细的语义关系。
3.  **难以集成**：这种"先统计，再分解"的流程，很难与现代的深度学习模型进行端到端的联合训练。

## 三、Word2Vec

与主题模型从全局文档统计中挖掘主题不同，由Google在2013年提出的Word2Vec算法[^1]，将视角聚焦于词语的**局部上下文**。它的思想来源于语言学中的**分布式假设（Distributional Hypothesis）**[^2]：

> **一个词的含义，由其上下文中的词语所决定**。

换言之，如果两个词的上下文经常是相似的，那么这两个词的语义就是相近的。Word2Vec正是这一思想的数学实现。

### 3.1 Word2Vec概述

Word2Vec 通常被认为是一种**浅层神经网络模型（Shallow Neural Network）**。其"浅层"体现在网络结构的简单性上：相比深度神经网络，Word2Vec只包含输入层、一个隐藏层（词向量查找表）和输出层，没有多层的复杂结构。这种简洁的设计使得 Word2Vec 的计算非常高效，甚至无需依赖大型深度学习框架即可实现。

**目标与手段的分离：**

理解Word2Vec的关键在于区分其**最终目标**与**实现手段**。神经网络结构本身只是获取词向量的一种方式，**并非模型的最终目的**。

-   **最终目标**：获取一个高质量的**词向量查询表（Lookup Table）**。这本质上是一个巨大的矩阵 `W`，矩阵的每一行就是对应单词的稠密向量。
-   **实现手段**：为了学习到这个查询表，Word2Vec设计了一个巧妙的"伪任务"——**根据上下文预测中心词**（或反之），并在这个过程中，将词向量查询表作为**模型参数**进行训练和优化。

训练结束后，用于执行预测任务的神经网络本身会被**丢弃**。不会使用它的输出，真正需要和保留的，只有作为其内部参数的那个**词向量查询表**。

### 3.2 可学习的词向量矩阵

从数学上看，将一个单词的ID转换为其稠密向量的过程，在概念上可以分解为两步：

1.  **输入**：一个代表单词的ID，例如 `3`。
2.  **哑编码**：将ID `3` 转换为一个维度等于词典大小 `V` 的高维稀疏向量，例如 `[0, 0, 0, 1, 0, ...]`，其中只有第3个位置为1。
3.  **矩阵乘法**：用这个One-Hot向量去乘以一个巨大的、可学习的**参数矩阵 `W`**（尺寸为 V×D）。这个矩阵 `W` 就是最终想要得到的词向量查询表。

由于One-Hot向量只有一个位置是1，这个矩阵乘法的结果，等效于直接从矩阵 `W` 中 **“抽取”出索引为3的那一行** 。

$$
\begin{bmatrix} 0 & 0 & 0 & 1 & 0 & 0 \end{bmatrix}
\times
\begin{bmatrix}
2 & 8 & 5 & 3 & 1 \\
9 & 4 & 7 & 2 & 6 \\
3 & 1 & 8 & 5 & 0 \\
5 & 6 & 2 & 9 & 4 \\
8 & 0 & 3 & 7 & 2 \\
4 & 2 & 9 & 6 & 1
\end{bmatrix}
=
\begin{bmatrix} 5 & 6 & 2 & 9 & 4 \end{bmatrix}
$$

在实践中，为了极大地提升效率，程序并不会真的执行稀疏的矩阵乘法，而是直接实现一个**查询**操作：根据输入的单词ID，直接从 `W` 矩阵中获取对应的行向量。

理解这里的关键在于，这个参数矩阵 `W` **本身就是学习的目标**。它被随机初始化，并在后续的训练过程中，通过CBOW或Skip-gram这样的**预测任务**不断地被优化和调整。

### 3.3 两种经典模型

Word2Vec包含两种具体的实现模型：CBOW和Skip-gram。两者在任务设计上恰好相反，但最终都实现了相同的目标：通过**训练过程**得到一个高质量的词向量查询表。


#### 3.3.1 CBOW 模型详解

CBOW (Continuous Bag-of-Words) 的任务是**“根据上下文预测中心词”**。

![CBOW](./images/3_3_3_1.svg)

**数据流与维度变化：**

假设 `B` 是批大小，`S` 是上下文单词数量，`V` 是词典大小，`D` 是词向量维度。

1.  **输入层**：上下文窗口内的所有词对应的ID。
    -   **数据形状**：`(B, S)`，例如输入为 `[[1, 8, 10, 13, 14, 16], [8, 5, 14, 16, 18, 10]]`。

2.  **词向量转换**：
    -   **操作**：将每个单词ID转换为对应的 `D` 维词向量。这通过输入矩阵 `U`（大小为 `V×D`）实现，相当于公式中的 $v_{c-k} = Uw_{c-k}$。
    -   **数据形状变化**：`(B, S)` -> `(B, S, D)`。

3.  **上下文向量求和**：
    -   **操作**：将一个样本中 `S` 个上下文单词的 `D` 维向量进行求和，得到上下文向量 $v_c$。
    -   **数据形状变化**：`(B, S, D)` -> `(B, D)`。

4.  **输出得分计算**：
    -   **操作**：将 `D` 维的上下文向量与输出矩阵 `V`（大小为 `D×V`）相乘，得到 `V` 维的得分向量，对应公式中的 $z_c = Vv_c$。
    -   **数据形状变化**：`(B, D)` -> `(B, V)`。

5.  **损失计算**：
    -   **操作**：使用Softmax将得分转换为概率分布，然后计算与真实中心词之间的交叉熵损失。
    -   **优化**：通过反向传播更新输入矩阵 `U` 和输出矩阵 `V`。最终需要的是训练好的输入矩阵 `U`。

**公式：**

1.  **词向量转换**：对于上下文中的每个词 $w_{c-k}$，从输入矩阵 $U$ 中获取对应的词向量：

    $$
    v_{c-k} = Uw_{c-k}
    $$

2.  **上下文向量**：将上下文窗口中所有词的词向量求和：

    $$
    v_c = v_{c-m} + \cdots + v_{c-1} + v_{c+1} + \cdots + v_{c+m}
    $$

3.  **输出得分**：将上下文向量与输出矩阵 $V$ 相乘：

    $$
    z_c = Vv_c
    $$

4.  **损失函数**：模型的优化目标是最小化负对数似然：

    $$
    \begin{split}
    \text{minimize } J &= -\log P(w_c | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}) \\
    &= -\log P(u_c | \hat{v}) \\
    &= -\log \frac{\exp(u_c^T \hat{v})}{\sum_{j=1}^{|V|} \exp(u_j^T \hat{v})} \\
    &= -u_c^T \hat{v} + \log \sum_{j=1}^{|V|} \exp(u_j^T \hat{v})
    \end{split}
    $$

    其中 $u_c$ 是目标中心词的输出向量，$\hat{v}$ 是上下文向量。

#### 3.3.2 Skip-gram 模型详解

对比 CBOW , Skip-gram 的任务恰好相反：**“根据中心词预测上下文”**。它将一个预测任务，分解成了多个独立的子任务。

**数据流与维度变化**

1.  **输入层**：中心词的ID。
    -   **数据形状**：`(B, 1)`。
2.  **词向量转换**：
    -   **操作**：将中心词ID通过查表（输入矩阵 `U`）转换为其 `D` 维词向量 $v_{w_c}$。
    -   **数据形状变化**：`(B, 1)` -> `(B, 1, D)`。
3.  **输出得分计算**：
    -   **操作**：将 `D` 维的中心词向量 $v_{w_c}$，与输出矩阵 `V` 相乘，得到一个 `V` 维的得分向量。
    -   **数据形状变化**：`(B, 1, D)` -> `(B, V)`。
4.  **损失计算 (多标签问题)**：
    -   **操作**：与CBOW不同，这里的目标是预测多个上下文单词（例如，`S`个）。因此，这一个 `V` 维的得分向量将被**复用 S 次**，分别与 `S` 个真实的上下文单词计算损失。
    -   **优化**：将 `S` 个位置的损失**全部相加**，然后进行反向传播，同时更新 `U` 和 `V`。因为一个输入对应多个输出标签，这本质上是一个**多标签分类问题**。在实际运用中，通常将其分解为多个独立的单标签分类任务：对每个上下文词位置，都使用Softmax进行一次独立的预测，然后将所有位置的损失相加。

**公式**

1.  **词向量转换**：对于中心词 $w_c$，从输入矩阵 $U$ 中获取对应的词向量：

    $$
    v_c = Uw_c
    $$

2.  **输出得分计算**：将中心词向量与输出矩阵 $V$ 相乘：

    $$
    z_{2m} = V_{2m}v_c
    $$

3.  **损失函数**：模型的优化目标是最小化负对数似然。完整的数学推导过程如下：

    $$
    \begin{split}
    \text{minimize } J &= -\log P(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} P(w_{c-m+j} | w_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} P(u_{c-m+j} | v_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} \frac{\exp(u_{c-m+j}^T v_c)}{\sum_{k=1}^{|V|} \exp(u_k^T v_c)} \\
    &= -\sum_{j=0, j \neq m}^{2m} u_{c-m+j}^T v_c + 2m \log \sum_{k=1}^{|V|} \exp(u_k^T v_c)
    \end{split}
    $$
    
    其中 $v_c$ 是中心词的输入向量，$u_{c-m+j}$ 是上下文词的输出向量。

**模型对比**

Skip-gram为每个"中心词-上下文词"对都创建了一个独立的学习任务，这使得它能够更好地学习到词与词之间更精细的关系。在处理**低频词**和**大数据集**时，通常能得到质量更高的词向量，但由于其任务量是CBOW的 `S` 倍，训练速度相对较慢。

#### 3.3.3 滑动窗口的学习直觉

在了解了模型的基本构造后，可以深入探讨Word2Vec是如何真正捕捉到语义的。以CBOW模型为例来建立这种直觉，其关键在于**滑动窗口（Sliding Window）**机制如何生成大量高度重叠的训练样本。

假设有一个很长的句子和大小为 `k=7` 的窗口（中心词左右各7个词）。

这个过程通过在文本上移动一个“滑动窗口”来生成训练样本。对于CBOW任务，当窗口中心为第8个单词时，模型会用其上下文 `[w_1, ..., w_7]` 和 `[w_9, ..., w_15]` 来预测 `w_8`。紧接着，窗口向右滑动一格，中心变为第9个单词，模型则用新的上下文 `[w_2, ..., w_8]` 和 `[w_10, ..., w_16]` 来预测 `w_9`。

现在，比较这两个训练样本的上下文。会发现，它们之间有12个单词是**完全相同**的（`w_2` 到 `w_7` 和 `w_10` 到 `w_15`）。

这意味着，**两个样本的“上下文向量”（所有上下文词向量之和）在初始时就会非常相似**。

模型的目标是：
-   对于第一个样本，它需要调整参数，使得相似的上下文向量能够成功预测出 `w_8`。
-   对于第二个样本，它需要让这个极其相似的上下文向量，又能成功预测出 `w_9`。

为了同时满足这两个看似矛盾的目标，优化算法（如梯度下降）会找到一个“捷径”：**如果 `w_8` 和 `w_9` 的词向量本身就足够接近，那么模型就能用一个相似的上下文向量同时很好地预测出它们俩**。

这个过程会在整个语料库上不断重复。当两个不同的词（例如“笔记本”和“电脑”）因为语言习惯而频繁地出现在相似的上下文语境中时（例如，都和“键盘”、“屏幕”、“CPU”等词一起出现），模型为了降低整体的预测损失，就会“被迫”将“笔记本”和“电脑”的词向量在向量空间中移动到彼此靠近的位置。

这就是Word2Vec能够学习到语义的原因：**上下文的高度重叠，迫使语义相似的中心词的向量彼此靠近**。

**训练目标与余弦相似度：**

从数学角度看，模型的最终目标是让 `V` 维得分向量中，对应真实目标词的那个维度的值最大化。这个得分值，是由上下文向量 `x`（CBOW）或中心词向量 `v_c`（Skip-gram）与输出矩阵 `W'` 中对应目标词的行向量 `v'_{target}` 进行**点积**得到的。

$$
\text{score} = x \cdot v'_{target}
$$

两个向量的点积是余弦相似度公式的分子部分。因此，最大化这个点积得分，在几何上就是在**促使上下文向量 `x` 和目标词向量 `v'_{target}` 的夹角尽可能小，即让它们在空间上更接近**。这为之前通过滑动窗口得到的直觉，提供了坚实的数学解释。

## 4. 训练优化：加速学习过程

无论是CBOW还是Skip-gram，如果采用传统的**Softmax归一化计算**，都会面临一个巨大的计算瓶颈。

**⚠️ Softmax的计算瓶颈**

Softmax函数的计算公式为：

$$
P(w_O|w_I) = \frac{\exp(v'_{w_O}{}^T v_{w_I})}{\sum_{w=1}^{V} \exp(v'_{w}{}^T v_{w_I})}
$$

问题出在分母上：为了计算一个词的输出概率，需要遍历词典中的**所有**词（$V$通常是几十万甚至上百万），计算它们与输入词的得分并求和。这意味着，模型每处理一个训练样本，就需要进行一次涉及整个词典的庞大计算，这使得训练过程异常缓慢。

为了解决这个计算瓶颈，Word2Vec提出了两种优化策略[^4]，用更高效的计算方式来替代传统的Softmax计算。

### 4.1 Hierarchical Softmax

**Hierarchical Softmax** 是一种将计算量巨大的多分类问题巧妙转化为一系列二分类问题的优化技巧。它利用**霍夫曼树（Huffman Tree）**来组织词典。

该方法不再直接计算一个词在整个词典上的概率分布，而是将其转化为从霍夫曼树的根节点开始，经过一系列决策最终到达目标词对应的叶子节点的过程。

#### 4.1.1 霍夫曼树的构建

-   **结构**：这是一棵二叉树，词典中的每个单词都对应一个**叶子节点**。
-   **构建依据**：树的构建基于语料库中单词的**出现频率**。频率越高的单词，其在树中的路径越短；频率越低的单词，路径越长。
-   **过程**：构建时，选择当前权重（频率）最小的两个节点合并成一个新节点（父节点），新节点的权重为两者之和。重复此过程，直到所有节点合并成一棵树。

#### 4.1.2 工作原理

-   **路径即编码**：从根节点到任意一个叶子节点（单词）都有一条唯一的路径。可以预先定义好规则（如：左子树为0，右子树为1），这样每个单词都可以得到一个唯一的01编码。
-   **二分类替代多分类**：在预测时，模型从根节点出发，在路径上的每个**非叶子节点**（内部节点）执行一次**二分类**任务，以决定是走向左子节点还是右子节点。
-   **参数学习**：每个内部节点都关联着一个可学习的向量`θ`。这个向量与输入的上下文向量`x`进行点积，再通过`Sigmoid`函数，来计算走向某个方向（如右子节点）的概率 `P(right|x, θ) = σ(x^Tθ)`。走向左边的概率自然就是 `1 - P(right)`。
-   **最终概率**：一个单词的最终输出概率，等于其在霍夫曼树上从根节点到对应叶子节点的**路径上所有二分类概率的乘积**。

![Hierarchical Softmax](https://raw.githubusercontent.com/dalvqw/my-image-bed/main/img/202408021110543.png)

#### 4.1.3 优势与总结

-   **降维打击**：它将计算复杂度从原来的 `O(V)`（V是词典大小）显著降低到了 `O(log V)`（树的平均深度）。
-   **高效更新**：在反向传播时，只需要更新从根节点到目标词路径上涉及到的内部节点的参数 `θ`，而无需触及其他节点，大大提升了训练效率。
-   **高频词优待**：由于高频词路径更短，模型在处理这些常见词时计算量更小，符合语言规律。

### 4.2 负采样

**负采样（Negative Sampling, NES）** 是比Hierarchical Softmax更常用且通常效果更好的一种优化方法。其核心思想是：**将原来复杂的多分类问题，转化为一个简单的二分类问题**。模型不再试图预测"正确的"那个词，而是学习去区分"真实的"上下文词和"假的"噪声词。

对于一个训练样本 `(中心词, 上下文词)`，将其视为一个**正样本 (Positive Sample)**。同时，再从词典中**随机**抽取 `k` 个与该上下文无关的词，构成 `k` 个**负样本 (Negative Samples)**。现在的任务变成了一个二分类任务：给定一对词 `(w, c)`，判断 `c` 是否是 `w` 的真实上下文。

**2. 目标函数**

我们希望最大化正样本的概率，同时最小化所有负样本的概率。这通过 `Sigmoid` 函数来实现。对于一个样本 `(w, c)`，其损失函数（使用对数似然）为：

$$
J = \log\sigma(v_c^T v_w) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}[\log\sigma(-v_{w_i}^T v_w)]
$$

-   $v_c$ 和 $v_w$ 分别是中心词和上下文词的向量。
-   第一项是最大化真实上下文词的概率。
-   第二项是针对 `k` 个负样本，最大化它们"不是"上下文的概率，即最小化它们"是"上下文的概率。

**3. 负采样策略**

如何高效地抽取负样本是该方法的关键。如果纯随机（均匀分布）地从词典中抽取，那么像 "the", "a", "in" 这样的高频词被抽中的概率会非常大，但它们提供的信息有限。

为了解决这个问题，Word2Vec的作者提出了一种启发式的采样方法，其概率分布与词频有关：

$$
P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=0}^{n} (f(w_j)^{3/4})}
$$

-   $f(w_i)$ 是单词 $w_i$ 的词频。
-   `3/4` 这个超参数是经验之谈，它的作用是**平滑词频分布**：
    -   **降低**了高频词被抽中的概率。
    -   **提高**了低频词被抽中的概率。
-   这样一来，模型能够更好地学习到那些不常见但同样重要的词的表示，同时避免了被少数高频词主导训练过程。

## 5. Word2Vec的局限性：静态词向量

尽管Word2Vec是里程碑式的算法，但存在一个根本性的局限性——它产生的是**静态词向量（Static Word Vectors）**。

1.  **上下文无关**
    -   对于词典中的任意一个词，Word2Vec只会生成一个**固定**的向量表示。这个向量是在整个语料库上训练得到的“平均”语义，与该词出现的具体上下文无关。
    -   这直接导致了Word2Vec**无法解决一词多义**的问题。例如，“小米”这个词，无论是在“农民伯伯正在收割小米”的语境中，还是在“小米公司发布了新手机”的语境中，Word2Vec赋予它的词向量都是**完全相同**的。

2.  **静态的本质**
    -   Word2Vec的输出是一个巨大的查询表。训练完成后，这个表就固定下来了。在使用时，只是根据单词ID去查找对应的行向量，整个过程不涉及对上下文的动态分析。

## 参考文献

[^1]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv:1301.3781. [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)

[^2]: Harris, Z. S. (1954). *Distributional structure*. Word, 10(2-3), 146-162.

[^3]: Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). *Indexing by latent semantic analysis*. Journal of the American Society for Information Science, 41(6), 391-407.

[^4]: Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). *Distributed Representations of Words and Phrases and their Compositionality*. arXiv:1310.4546. [https://arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)
