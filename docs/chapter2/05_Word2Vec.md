# 第三节 从主题模型到Word2Vec

无论是哑编码还是序号化，它们本身都存在一个根本性的缺陷：无法表达词与词之间的 **语义关系**。在这些表示方法中，不同词的向量通常是正交的（如One-Hot编码中点积为0），或者其ID大小关系是随机的，这导致模型无法从输入层面理解“国王”与“女王”的语义比“国王”与“苹果”更近。

为了解决这个问题，**分布式表示（Distributed Representation）** 被提出，目的是将词语映射到一个 **低维、稠密、且蕴含丰富语义信息** 的连续向量空间中。本节将首先介绍一种基于传统机器学习思想的分布式表示方法——**主题模型**，并在此基础上，引出真正开启了 NLP 新时代的经典算法——**Word2Vec**。

## 一、寻找理想的词向量

理想中的词向量需要同时满足两个主要目标：

1.  **语义蕴含**：向量之间的距离（如余弦相似度或欧氏距离）能够度量词语之间的语义相似度。这背后的原理是 **分布式假设** 的朴素应用：**如果两个词经常在相似的上下文中共同出现（Co-occurrence），那么它们的向量在空间上应该是彼此靠近的**。例如，"被子"和"床铺"经常一起出现，它们的向量就应该接近；而"椰子"和"企鹅"则应该相互远离。语义的相似性，正是这种"上下文共现"关系在向量空间中的自然体现。
2.  **低维稠密** ：摆脱“维度灾难”。词向量的维度应该是一个较小的、可控的超参数，而不是动辄数万的词典大小。并且，向量中的每一维都应是有意义的浮点数，而非绝大部分为0的稀疏表示。

为实现这一目标，研究者们探索了不同的技术路径。其中，一条是基于 **全局文档统计** 的 **主题模型**；另一条则是后来居上、并成为主流的、基于 **局部上下文预测** 的 **神经网络模型**。

## 二、主题模型

主题模型是基于机器学习和传统数学思想的经典方法。它尝试从宏观的视角，通过分析大量文档的词语共现统计，来发现词语间的潜在语义关联。其关键假设是：**一篇文档由多个"主题"按一定比例混合而成，而一个主题又由多个"词语"按一定概率组成**。词语之所以会一同出现在某篇文档中，是因为它们都在共同描述这篇文章所包含的某个或某些潜在主题。

例如，一篇关于"人工智能"的文档，会高频出现"深度学习"、"Transformer"、"注意力机制"等词。正是因为这些词都强关联于"AI技术"这个主题，它们才频繁地共现在一起。所以，**一个词的向量，就可以用它与各个主题的关联强度来表示**。这其中最核心的技术，就是 **矩阵分解（Matrix Factorization）**。

### 2.1 SVD矩阵分解

该方法将获取词向量的过程，巧妙地转化成了一个矩阵分解问题。

<div align="center">
<img src="./images/3_2_2.gif" alt="矩阵分解图示" width="800">
</div>

1.  **构建“词-文档”矩阵**

    首先，以整个语料库为基础，构建一个巨大的 **词-文档矩阵 $X$**。这个矩阵的每一行代表一个词，每一列代表一篇文档，矩阵中 $X(i, j)$ 的值是词 $i$ 在文档 $j$ 中的重要性权重，可以使用 **TF-IDF** 值来填充。这个矩阵通常是 **非常巨大且高度稀疏** 的。

2.  **矩阵分解**

    从线性代数的角度看，这个巨大的稀疏矩阵 $X$ 可以被近似分解为两个更小的、更稠密的矩阵的乘积。最常用的分解技术之一是 **奇异值分解（SVD）**。

    $$
    X_{m \times n} \approx W_{topic\, m \times k} \times H_{topic\, k \times n}
    $$

    -   $X_{m \times n}$ 是原始的词-文档矩阵， $m$ 是词典大小， $n$ 是文档数量。
    -   $k$ 是一个远小于 $m$ 和 $n$ 的超参数，代表期望发现的 **潜在主题数量**。
    -   $W_{topic}$ 表示 **“词-主题矩阵”**。它的每一行，都是一个 $k$ 维的稠密向量，表示一个词与 $k$ 个主题的关联度。
    -   $H_{topic}$ 表示 **“文档-主题矩阵”**。它的每一列，都是一个 $k$ 维的稠密向量，表示一篇文档在 $k$ 个主题上的分布。

3.  **获取词向量**

    分解完成后，我们真正关心的是 **“词-主题”矩阵 $W_{topic}$**。这个矩阵的 **每一行**，正是我们需要的 **词向量**。
    -   它将原来 $m$ 维的One-Hot编码，降维到了 $k$ 维。
    -   它是一个稠密向量，每个维度都代表了与某个主题的关联强度。
    -   它蕴含了语义信息。如果两个词（如"CPU"和"GPU"）经常在描述"硬件"这个主题的文档中共同出现，那么SVD分解的结果会使它们在对应"硬件"主题的那个维度上都有很高的值，从而使它们的最终词向量在空间上非常接近。

### 2.2 聚类视角理解主题模型

从机器学习的角度看，主题模型本质上是一个 **聚类算法**：

-   **文档聚类**：文档主题矩阵 $H_{topic}$ 将 $n$ 篇文档聚成 $k$ 个主题类别。每篇文档都有一个 $k$ 维向量，表示其属于各个主题的 **置信度** 或 **软分配**。例如，一篇文档可能70%属于"AI技术"主题，30%属于"数学理论"主题。

-   **词语倾向性**：单词主题矩阵 $W_{topic}$ 揭示了词语的主题倾向。有些词语（如"深度学习"、"Transformer"、"注意力机制"）更倾向于描述"AI技术"主题，而另一些词语（如"坦克"、"导弹"、"战争"）则更倾向于描述"军事"主题。

-   **语义相关性的来源**：正因为描述同一主题的词语会在相同的主题维度上有较高的权重，它们的词向量才会在空间中彼此靠近，从而实现了语义信息的捕捉。

### 2.3 总结与局限

主题模型（如其更广为人知的名字 **LSA, Latent Semantic Analysis** [^3]）通过对全局的"词-文档"共现矩阵进行分解，成功地将词语映射到了一个低维的"主题空间"，从而得到了能够表达语义的稠密词向量。相关的技术还包括 **PCA** 和 **NMF（非负矩阵分解）**。

然而，这种方法也存在明显的局限性：
1.  **计算代价高昂**：对一个大型语料库进行SVD分解，计算量和内存开销都极大。
2.  **依赖全局统计**：它依赖的是全局的、粗粒度的文档级别共现信息，忽略了词语在句子中的局部上下文和词序信息，这使得它难以捕捉更精细的语义关系。
3.  **难以集成**：这种"先统计，再分解"的流程，很难与现代的深度学习模型进行端到端的联合训练。

## 三、Word2Vec

与主题模型从全局文档统计中挖掘主题不同，由Google在2013年提出的Word2Vec算法[^1]，将视角聚焦于词语的 **局部上下文**。它的思想来源于语言学中的 **分布式假设（Distributional Hypothesis）[^2]：一个词的含义，由其上下文中的词语所决定**。

换言之，如果两个词的上下文经常是相似的，那么这两个词的语义就是相近的。Word2Vec正是这一思想的数学实现。

### 3.1 Word2Vec概述

Word2Vec 通常被认为是一种 **浅层神经网络模型（Shallow Neural Network）**。其"浅层"体现在网络结构的简单性上：相比深度神经网络，Word2Vec只包含输入层、一个隐藏层（词向量查找表）和输出层，没有多层的复杂结构。这种简洁的设计使得 Word2Vec 的计算非常高效，甚至无需依赖大型深度学习框架即可实现。

**目标与手段的分离：**

理解Word2Vec的关键在于区分其 **最终目标** 与 **实现手段**。神经网络结构本身只是获取词向量的一种方式， **并非模型的最终目的**。

-   **最终目标** ：获取一个高质量的 **词向量查询表**。这本质上是一个巨大的矩阵 $W_{in}$ ， 矩阵的每一行就是对应单词的稠密向量。
-   **实现手段** ：为了学习到这个查询表，Word2Vec设计了一个巧妙的"伪任务"—— **根据上下文预测中心词**（或反之），并在这个过程中，将词向量查询表作为 **模型参数** 进行训练和优化。

训练结束后，用于执行预测任务的神经网络本身会被 **丢弃**。不会使用它的输出，真正需要和保留的，只有作为其内部参数的那个 **词向量查询表**。

### 3.2 可学习的词向量矩阵

从数学上看，将一个单词的ID转换为其稠密向量的过程，在概念上可以分解为两步：

1.  **输入**：一个代表单词的ID，例如 $3$ 。
2.  **哑编码**：将ID $3$ 转换为一个维度等于词典大小 $|V|$ 的高维稀疏向量，例如 $[0, 0, 0, 1, 0, \ldots]$ ， 其中只有第 3 个位置为 1。
3.  **矩阵乘法**：用这个One-Hot向量去乘以一个巨大的、可学习的 **参数矩阵 $W_{in}$**（尺寸为 $|V|\times D$）。这个矩阵 $W_{in}$ 就是最终想要得到的词向量查询表。

由于One-Hot向量只有一个位置是1，这个矩阵乘法的结果，等效于直接从矩阵 $W_{in}$ 中 **“抽取”出索引为 3 的那一行**。

```math
\begin{bmatrix} 0 & 0 & 0 & \color{#42b983}{1} & 0 & 0 \end{bmatrix}
\times
\begin{bmatrix}
2 & 8 & 5 & 3 & 1 \\
9 & 4 & 7 & 2 & 6 \\
3 & 1 & 8 & 5 & 0 \\
5 & 6 & 2 & 9 & 4 \\
8 & 0 & 3 & 7 & 2 \\
4 & 2 & 9 & 6 & 1
\end{bmatrix}
=
\begin{bmatrix} \color{#42b983}{5} & \color{#42b983}{6} & \color{#42b983}{2} & \color{#42b983}{9} & \color{#42b983}{4} \end{bmatrix}
```

在实践中，为了极大地提升效率，程序并不会真的执行稀疏的矩阵乘法，而是直接实现一个 **查询** 操作：根据输入的单词ID，直接从 $W_{in}$ 矩阵中获取对应的行向量。理解这里的关键在于，这个参数矩阵 $W_{in}$ **本身就是学习的目标**。它被随机初始化，并在后续的训练过程中，通过CBOW或Skip-gram这样的 **预测任务** 不断地被优化和调整。

### 3.3 两种经典模型

Word2Vec包含两种具体的实现模型：CBOW和Skip-gram。两者在任务设计上恰好相反，但最终都实现了相同的目标：通过 **训练过程** 得到一个高质量的词向量查询表。


#### 3.3.1 CBOW 模型详解

CBOW (Continuous Bag-of-Words) 的任务是 **“根据上下文预测中心词”**。

<div align="center">
<img src="./images/3_3_3_1.svg" alt="CBOW" width="600">
</div>

**数据流与维度变化：**

假设 $B$ 是批大小， $S$ 是上下文单词数量， $|V|$ 是词典大小， $D$ 是词向量维度。

1.  **输入层**：上下文窗口内的所有词对应的ID。
    -   **数据形状**： $(B, S)$ ， 例如输入为 $[[1, 8, 10, 13, 14, 16], [8, 5, 14, 16, 18, 10]]$ 。

2.  **词向量转换**：
    -   **操作**：将每个单词ID转换为对应的 $D$ 维词向量。这通过输入矩阵 $W_{in}$ （大小为 $|V|\times D$ ）实现，相当于公式中的 $v_{c-k} = W_{in}w_{c-k}$ 。
    -   **数据形状变化**： $(B, S)$ -> $(B, S, D)$ 。

3.  **上下文向量求和**：
    -   **操作**：将一个样本中 $S$ 个上下文单词的 $D$ 维向量进行求和，得到上下文向量 $h$ 。
    -   **数据形状变化**： $(B, S, D)$ -> $(B, D)$ 。

4.  **输出得分计算**：
    -   **操作**：将 $D$ 维的上下文向量与输出矩阵 $W_{out}$ （大小为 $D\times |V|$ ）相乘，得到 $|V|$ 维的得分向量，对应公式中的 $z_c = W_{out}^T\, h$ （或按实现以右乘形式表示）。
    -   **数据形状变化**： $(B, D)$ -> $(B, |V|)$ 。

5.  **损失计算**：
    -   **操作**：使用 Softmax 将得分转换为概率分布，然后计算与真实中心词之间的交叉熵损失。
    -   **优化**：通过反向传播更新输入矩阵 $W_{in}$ 和输出矩阵 $W_{out}$ 。最终需要的是训练好的输入矩阵 $W_{in}$ 。

**公式：**

1.  **词向量转换**：对于上下文中的每个词 $w_{c-k}$ ，从输入矩阵 $W_{in}$ 中获取对应的词向量： $v_{c-k} = W_{in}w_{c-k}$

2.  **上下文向量**：将上下文窗口中所有词的词向量求和： $h = v_{c-m} + \cdots + v_{c-1} + v_{c+1} + \cdots + v_{c+m}$

3.  **输出得分**：将上下文向量与输出矩阵 $W_{out}$ 相乘： $z_c = W_{out}^T h$

4.  **损失函数**：模型的优化目标是最小化负对数似然：

    $$
    \begin{aligned}
    \text{minimize } J &= -\log P(w_c | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}) \\
    &= -\log P(u_c | h) \\
    &= -\log \frac{\exp(u_c^T h)}{\sum_{j=1}^{|V|} \exp(u_j^T h)} \\
    &= -u_c^T h + \log \sum_{j=1}^{|V|} \exp(u_j^T h)
    \end{aligned}
    $$

    其中 $u_c$ 是目标中心词的输出向量， $h$ 是上下文向量。

#### 3.3.2 Skip-gram 模型详解

对比 CBOW , Skip-gram 的任务恰好相反：**“根据中心词预测上下文”**。它将一个预测任务，分解成了多个独立的子任务。

**数据流与维度变化：**

1.  **输入层**：中心词的 ID 。
    -   **数据形状**： $(B, 1)$ 。
2.  **词向量转换**：
    -   **操作**：将中心词 ID 通过查表（输入矩阵 $W_{in}$ ）转换为其 $D$ 维词向量 $v_{w_c}$ 。
    -   **数据形状变化**： $(B, 1)$ -> $(B, 1, D)$ 。
3.  **输出得分计算**：
    -   **操作**：将 $D$ 维的中心词向量 $v_{w_c}$ ， 与输出矩阵 $W_{out}$ 相乘，得到一个 $|V|$ 维的得分向量。
    -   **数据形状变化**： $(B, 1, D)$ -> $(B, |V|)$ 。
4.  **损失计算 (多标签问题)**：
    -   **操作**：与 CBOW 不同，这里的目标是预测多个上下文单词（例如， $S$ 个）。因此，这一个 $|V|$ 维的得分向量将被 **复用 $S$ 次**，分别与 $S$ 个真实的上下文单词计算损失。
    -   **优化**：将 $S$ 个位置的损失 **全部相加**，然后进行反向传播，同时更新 $W_{in}$ 和 $W_{out}$ 。因为一个输入对应多个输出标签，这本质上是一个 **多标签分类问题**。在实际运用中，通常将其分解为多个独立的单标签分类任务：对每个上下文词位置，都使用 Softmax 进行一次独立的预测，然后将所有位置的损失相加。

**公式：**

1.  **词向量转换**：对于中心词 $w_c$，从输入矩阵 $W_{in}$ 中获取对应的词向量： $v_c = W_{in}w_c$

2.  **输出得分计算**：将中心词向量与输出矩阵 $W_{out}$ 相乘： $z = W_{out}^T v_c$

3.  **损失函数**：模型的优化目标是最小化负对数似然。完整的数学推导过程如下：

    $$
    \begin{aligned}
    \text{minimize } J &= -\log P(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} P(w_{c-m+j} | w_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} P(u_{c-m+j} | v_c) \\
    &= -\log \prod_{j=0, j \neq m}^{2m} \frac{\exp(u_{c-m+j}^T v_c)}{\sum_{k=1}^{|V|} \exp(u_k^T v_c)} \\
    &= -\sum_{j=0, j \neq m}^{2m} u_{c-m+j}^T v_c + 2m \log \sum_{k=1}^{|V|} \exp(u_k^T v_c)
    \end{aligned}
    $$

    其中 $v_c$ 是中心词的输入向量， $u_{c-m+j}$ 是上下文词的输出向量。

> Skip-gram为每个"中心词-上下文词"对都创建了一个独立的学习任务，这使得它能够更好地学习到词与词之间更精细的关系。在处理 **低频词** 和 **大数据集** 时，通常能得到质量更高的词向量，但由于其任务量是CBOW的 $S$ 倍，训练速度相对较慢。

#### 3.3.3 滑动窗口的直观理解

在了解了模型的基本构造后，可以深入探讨Word2Vec是如何真正捕捉到语义的。以CBOW模型为例，其关键在于 **滑动窗口** 机制如何生成大量高度重叠的训练样本。

假设有一个很长的句子，并设窗口大小为 $k=7$（中心词左右各 7 个词）。通过在文本上滑动该窗口可以生成大量训练样本：对于 CBOW 任务，当窗口中心位于第 8 个单词时，模型使用上下文 $[w_1, \ldots, w_7]$ 与 $[w_9, \ldots, w_{15}]$ 预测 $w_8$；随后窗口右移一格，中心变为第 9 个单词，模型使用新的上下文 $[w_2, \ldots, w_8]$ 与 $[w_{10}, \ldots, w_{16}]$ 预测 $w_9$。比较这两次样本可见，它们有 12 个上下文词 **完全相同**（ $w_2 \sim w_7$ 与 $w_{10} \sim w_{15}$），因此两者的 **上下文向量**（所有上下文词向量之和）在初始时就非常相似。

模型的目标是：
-   对于第一个样本，它需要调整参数，使得相似的上下文向量能够成功预测出 $w_8$。
-   对于第二个样本，它需要让这个极其相似的上下文向量，又能成功预测出 $w_9$。

为了同时满足这两个看似矛盾的目标，优化算法（如梯度下降）会找到一个“捷径”：**如果 $w_8$ 和 $w_9$ 的词向量本身就足够接近，那么模型就能用一个相似的上下文向量同时很好地预测出它们俩**。这一现象在整个语料库中不断重复：当两个不同的词（如“笔记本”和“电脑”）因语言习惯而频繁出现在相似的上下文（如与“键盘”“屏幕”“CPU”等共现）时，为了降低总体损失，模型会将它们的词向量在空间中推向彼此靠近的位置，从而形成语义相似性。

**训练目标与余弦相似度：**

从数学角度看，模型的最终目标是让 $|V|$ 维得分向量中，对应真实目标词的那个维度的值最大化。这个得分值，是由上下文向量 $x$（CBOW）或中心词向量 $v_c$（Skip-gram）与输出矩阵 $W_{out}$ 中对应目标词的行向量 $u_{target}$ 进行 **点积** 得到的。

$$
\text{score} = x \cdot u_{target}
$$

两个向量的点积是余弦相似度公式的分子部分。因此，最大化这个点积得分，在几何上就是在 **促使上下文向量 $x$ 和目标词向量 $u_{target}$ 的夹角尽可能小，即让它们在空间上更接近**。这为前述的滑动窗口机制，提供了数学解释。实际训练中，为避免对整个词表进行 Softmax 归一化带来的高开销，可以用 Hierarchical Softmax 与负采样等近似方法加速训练[^4]。

## 4. Word2Vec的局限性

尽管Word2Vec是里程碑式的算法，但存在一个根本性的局限性——它产生的是 **静态词向量**。

1.  **上下文无关**
    -   对于词典中的任意一个词，Word2Vec只会生成一个 **固定** 的向量表示。这个向量是在整个语料库上训练得到的“平均”语义，与该词出现的具体上下文无关。
    -   这直接导致了Word2Vec **无法解决一词多义** 的问题。例如，“小米”这个词，无论是在“农民伯伯正在收割小米”的语境中，还是在“小米公司发布了新手机”的语境中，Word2Vec赋予它的词向量都是 **完全相同** 的。

2.  **静态的本质**
    -   Word2Vec的输出是一个巨大的查询表。训练完成后，这个表就固定下来了。在使用时，只是根据单词ID去查找对应的行向量，整个过程不涉及对上下文的动态分析。

## 练习

- 根据已经学过的内容使用`20newsgroups`数据（from sklearn.datasets import fetch_20newsgroups）实现基于全连接的文本分类模型训练和推理代码。(参考思路，优先自己实现)[https://github.com/datawhalechina/base-nlp/blob/main/docs/chapter7/01_text_classification.md]

---

## 参考文献

[^1]: [Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation of Word Representations in Vector Space*. arXiv:1301.3781](https://arxiv.org/abs/1301.3781)

[^2]: [Harris, Z. S. (1954). *Distributional structure*. Word, 10(2-3), 146-162](https://doi.org/10.1080/00437956.1954.11659520)

[^3]: [Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). *Indexing by latent semantic analysis*. Journal of the American Society for Information Science, 41(6), 391-407](https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9)

[^4]: [Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). *Distributed Representations of Words and Phrases and their Compositionality*. arXiv:1310.4546](https://arxiv.org/abs/1310.4546)
