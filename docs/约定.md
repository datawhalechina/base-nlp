# 符号与实现约定

> 目的：统一数学符号与 PyTorch 实现的对齐关系，降低歧义。

---

## 一、基础符号 (Fundamental Symbols)

### 1.1 向量、维度与索引
- **向量 (Vectors)**
  - $x$: 输入向量
  - $h$: 隐藏状态向量
  - $o$: 输出向量
  - $b$: 偏置向量
    - $b_{ih}$: 输入-隐藏偏置 (input-hidden bias)
    - $b_{hh}$: 隐藏-隐藏偏置 (hidden-hidden bias)
    - $b_{o}$: 隐藏-输出偏置 (hidden-output bias)
- **维度 (Dimensions)**
  - $B$: 批大小 (Batch Size)
  - $T$: 序列长度 (Sequence Length)
  - $E$: 输入特征维度 (Input Feature Dimension)
  - $H$: 隐藏状态维度 (Hidden State Dimension)
- **索引 (Indices)**
  - $t$: 时间步 (Time Step)

### 1.2 权重矩阵 (Weight Matrices)
- $W$, $U$, $V$: 通用权重矩阵符号

---

## 二、特定模型/组件符号 (Specific Model/Component Notation)

### 2.1 词嵌入 (Word Embedding)
- $W_{in} \in \mathbb{R}^{|V| \times D}$
- $W_{out} \in \mathbb{R}^{D \times |V|}$
- $D$: 词向量维度 (Embedding Dimension)
- $|V|$: 词汇表大小 (Vocabulary Size)
- $v_w$: 中心词向量
- $u_w$: 目标词向量（亦称上下文词向量）
- $x$ 或 $h$: CBOW 的上下文聚合向量
- $u_{target}$: 目标词的输出向量

### 2.2 循环神经网络 (Recurrent Neural Networks)
- $h_t$: 时间步 $t$ 的隐藏状态
- $c_t$: (LSTM) 时间步 $t$ 的细胞状态
- $f_t$, $i_t$, $o_t$: (LSTM) 遗忘门、输入门、输出门向量

### 2.3 注意力机制 (Attention Mechanism)
- $Q$, $K$, $V$: 查询 (Query), 键 (Key), 值 (Value) 矩阵
- $d_k$, $d_v$: 键、值的维度

---

## 三、PyTorch 实现对齐 (PyTorch Implementation Alignment)

### 3.1 维度参数
- $B$ ↔ batch_size (张量中的批次维度)
- $T$ ↔ seq_len (张量中的序列长度维度)
- $E$ ↔ input_size
- $H$ ↔ hidden_size
- $D$ ↔ embedding_dim

### 3.2 RNN 参数
- $U$ ↔ weight_ih_lk
- $W$ ↔ weight_hh_lk
- $b_{ih}$ ↔ bias_ih_lk
- $b_{hh}$ ↔ bias_hh_lk
