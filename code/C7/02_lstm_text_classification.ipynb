{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 数据加载与探索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# 加载数据集并查看样本\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "train_dataset_raw = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "test_dataset_raw = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "sample = {\n",
        "    \"text_preview\": train_dataset_raw.data[0][:200],\n",
        "    \"label\": train_dataset_raw.target_names[train_dataset_raw.target[0]],\n",
        "}\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 训练集文本长度分布\n",
        "\n",
        "def basic_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9(),.!?\\'`]\", \" \", text)\n",
        "    text = re.sub(r\"([,.!?\\'`])\", r\" \\1 \", text)\n",
        "    return text.strip().split()\n",
        "\n",
        "train_text_lengths = [len(basic_tokenize(text)) for text in train_dataset_raw.data]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(train_text_lengths, bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Distribution of Text Lengths in Training Data')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# 计算所有词元的频率\n",
        "word_counts = Counter()\n",
        "for text in train_dataset_raw.data:\n",
        "    word_counts.update(basic_tokenize(text))\n",
        "\n",
        "# 获取频率并按降序排序\n",
        "frequencies = sorted(word_counts.values(), reverse=True)\n",
        "# 生成排名\n",
        "ranks = np.arange(1, len(frequencies) + 1)\n",
        "\n",
        "# 绘制对数坐标图\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.loglog(ranks, frequencies)\n",
        "plt.title('Rank vs. Frequency (Log-Log Scale)')\n",
        "plt.xlabel('Rank (Log)')\n",
        "plt.ylabel('Frequency (Log)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 定义所需组件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.token_to_id = {token: idx for token, idx in vocab.items()}\n",
        "\n",
        "    @staticmethod\n",
        "    def _tokenize_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-z0-9(),.!?\\'`]\", \" \", text)\n",
        "        text = re.sub(r\"([,.!?\\'`])\", r\" \\1 \", text)\n",
        "        tokens = text.strip().split()\n",
        "        return tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.token_to_id.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self._tokenize_text(text)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab_from_counts(word_counts, min_freq=5):\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab_from_counts(word_counts, min_freq=5)\n",
        "tokenizer = Tokenizer(vocab)\n",
        "{\"vocab_size\": len(tokenizer)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.processed_data = []\n",
        "\n",
        "        for text, label in tqdm(zip(texts, labels), total=len(labels)):\n",
        "            token_ids = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
        "            \n",
        "            if len(token_ids) <= self.max_len:\n",
        "                self.processed_data.append({\"token_ids\": token_ids, \"label\": label})\n",
        "            else:\n",
        "                stride = max(1, int(self.max_len * 0.8))\n",
        "                for i in range(0, len(token_ids) - self.max_len + 1, stride):\n",
        "                    chunk = token_ids[i:i+self.max_len]\n",
        "                    self.processed_data.append({\"token_ids\": chunk, \"label\": label})\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    max_batch_len = max(len(item[\"token_ids\"]) for item in batch)\n",
        "    \n",
        "    batch_token_ids, batch_labels, batch_lengths = [], [], []\n",
        "\n",
        "    for item in batch:\n",
        "        token_ids = item[\"token_ids\"]\n",
        "        lengths = len(token_ids)\n",
        "        padding_len = max_batch_len - lengths\n",
        "        \n",
        "        padded_ids = token_ids + [0] * padding_len\n",
        "        batch_token_ids.append(padded_ids)\n",
        "        batch_labels.append(item[\"label\"])\n",
        "        batch_lengths.append(lengths)\n",
        "        \n",
        "    return {\n",
        "        \"token_ids\": torch.tensor(batch_token_ids, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
        "        \"lengths\": torch.tensor(batch_lengths, dtype=torch.long),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = TextClassificationDataset(train_dataset_raw.data, train_dataset_raw.target, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_dataset = TextClassificationDataset(test_dataset_raw.data, test_dataset_raw.target, tokenizer)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "{\"train_samples\": len(train_dataset), \"valid_samples\": len(valid_dataset), \"batch_size\": 32}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassifierLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, n_layers=1, dropout=0.5, bidirectional=False):\n",
        "        super(TextClassifierLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers=n_layers, \n",
        "                            dropout=dropout,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True)\n",
        "        \n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.classifier = nn.Linear(hidden_dim * num_directions, num_classes)\n",
        "        \n",
        "    def forward(self, token_ids, lengths):\n",
        "        embedded = self.embedding(token_ids)\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        \n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1,:,:]\n",
        "            \n",
        "        logits = self.classifier(hidden)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, criterion, train_loader, valid_loader, device, output_dir=\".\"):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.device = device\n",
        "        self.best_accuracy = 0.0\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.train_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch+1} [训练中]\"):\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            token_ids = batch[\"token_ids\"].to(self.device)\n",
        "            labels = batch[\"labels\"].to(self.device)\n",
        "            lengths = batch[\"lengths\"] # lengths在pack_padded_sequence中需要在cpu上\n",
        "            \n",
        "            outputs = self.model(token_ids, lengths)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def _evaluate(self, epoch):\n",
        "        self.model.eval()\n",
        "        correct_preds = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.valid_loader, desc=f\"Epoch {epoch+1} [评估中]\"):\n",
        "                token_ids = batch[\"token_ids\"].to(self.device)\n",
        "                labels = batch[\"labels\"].to(self.device)\n",
        "                lengths = batch[\"lengths\"]\n",
        "                \n",
        "                outputs = self.model(token_ids, lengths)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                \n",
        "                total_samples += labels.size(0)\n",
        "                correct_preds += (predicted == labels).sum().item()\n",
        "        \n",
        "        return correct_preds / total_samples\n",
        "\n",
        "    def _save_checkpoint(self, epoch, val_accuracy):\n",
        "        if val_accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            save_path = os.path.join(self.output_dir, \"best_model.pth\")\n",
        "            torch.save(self.model.state_dict(), save_path)\n",
        "            print(f\"新最佳模型已保存! Epoch: {epoch+1}, 验证集准确率: {val_accuracy:.4f}\")\n",
        "\n",
        "    def train(self, epochs, tokenizer, label_map):\n",
        "        self.train_losses = []\n",
        "        self.val_accuracies = []\n",
        "        for epoch in range(epochs):\n",
        "            avg_loss = self._run_epoch(epoch)\n",
        "            val_accuracy = self._evaluate(epoch)\n",
        "            \n",
        "            self.train_losses.append(avg_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{epochs} | 训练损失: {avg_loss:.4f} | 验证集准确率: {val_accuracy:.4f}\")\n",
        "            \n",
        "            self._save_checkpoint(epoch, val_accuracy)\n",
        "        \n",
        "        print(\"训练完成！\")\n",
        "        vocab_path = os.path.join(self.output_dir, 'vocab.json')\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(tokenizer.vocab, f, ensure_ascii=False, indent=4)\n",
        "           \n",
        "        labels_path = os.path.join(self.output_dir, 'label_map.json')\n",
        "        with open(labels_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(label_map, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"词典 ({vocab_path}) 和标签映射 ({labels_path}) 已保存。\")\n",
        "        return self.train_losses, self.val_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Predictor:\n",
        "    def __init__(self, model, tokenizer, label_map, device, max_len=128):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = label_map\n",
        "        self.id_to_label = {idx: label for label, idx in self.label_map.items()}\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def predict(self, text):\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
        "        chunks = []\n",
        "        if len(token_ids) <= self.max_len:\n",
        "            chunks.append(token_ids)\n",
        "        else:\n",
        "            stride = max(1, int(self.max_len * 0.8))\n",
        "            for i in range(0, len(token_ids) - self.max_len + 1, stride):\n",
        "                chunks.append(token_ids[i:i + self.max_len])\n",
        "        \n",
        "        chunk_lengths = [len(c) for c in chunks]\n",
        "        max_chunk_len = max(chunk_lengths) if chunk_lengths else 0\n",
        "        \n",
        "        padded_chunks = []\n",
        "        for chunk in chunks:\n",
        "            padding_len = max_chunk_len - len(chunk)\n",
        "            padded_chunks.append(chunk + [0] * padding_len)\n",
        "            \n",
        "        if not padded_chunks:\n",
        "            return \"无法预测（文本过短）\"\n",
        "\n",
        "        chunk_tensors = torch.tensor(padded_chunks, dtype=torch.long).to(self.device)\n",
        "        length_tensors = torch.tensor(chunk_lengths, dtype=torch.long) # lengths need to be on cpu\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(chunk_tensors, length_tensors)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        final_pred_id = torch.bincount(preds).argmax().item()\n",
        "        \n",
        "        final_pred_label = self.id_to_label[final_pred_id]\n",
        "        return final_pred_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 训练模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 超参数\n",
        "hparams = {\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "    \"embed_dim\": 128,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"num_classes\": len(train_dataset_raw.target_names),\n",
        "    \"n_layers\": 2,\n",
        "    \"dropout\": 0.5,\n",
        "    \"bidirectional\": True,\n",
        "    \"epochs\": 20,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"output_dir\": \"output_lstm\"\n",
        "}\n",
        "\n",
        "# 实例化\n",
        "model = TextClassifierLSTM(\n",
        "    vocab_size=hparams[\"vocab_size\"], \n",
        "    embed_dim=hparams[\"embed_dim\"], \n",
        "    hidden_dim=hparams[\"hidden_dim\"], \n",
        "    num_classes=hparams[\"num_classes\"],\n",
        "    n_layers=hparams[\"n_layers\"],\n",
        "    dropout=hparams[\"dropout\"],\n",
        "    bidirectional=hparams[\"bidirectional\"]\n",
        ").to(hparams[\"device\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n",
        "hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model, \n",
        "    optimizer, \n",
        "    criterion, \n",
        "    train_loader, \n",
        "    valid_loader, \n",
        "    hparams[\"device\"], \n",
        "    output_dir=hparams[\"output_dir\"]\n",
        ")\n",
        "\n",
        "label_map = {name: i for i, name in enumerate(train_dataset_raw.target_names)}\n",
        "\n",
        "lstm_train_losses, lstm_val_accuracies = trainer.train(epochs=hparams[\"epochs\"], tokenizer=tokenizer, label_map=label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(train_losses, val_accuracies, title_prefix=\"\"):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot Training Loss\n",
        "    ax1.plot(epochs, train_losses, 'bo-', label='Training Loss')\n",
        "    ax1.set_title(f'{title_prefix} Training Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot Validation Accuracy\n",
        "    ax2.plot(epochs, val_accuracies, 'ro-', label='Validation Accuracy')\n",
        "    ax2.set_title(f'{title_prefix} Validation Accuracy')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.grid(True)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.suptitle(f'{title_prefix} Training and Validation Metrics', fontsize=16)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(lstm_train_losses, lstm_val_accuracies, title_prefix=\"Base LSTM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载资源\n",
        "vocab_path = os.path.join(hparams[\"output_dir\"], 'vocab.json')\n",
        "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "    loaded_vocab = json.load(f)\n",
        "\n",
        "labels_path = os.path.join(hparams[\"output_dir\"], 'label_map.json')\n",
        "with open(labels_path, 'r', encoding='utf-8') as f:\n",
        "    label_map_loaded = json.load(f)\n",
        "\n",
        "# 实例化推理组件\n",
        "inference_tokenizer = Tokenizer(vocab=loaded_vocab)\n",
        "inference_model = TextClassifierLSTM(\n",
        "    vocab_size=len(inference_tokenizer),\n",
        "    embed_dim=hparams[\"embed_dim\"], \n",
        "    hidden_dim=hparams[\"hidden_dim\"], \n",
        "    num_classes=len(label_map_loaded),\n",
        "    n_layers=hparams[\"n_layers\"],\n",
        "    dropout=hparams[\"dropout\"],\n",
        "    bidirectional=hparams[\"bidirectional\"]\n",
        ").to(hparams[\"device\"])\n",
        "\n",
        "model_path = os.path.join(hparams[\"output_dir\"], \"best_model.pth\")\n",
        "inference_model.load_state_dict(torch.load(model_path, map_location=hparams[\"device\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = Predictor(\n",
        "    inference_model, \n",
        "    inference_tokenizer, \n",
        "    label_map_loaded, \n",
        "    hparams[\"device\"]\n",
        ")\n",
        "\n",
        "# 预测\n",
        "new_text = \"The doctor prescribed a new medicine for the patient's illness, focusing on its gpu accelerated healing properties.\"\n",
        "predicted_class = predictor.predict(new_text)\n",
        "\n",
        "{\"text\": new_text, \"pred\": predicted_class}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 过拟合解决方案"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class TextClassificationDatasetWithMasking(TextClassificationDataset):\n",
        "    \"\"\"\n",
        "    继承自TextClassificationDataset，并增加了在训练时随机遮盖Token的功能。\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128, is_train=False, mask_prob=0.15):\n",
        "        \"\"\"\n",
        "        新增参数:\n",
        "        is_train (bool): 是否为训练集。只对训练集进行遮盖。\n",
        "        mask_prob (float): 单个Token被遮盖的概率。\n",
        "        \"\"\"\n",
        "        super().__init__(texts, labels, tokenizer, max_len)\n",
        "        self.is_train = is_train\n",
        "        self.mask_prob = mask_prob\n",
        "        # 获取<UNK>的ID，用于替换\n",
        "        self.unk_token_id = tokenizer.token_to_id.get(\"<UNK>\", 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 首先调用父类的方法获取原始数据，并创建副本\n",
        "        item = super().__getitem__(idx).copy()\n",
        "        \n",
        "        # 如果是训练集，则执行随机遮盖\n",
        "        if self.is_train:\n",
        "            token_ids = item['token_ids']\n",
        "            masked_token_ids = []\n",
        "            for token_id in token_ids:\n",
        "                # PAD token (ID=0) 不应被遮盖\n",
        "                if token_id != 0 and random.random() < self.mask_prob:\n",
        "                    masked_token_ids.append(self.unk_token_id)\n",
        "                else:\n",
        "                    masked_token_ids.append(token_id)\n",
        "            # 用遮盖后的序列替换原始序列\n",
        "            item['token_ids'] = masked_token_ids\n",
        "            \n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainerWithEarlyStopping(Trainer):\n",
        "    \"\"\"\n",
        "    继承自Trainer，并增加了提前停止的功能。\n",
        "    \"\"\"\n",
        "    def __init__(self, model, optimizer, criterion, train_loader, valid_loader, device, output_dir=\".\", patience=3):\n",
        "        \"\"\"\n",
        "        新增参数:\n",
        "        patience (int): 连续patience个epoch验证集表现没有提升后，停止训练。\n",
        "        \"\"\"\n",
        "        super().__init__(model, optimizer, criterion, train_loader, valid_loader, device, output_dir)\n",
        "        self.patience = patience\n",
        "        self.epochs_no_improve = 0\n",
        "\n",
        "    def train(self, epochs, tokenizer, label_map):\n",
        "        self.train_losses = []\n",
        "        self.val_accuracies = []\n",
        "        for epoch in range(epochs):\n",
        "            avg_loss = self._run_epoch(epoch)\n",
        "            val_accuracy = self._evaluate(epoch)\n",
        "            \n",
        "            self.train_losses.append(avg_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{epochs} | 训练损失: {avg_loss:.4f} | 验证集准确率: {val_accuracy:.4f}\")\n",
        "            \n",
        "            # 记录调用_save_checkpoint之前的最佳准确率\n",
        "            current_best = self.best_accuracy\n",
        "            self._save_checkpoint(epoch, val_accuracy)\n",
        "            \n",
        "            # 检查最佳准确率是否有更新\n",
        "            if self.best_accuracy > current_best:\n",
        "                self.epochs_no_improve = 0\n",
        "            else:\n",
        "                self.epochs_no_improve += 1\n",
        "            \n",
        "            # 如果达到patience，则提前停止\n",
        "            if self.epochs_no_improve >= self.patience:\n",
        "                print(f\"\\n提前停止于 Epoch {epoch+1}，因为验证集准确率连续 {self.patience} 轮未提升。\")\n",
        "                break\n",
        "        \n",
        "        print(\"\\n训练完成！\")\n",
        "        # 保存词典和标签映射\n",
        "        vocab_path = os.path.join(self.output_dir, 'vocab.json')\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(tokenizer.vocab, f, ensure_ascii=False, indent=4)\n",
        "           \n",
        "        labels_path = os.path.join(self.output_dir, 'label_map.json')\n",
        "        with open(labels_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(label_map, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"词典 ({vocab_path}) 和标签映射 ({labels_path}) 已保存。\")\n",
        "        return self.train_losses, self.val_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 构建带有数据增强的Dataset和DataLoader\n",
        "# 训练集使用新的Dataset，并开启is_train和mask_prob\n",
        "train_dataset_reg = TextClassificationDatasetWithMasking(\n",
        "    train_dataset_raw.data, \n",
        "    train_dataset_raw.target, \n",
        "    tokenizer,\n",
        "    is_train=True,\n",
        "    mask_prob=0.15\n",
        ")\n",
        "train_loader_reg = DataLoader(train_dataset_reg, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 验证集保持不变，不进行数据增强\n",
        "# 注意：valid_dataset是之前已经创建好的不带masking的dataset\n",
        "valid_loader_reg = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "{len(train_dataset_reg)}, 验证集样本数: {len(valid_dataset)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 定义新的超参数\n",
        "hparams_reg = hparams.copy()\n",
        "hparams_reg[\"output_dir\"] = \"output_lstm_regularized\"\n",
        "hparams_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 实例化模型、优化器\n",
        "model_reg = TextClassifierLSTM(\n",
        "    vocab_size=hparams_reg[\"vocab_size\"], \n",
        "    embed_dim=hparams_reg[\"embed_dim\"], \n",
        "    hidden_dim=hparams_reg[\"hidden_dim\"], \n",
        "    num_classes=hparams_reg[\"num_classes\"],\n",
        "    n_layers=hparams_reg[\"n_layers\"],\n",
        "    dropout=hparams_reg[\"dropout\"],\n",
        "    bidirectional=hparams_reg[\"bidirectional\"]\n",
        ").to(hparams_reg[\"device\"])\n",
        "\n",
        "optimizer_reg = torch.optim.Adam(model_reg.parameters(), lr=hparams_reg[\"learning_rate\"])\n",
        "\n",
        "# 损失函数可以复用之前定义的 criterion\n",
        "model_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. 实例化新的Trainer并开始训练\n",
        "trainer_reg = TrainerWithEarlyStopping(\n",
        "    model_reg, \n",
        "    optimizer_reg, \n",
        "    criterion, \n",
        "    train_loader_reg, \n",
        "    valid_loader_reg, \n",
        "    hparams_reg[\"device\"], \n",
        "    output_dir=hparams_reg[\"output_dir\"],\n",
        "    patience=3 # 设置patience为3\n",
        ")\n",
        "\n",
        "reg_lstm_train_losses, reg_lstm_val_accuracies = trainer_reg.train(epochs=hparams_reg[\"epochs\"], tokenizer=tokenizer, label_map=label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(reg_lstm_train_losses, reg_lstm_val_accuracies, title_prefix=\"Regularized LSTM\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
