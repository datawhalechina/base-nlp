{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 数据加载与探索"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载数据集并查看样本\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "train_dataset_raw = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "test_dataset_raw = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "sample = {\n",
        "    \"text_preview\": train_dataset_raw.data[0][:200],\n",
        "    \"label\": train_dataset_raw.target_names[train_dataset_raw.target[0]],\n",
        "}\n",
        "sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练集文本长度分布\n",
        "\n",
        "def basic_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9(),.!?\\'`]\", \" \", text)\n",
        "    text = re.sub(r\"([,.!?\\'`])\", r\" \\1 \", text)\n",
        "    return text.strip().split()\n",
        "\n",
        "train_text_lengths = [len(basic_tokenize(text)) for text in train_dataset_raw.data]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(train_text_lengths, bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Distribution of Text Lengths in Training Data')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# 计算所有词元的频率\n",
        "word_counts = Counter()\n",
        "for text in train_dataset_raw.data:\n",
        "    word_counts.update(basic_tokenize(text))\n",
        "\n",
        "# 获取频率并按降序排序\n",
        "frequencies = sorted(word_counts.values(), reverse=True)\n",
        "# 生成排名\n",
        "ranks = np.arange(1, len(frequencies) + 1)\n",
        "\n",
        "# 绘制对数坐标图\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.loglog(ranks, frequencies)\n",
        "plt.title('Rank vs. Frequency (Log-Log Scale)')\n",
        "plt.xlabel('Rank (Log)')\n",
        "plt.ylabel('Frequency (Log)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 定义所需组件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocab, do_lower_case=True):\n",
        "        self.vocab = vocab\n",
        "        self.token_to_id = {token: idx for token, idx in vocab.items()}\n",
        "        self.id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    @staticmethod\n",
        "    def _tokenize_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-z0-9(),.!?\\'`]\", \" \", text)\n",
        "        text = re.sub(r\"([,.!?\\'`])\", r\" \\1 \", text)\n",
        "        tokens = text.strip().split()\n",
        "        return tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return [self.token_to_id.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        if self.do_lower_case:\n",
        "            text = text.lower()\n",
        "        return self._tokenize_text(text)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab_from_counts(word_counts, min_freq=5):\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for word, count in word_counts.items():\n",
        "        if count >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.processed_data = []\n",
        "\n",
        "        for text, label in tqdm(zip(texts, labels), total=len(labels)):\n",
        "            token_ids = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
        "            \n",
        "            if len(token_ids) <= self.max_len:\n",
        "                self.processed_data.append({\"token_ids\": token_ids, \"label\": label})\n",
        "            else:\n",
        "                stride = self.max_len // 4\n",
        "                for i in range(0, len(token_ids) - self.max_len + 1, stride):\n",
        "                    chunk = token_ids[i:i+self.max_len]\n",
        "                    self.processed_data.append({\"token_ids\": chunk, \"label\": label})\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    max_batch_len = max(len(item[\"token_ids\"]) for item in batch)\n",
        "    \n",
        "    batch_token_ids, batch_labels = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        token_ids = item[\"token_ids\"]\n",
        "        padding_len = max_batch_len - len(token_ids)\n",
        "        \n",
        "        padded_ids = token_ids + [0] * padding_len\n",
        "        batch_token_ids.append(padded_ids)\n",
        "        batch_labels.append(item[\"label\"])\n",
        "        \n",
        "    return {\n",
        "        \"token_ids\": torch.tensor(batch_token_ids, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        \n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Linear(hidden_dim * 4, num_classes)\n",
        "        \n",
        "    def forward(self, token_ids):\n",
        "        # token_ids: [batch_size, seq_len]\n",
        "        embedded = self.embedding(token_ids) # -> [batch_size, seq_len, embed_dim]\n",
        "        token_features = self.feature_extractor(embedded) # -> [batch_size, seq_len, hidden_dim * 4]\n",
        "        \n",
        "        # --- Masked Average Pooling ---\n",
        "        padding_mask = (token_ids != self.embedding.padding_idx).float() # -> [batch_size, seq_len]\n",
        "        masked_features = token_features * padding_mask.unsqueeze(-1) # -> [batch_size, seq_len, hidden_dim * 4]\n",
        "        summed_features = torch.sum(masked_features, 1) # -> [batch_size, hidden_dim * 4]\n",
        "        real_lengths = padding_mask.sum(1, keepdim=True) # -> [batch_size, 1]\n",
        "        pooled_features = summed_features / torch.clamp(real_lengths, min=1e-9) # -> [batch_size, hidden_dim * 4]\n",
        "        \n",
        "        logits = self.classifier(pooled_features) # -> [batch_size, num_classes]\n",
        "        \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, criterion, train_loader, valid_loader, device, output_dir=\".\"):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.device = device\n",
        "        self.best_accuracy = 0.0\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch+1} [训练中]\"):\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            token_ids = batch[\"token_ids\"].to(self.device)\n",
        "            labels = batch[\"labels\"].to(self.device)\n",
        "            \n",
        "            outputs = self.model(token_ids)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def _evaluate(self, epoch):\n",
        "        self.model.eval()\n",
        "        correct_preds = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.valid_loader, desc=f\"Epoch {epoch+1} [评估中]\"):\n",
        "                token_ids = batch[\"token_ids\"].to(self.device)\n",
        "                labels = batch[\"labels\"].to(self.device)\n",
        "                \n",
        "                outputs = self.model(token_ids)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                \n",
        "                total_samples += labels.size(0)\n",
        "                correct_preds += (predicted == labels).sum().item()\n",
        "        \n",
        "        return correct_preds / total_samples\n",
        "\n",
        "    def _save_checkpoint(self, epoch, val_accuracy):\n",
        "        if val_accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            save_path = os.path.join(self.output_dir, \"best_model.pth\")\n",
        "            torch.save(self.model.state_dict(), save_path)\n",
        "            print(f\"新最佳模型已保存! Epoch: {epoch+1}, 验证集准确率: {val_accuracy:.4f}\")\n",
        "\n",
        "    def train(self, epochs, tokenizer, label_map):\n",
        "        for epoch in range(epochs):\n",
        "            avg_loss = self._run_epoch(epoch)\n",
        "            val_accuracy = self._evaluate(epoch)\n",
        "            \n",
        "            print(f\"Epoch {epoch+1}/{epochs} | 训练损失: {avg_loss:.4f} | 验证集准确率: {val_accuracy:.4f}\")\n",
        "            \n",
        "            self._save_checkpoint(epoch, val_accuracy)\n",
        "        \n",
        "        print(\"训练完成！\")\n",
        "        vocab_path = os.path.join(self.output_dir, 'vocab.json')\n",
        "        with open(vocab_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(tokenizer.vocab, f, ensure_ascii=False, indent=4)\n",
        "           \n",
        "        labels_path = os.path.join(self.output_dir, 'label_map.json')\n",
        "        with open(labels_path, 'w', encoding='utf-8') as f:\n",
        "           json.dump(label_map, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"词典 ({vocab_path}) 和标签映射 ({labels_path}) 已保存。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Predictor:\n",
        "    def __init__(self, model, tokenizer, label_map, device, max_len=128):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_map = label_map\n",
        "        self.id_to_label = {idx: label for label, idx in self.label_map.items()}\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def predict(self, text):\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
        "        chunks = []\n",
        "        if len(token_ids) <= self.max_len:\n",
        "            chunks.append(token_ids)\n",
        "        else:\n",
        "            stride = self.max_len // 2\n",
        "            for i in range(0, len(token_ids) - self.max_len + 1, stride):\n",
        "                chunks.append(token_ids[i:i + self.max_len])\n",
        "        \n",
        "        chunk_tensors = torch.tensor(chunks, dtype=torch.long).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(chunk_tensors)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "        vote_counts = Counter(preds.cpu().numpy())\n",
        "        final_pred_id = vote_counts.most_common(1)[0][0]\n",
        "        \n",
        "        final_pred_label = self.id_to_label[final_pred_id]\n",
        "        return final_pred_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 构建词典和DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_counts(word_counts, min_freq=5)\n",
        "tokenizer = Tokenizer(vocab, do_lower_case=True)\n",
        "{\"vocab_size\": len(tokenizer)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(train_dataset_raw.data, train_dataset_raw.target, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "valid_dataset = TextClassificationDataset(test_dataset_raw.data, test_dataset_raw.target, tokenizer)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "{\"train_samples\": len(train_dataset), \"valid_samples\": len(valid_dataset), \"batch_size\": 32}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 训练模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 超参数\n",
        "hparams = {\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "    \"embed_dim\": 128,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"num_classes\": len(train_dataset_raw.target_names),\n",
        "    \"epochs\": 10,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"output_dir\": \"output\"\n",
        "}\n",
        "\n",
        "# 实例化\n",
        "model = TextClassifier(\n",
        "    hparams[\"vocab_size\"], \n",
        "    hparams[\"embed_dim\"], \n",
        "    hparams[\"hidden_dim\"], \n",
        "    hparams[\"num_classes\"]\n",
        ").to(hparams[\"device\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n",
        "\n",
        "hparams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model, \n",
        "    optimizer, \n",
        "    criterion, \n",
        "    train_loader, \n",
        "    valid_loader, \n",
        "    hparams[\"device\"], \n",
        "    output_dir=hparams[\"output_dir\"]\n",
        ")\n",
        "\n",
        "label_map = {name: i for i, name in enumerate(train_dataset_raw.target_names)}\n",
        "\n",
        "trainer.train(epochs=hparams[\"epochs\"], tokenizer=tokenizer, label_map=label_map)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载资源\n",
        "vocab_path = os.path.join(hparams[\"output_dir\"], 'vocab.json')\n",
        "with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "    loaded_vocab = json.load(f)\n",
        "\n",
        "labels_path = os.path.join(hparams[\"output_dir\"], 'label_map.json')\n",
        "with open(labels_path, 'r', encoding='utf-8') as f:\n",
        "    label_map_loaded = json.load(f)\n",
        "\n",
        "# 实例化推理组件\n",
        "inference_tokenizer = Tokenizer(vocab=loaded_vocab)\n",
        "inference_model = TextClassifier(\n",
        "    len(inference_tokenizer),\n",
        "    hparams[\"embed_dim\"], \n",
        "    hparams[\"hidden_dim\"], \n",
        "    len(label_map_loaded)\n",
        ").to(hparams[\"device\"])\n",
        "\n",
        "model_path = os.path.join(hparams[\"output_dir\"], \"best_model.pth\")\n",
        "inference_model.load_state_dict(torch.load(model_path, map_location=hparams[\"device\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = Predictor(\n",
        "    inference_model, \n",
        "    inference_tokenizer, \n",
        "    label_map_loaded, \n",
        "    hparams[\"device\"]\n",
        ")\n",
        "\n",
        "# 预测\n",
        "new_text = \"The doctor prescribed a new medicine for the patient's illness, focusing on its gpu accelerated healing properties.\"\n",
        "predicted_class = predictor.predict(new_text)\n",
        "\n",
        "{\"text\": new_text, \"pred\": predicted_class}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
