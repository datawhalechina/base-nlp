{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 一、环境准备\n",
        "\n",
        "依赖：`peft`, `transformers`, `datasets`, `accelerate`, `bitsandbytes`。\n",
        "提示：CUDA 与 `bitsandbytes` 需版本匹配。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q -U peft transformers datasets accelerate bitsandbytes hf_xet ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 二、加载基础模型和分词器\n",
        "\n",
        "要点：\n",
        "- 使用 `BitsAndBytesConfig` + `quantization_config` 进行 8-bit 量化\n",
        "- 设置 `device_map=\"auto\"` 由 `accelerate` 自动分配设备\n",
        "- tokenizer 无 `pad_token` 时对齐到 `eos_token`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# 定义模型 ID\n",
        "pythia_model_id = \"EleutherAI/pythia-2.8b-deduped\"\n",
        "\n",
        "# --- 使用 BitsAndBytesConfig 定义 8-bit 量化配置 ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "# 加载模型\n",
        "# 将量化配置传给 `quantization_config` 参数\n",
        "pythia_model = AutoModelForCausalLM.from_pretrained(\n",
        "    pythia_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "pythia_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载分词器\n",
        "pythia_tokenizer = AutoTokenizer.from_pretrained(pythia_model_id)\n",
        "\n",
        "# Pythia 模型的 tokenizer 默认没有 pad_token，我们将其设置为 eos_token\n",
        "pythia_tokenizer.pad_token = pythia_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 三、模型预处理\n",
        "\n",
        "使用 `prepare_model_for_kbit_training` 让 4/8-bit 量化模型进入可训练状态，并启用梯度检查点等节省显存的优化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# 对量化后的模型进行预处理\n",
        "pythia_model = prepare_model_for_kbit_training(pythia_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 四、定义 LoRA 配置并创建 PeftModel\n",
        "\n",
        "要点：\n",
        "- Pythia 常用 `target_modules`: `[\"query_key_value\", \"dense\"]`\n",
        "- 仅训练 LoRA 相关参数，显著降低显存与计算\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# 定义 LoRA 配置\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\", \"dense\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# 应用配置，获得 PEFT 模型\n",
        "peft_model = get_peft_model(pythia_model, lora_config)\n",
        "\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 五、数据处理\n",
        "\n",
        "数据集：`Abirate/english_quotes`\n",
        "流程：加载 -> 仅对 `quote` 列分词 -> 使用 `.map()` 批处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 加载数据集\n",
        "quotes_dataset = load_dataset(\"Abirate/english_quotes\")\n",
        "\n",
        "quotes_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义分词函数\n",
        "def tokenize_quotes(batch):\n",
        "    # 我们只对 \"quote\" 列进行分词\n",
        "    return pythia_tokenizer(batch[\"quote\"], truncation=True)\n",
        "\n",
        "# 对整个数据集进行分词处理\n",
        "tokenized_quotes = quotes_dataset.map(tokenize_quotes, batched=True)\n",
        "\n",
        "tokenized_quotes['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 六、定义 Trainer 并开始训练\n",
        "\n",
        "`TrainingArguments` 关键项：\n",
        "- `per_device_train_batch_size` 与 `gradient_accumulation_steps` 决定“有效批量”\n",
        "- `warmup_steps` 学习率预热\n",
        "- `fp16` 混合精度训练\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "# 这是推荐操作，可以使训练日志更清晰\n",
        "peft_model.config.use_cache = False\n",
        "\n",
        "# 定义训练参数\n",
        "train_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    output_dir=\"outputs\",\n",
        ")\n",
        "\n",
        "# 数据整理器\n",
        "quote_collator = DataCollatorForLanguageModeling(pythia_tokenizer, mlm=False)\n",
        "\n",
        "# 实例化 Trainer\n",
        "quote_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_quotes[\"train\"],\n",
        "    args=train_args,\n",
        "    data_collator=quote_collator,\n",
        ")\n",
        "\n",
        "# 开始训练\n",
        "quote_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 七、模型保存与推理\n",
        "\n",
        "仅保存 LoRA 适配器（`adapter_config.json`, `adapter_model.safetensors`）。\n",
        "\n",
        "随后做一次推理验证，观察生成效果变化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义 LoRA 适配器的保存路径\n",
        "adapter_save_path = \"./peft-lora-pythia-2.8b\"\n",
        "\n",
        "# 保存 LoRA 适配器\n",
        "peft_model.save_pretrained(adapter_save_path)\n",
        "\n",
        "adapter_save_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 微调后的推理测试\n",
        "\n",
        "要点：\n",
        "- 显式传递 `attention_mask`\n",
        "- 采样：`do_sample=True` 配合 `temperature`、`top_p`、`top_k`\n",
        "- 正确设置 `pad_token_id`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将模型设置为评估模式\n",
        "peft_model.eval()\n",
        "\n",
        "# 设置 pad_token_id 到模型配置中（避免警告）\n",
        "peft_model.config.pad_token_id = pythia_tokenizer.pad_token_id\n",
        "\n",
        "prompt = \"Be yourself; everyone\"\n",
        "\n",
        "# 对输入进行分词，并获取 attention_mask\n",
        "after_inputs = pythia_tokenizer(prompt, return_tensors=\"pt\")\n",
        "after_input_ids = after_inputs[\"input_ids\"].to(peft_model.device)\n",
        "after_attention_mask = after_inputs[\"attention_mask\"].to(peft_model.device)\n",
        "\n",
        "# 生成文本\n",
        "with torch.no_grad():\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        after_output = peft_model.generate(\n",
        "            input_ids=after_input_ids,\n",
        "            attention_mask=after_attention_mask,\n",
        "            max_length=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=40,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=pythia_tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "# 解码生成的文本\n",
        "after_text = pythia_tokenizer.decode(after_output[0], skip_special_tokens=True)\n",
        "after_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "peft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
